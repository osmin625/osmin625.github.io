<!DOCTYPE html>
<html lang="en"><head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <style>
        :root {
            --accent-color: #FF4D4D;
        }
    </style>

    
    
    
    
    
    

    
    <title>PyTorch의 Backward에 대해 알아보자.</title>
    <meta name="description" content="ML Engineer, Data Science">
    <meta name="keywords" content='blog, gokarna, hugo, PyTorch, Backward, Autograd'>

    <meta property="og:url" content="https://osmin625.github.io/posts/backward/">
    <meta property="og:type" content="website">
    <meta property="og:title" content="PyTorch의 Backward에 대해 알아보자.">
    <meta property="og:description" content="ML Engineer, Data Science">
    <meta property="og:image" content="https://osmin625.github.io/images/from_scratch.webp">
    <meta property="og:image:secure_url" content="https://osmin625.github.io/images/from_scratch.webp">

    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:title" content="PyTorch의 Backward에 대해 알아보자.">
    <meta name="twitter:description" content="ML Engineer, Data Science">
    <meta property="twitter:domain" content="https://osmin625.github.io/posts/backward/">
    <meta property="twitter:url" content="https://osmin625.github.io/posts/backward/">
    <meta name="twitter:image" content="https://osmin625.github.io/images/from_scratch.webp">

    
    <link rel="canonical" href="https://osmin625.github.io/posts/backward/" />

    
    <link rel="stylesheet" type="text/css" href="/css/normalize.min.css" media="print">

    
    <link rel="stylesheet" type="text/css" href="/css/main.min.css">

    
    <link id="dark-theme" rel="stylesheet" href="/css/dark.min.css">

    
    <script src="/js/bundle.min.d751e63c00e1161a75e60eaf823256097427a013523377fb636f7605271f62a5.js" integrity="sha256-11HmPADhFhp15g6vgjJWCXQnoBNSM3f7Y292BScfYqU="></script>

    
    
</head>
<body>
        <script type="text/javascript">
            
            setThemeByUserPref();
        </script><header class="header">
    <nav class="header-nav">

        
        <div class="avatar">
            <a href="https://osmin625.github.io">
                <img src='/images/from_scratch.webp' alt="OMIN" />
            </a>
        </div>
        

        <div class="nav-title">
            <a class="nav-brand" href="https://osmin625.github.io">OMIN</a>
        </div>

        <div class="nav-links">
            
            <div class="nav-link">
                <a href="https://osmin625.github.io/"><span data-feather='home'></span> Home </a>
            </div>
            
            <div class="nav-link">
                <a href="https://osmin625.github.io/posts/"><span data-feather='book'></span> Posts </a>
            </div>
            
            <div class="nav-link">
                <a href="https://osmin625.github.io/tags/"><span data-feather='tag'></span> Tags </a>
            </div>
            
            <div class="nav-link">
                <a href="https://github.com/osmin625"><span data-feather='github'></span>  </a>
            </div>
            

            <span class="nav-icons-divider"></span>
            <div class="nav-link dark-theme-toggle">
                <span id="dark-theme-toggle-screen-reader-target" class="sr-only"></span>
                <a>
                    <span id="theme-toggle-icon" data-feather="moon"></span>
                </a>
            </div>

            <div class="nav-link" id="hamburger-menu-toggle">
                <span id="hamburger-menu-toggle-screen-reader-target" class="sr-only">menu</span>
                <a>
                    <span data-feather="menu"></span>
                </a>
            </div>

            
            <ul class="nav-hamburger-list visibility-hidden">
                
                <li class="nav-item">
                    <a href="https://osmin625.github.io/"><span data-feather='home'></span> Home </a>
                </li>
                
                <li class="nav-item">
                    <a href="https://osmin625.github.io/posts/"><span data-feather='book'></span> Posts </a>
                </li>
                
                <li class="nav-item">
                    <a href="https://osmin625.github.io/tags/"><span data-feather='tag'></span> Tags </a>
                </li>
                
                <li class="nav-item">
                    <a href="https://github.com/osmin625"><span data-feather='github'></span>  </a>
                </li>
                
                <li class="nav-item dark-theme-toggle">
                    <span id="dark-theme-toggle-screen-reader-target" class="sr-only">theme</span>
                    <a>
                        <span id="theme-toggle-icon" data-feather="moon"></span>
                    </a>
                </li>
            </ul>

        </div>
    </nav>
</header>
<main id="content">
    <div class="post container">
    <div class="post-header-section">
        <h1>PyTorch의 Backward에 대해 알아보자.</h1>
        <small role="doc-subtitle"></small>
        <p class="post-date">3월 13, 2023
        
        </p>

        <ul class="post-tags">
        
            <li class="post-tag"><a href="https://osmin625.github.io/tags/pytorch">PyTorch</a></li>
        
            <li class="post-tag"><a href="https://osmin625.github.io/tags/backward">Backward</a></li>
        
            <li class="post-tag"><a href="https://osmin625.github.io/tags/autograd">Autograd</a></li>
        
        </ul>
    </div>

    <div class="post-content">
        <p>
            <blockquote>
<p>✔️ 간단 요약<br>
<code>forward</code>함수를 정의하면 자동으로 정의된다.<br>
<strong>동작 과정</strong></p>
<ol>
<li>tensor(loss에 해당)가 포함된 식을 미분한다.</li>
<li>미분 값을 tensor에 저장한다.<br>
{: .prompt-info }
Autograd, loss, optimizer, nn.Module</li>
</ol>
</blockquote>
<ol>
<li>
<p><strong>tensor(loss에 해당)가 포함된 식을 미분한다.</strong></p>
<p>Tensor 객체의 backward 함수에는 default로 Autograd 설정이 되어 있기 때문에, 미분 수식을 따로 작성하지 않아도 자동으로 미분이 가능하다.</p>
<p><strong>기본 예제</strong></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>w <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>tensor(<span style="color:#ae81ff">2.0</span>, requires_grad<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>y <span style="color:#f92672">=</span> w<span style="color:#f92672">**</span><span style="color:#ae81ff">2</span>
</span></span><span style="display:flex;"><span>z <span style="color:#f92672">=</span> <span style="color:#ae81ff">10</span><span style="color:#f92672">*</span>y <span style="color:#f92672">+</span> <span style="color:#ae81ff">25</span>
</span></span><span style="display:flex;"><span>z<span style="color:#f92672">.</span>backward()
</span></span><span style="display:flex;"><span>w<span style="color:#f92672">.</span>grad()
</span></span><span style="display:flex;"><span><span style="color:#75715e"># output : tensor(40.)</span>
</span></span></code></pre></div><p><code>$$</code>
w = 2\
y = w^2\
z = 10\times y  + 25\
z = 10 \times w^2 + 25\
{dz\over dw} = 20 \times w = 40
<code>$$</code></p>
<p><strong>미분 값이 여러 개인 경우</strong></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>a <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>tensor([<span style="color:#ae81ff">2.</span>,<span style="color:#ae81ff">3.</span>], requires_grad<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>b <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>tensor([<span style="color:#ae81ff">6.</span>,<span style="color:#ae81ff">4.</span>], requires_grad<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>Q <span style="color:#f92672">=</span> <span style="color:#ae81ff">3</span> <span style="color:#f92672">*</span> a <span style="color:#f92672">**</span> <span style="color:#ae81ff">3</span> <span style="color:#f92672">-</span> b <span style="color:#f92672">**</span> <span style="color:#ae81ff">2</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 미분값이 두 개가 나와야하기 때문에, </span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># gradient 값의 크기를 잡아준다.</span>
</span></span><span style="display:flex;"><span>external_grad <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>tensor([<span style="color:#ae81ff">1.</span>, <span style="color:#ae81ff">1.</span>])
</span></span><span style="display:flex;"><span>Q<span style="color:#f92672">.</span>backward(gradient<span style="color:#f92672">=</span>external_grad)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>a<span style="color:#f92672">.</span>grad
</span></span><span style="display:flex;"><span>b<span style="color:#f92672">.</span>grad
</span></span><span style="display:flex;"><span><span style="color:#75715e"># output: tensor([36.,81.])</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># output: tensor([-12.,-8.])</span>
</span></span></code></pre></div><p><code>$$</code>
\begin{aligned}
&amp;Q = 3a^3 - b^2\
&amp; \frac{\partial Q}{\partial a}=9 a^2 \
&amp; \frac{\partial Q}{\partial b}=-2 b
\end{aligned}
<code>$$</code></p>
</li>
<li>
<p><strong>미분 값을 tensor에 저장한다.</strong></p>
<p>tensor를 선언할 때 require_grad=True로 설정하면 tensor에 grad_fn 정보가 저장된다.</p>
<p>e.g. <code>tensor(6.2564e-05, grad_fn=&lt;MseLossBackward0&gt;)</code></p>
<p>또한, <code>tensor.grad()</code>를 통해 미분 값을 확인할 수 있다.</p>
<p>grad_fn에는 텐서가 어떤 연산을 했는 연산 정보를 담고 있고, 이 정보는 역전파에 사용된다.</p>
</li>
</ol>
<p><a href="https://gaussian37.github.io/dl-pytorch-gradient/">PyTorch Gradient 관련 설명 (Autograd)</a></p>
<p>실제 backward는 Module 단계에서 직접 지정이 가능하다.</p>
<p>Module에서 backward와 optimizer를 오버라이딩 해주면 된다.</p>
<p>사용자가 직접 미분 수식을 써야 하는 부담이 있다.</p>
<p>쓸 일은 없지만, 순서를 이해할 필요는 있다.</p>
<h3 id="예제--logistic-regression">예제 — Logistic Regression</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">LR</span>(nn<span style="color:#f92672">.</span>Module):
</span></span><span style="display:flex;"><span>	<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">init</span> (self, dim, lr<span style="color:#f92672">=</span>torch<span style="color:#f92672">.</span>scalar_tensor(<span style="color:#ae81ff">0.01</span>)):
</span></span><span style="display:flex;"><span>		super(LR, self)<span style="color:#f92672">.</span> init () 
</span></span><span style="display:flex;"><span>		<span style="color:#75715e"># intialize parameters</span>
</span></span><span style="display:flex;"><span>		self<span style="color:#f92672">.</span>w <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>zeros(dim, <span style="color:#ae81ff">1</span>, dtype<span style="color:#f92672">=</span>torch<span style="color:#f92672">.</span>float)<span style="color:#f92672">.</span>to(device)
</span></span><span style="display:flex;"><span>		self<span style="color:#f92672">.</span>b <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>scalar_tensor(<span style="color:#ae81ff">0</span>)<span style="color:#f92672">.</span>to(device)
</span></span><span style="display:flex;"><span>		self<span style="color:#f92672">.</span>grads <span style="color:#f92672">=</span> {<span style="color:#e6db74">&#34;dw&#34;</span>: torch<span style="color:#f92672">.</span>zeros(dim, <span style="color:#ae81ff">1</span>, dtype<span style="color:#f92672">=</span>torch<span style="color:#f92672">.</span>float)<span style="color:#f92672">.</span>to(device), 
</span></span><span style="display:flex;"><span>					  <span style="color:#e6db74">&#34;db&#34;</span>: torch<span style="color:#f92672">.</span>scalar_tensor(<span style="color:#ae81ff">0</span>)<span style="color:#f92672">.</span>to(device)}
</span></span><span style="display:flex;"><span>		self<span style="color:#f92672">.</span>lr <span style="color:#f92672">=</span> lr<span style="color:#f92672">.</span>to(device)
</span></span></code></pre></div><p><code>$$ h_\theta(x)=\frac{1}{1+e^{-\theta^T \mathbf{x}}} $$</code></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, x): 
</span></span><span style="display:flex;"><span>	<span style="color:#75715e">## compute forward</span>
</span></span><span style="display:flex;"><span>	z <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>mm(self<span style="color:#f92672">.</span>w<span style="color:#f92672">.</span>T, x) 
</span></span><span style="display:flex;"><span>	a <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>sigmoid(z) 
</span></span><span style="display:flex;"><span>	<span style="color:#66d9ef">return</span> a
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">sigmoid</span>(self, z):
</span></span><span style="display:flex;"><span>	<span style="color:#66d9ef">return</span> <span style="color:#ae81ff">1</span><span style="color:#f92672">/</span> (<span style="color:#ae81ff">1</span> <span style="color:#f92672">+</span> torch<span style="color:#f92672">.</span>exp(<span style="color:#f92672">-</span>z))
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">backward</span>(self, x, yhat, y): 
</span></span><span style="display:flex;"><span>	<span style="color:#75715e">## compute backward</span>
</span></span><span style="display:flex;"><span>	self<span style="color:#f92672">.</span>grads[<span style="color:#e6db74">&#34;dw&#34;</span>] <span style="color:#f92672">=</span> (<span style="color:#ae81ff">1</span><span style="color:#f92672">/</span>x<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">1</span>]) <span style="color:#f92672">*</span> torch<span style="color:#f92672">.</span>mm(x, (yhat <span style="color:#f92672">-</span> y)<span style="color:#f92672">.</span>T) 
</span></span><span style="display:flex;"><span>	self<span style="color:#f92672">.</span>grads[<span style="color:#e6db74">&#34;db&#34;</span>] <span style="color:#f92672">=</span> (<span style="color:#ae81ff">1</span><span style="color:#f92672">/</span>x<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">1</span>]) <span style="color:#f92672">*</span> torch<span style="color:#f92672">.</span>sum(yhat <span style="color:#f92672">-</span> y)
</span></span></code></pre></div><p><code>$$ \begin{aligned}&amp;\frac{\partial}{\partial \theta_j} J(\theta)=\frac{1}{m} \sum_{i=1}^m\left(h_\theta\left(x^i\right)-y^i\right) x_j^i\end{aligned} $$</code></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">optimize</span>(self):
</span></span><span style="display:flex;"><span>	<span style="color:#75715e">## optimization step</span>
</span></span><span style="display:flex;"><span>	self<span style="color:#f92672">.</span>w <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>w <span style="color:#f92672">-</span> self<span style="color:#f92672">.</span>lr <span style="color:#f92672">*</span> self<span style="color:#f92672">.</span>grads[<span style="color:#e6db74">&#34;dw&#34;</span>]
</span></span><span style="display:flex;"><span>	self<span style="color:#f92672">.</span>b <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>b <span style="color:#f92672">-</span> self<span style="color:#f92672">.</span>lr <span style="color:#f92672">*</span> self<span style="color:#f92672">.</span>grads[<span style="color:#e6db74">&#34;db&#34;</span>]
</span></span></code></pre></div><p>기존의 <code>$\theta$</code>, 즉, <code>$\tt w$</code> 값에 미분값 만큼의 업데이트를 수행해주는 함수.</p>
<p><code>$$ \begin{aligned}\theta_j &amp; :=\theta_j-\alpha \frac{\partial}{\partial \theta_j} J(\theta) \\&amp; :=\theta_j-\alpha \sum_{i=1}^m\left(h_\theta\left(x^i\right)-y^i\right) x_j^i\end{aligned} $$</code></p>
<ul>
<li>
<p>전체 코드</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">LR</span>(nn<span style="color:#f92672">.</span>Module):
</span></span><span style="display:flex;"><span>	<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">init</span> (self, dim, lr<span style="color:#f92672">=</span>torch<span style="color:#f92672">.</span>scalar_tensor(<span style="color:#ae81ff">0.01</span>)):
</span></span><span style="display:flex;"><span>		super(LR, self)<span style="color:#f92672">.</span> init () 
</span></span><span style="display:flex;"><span>		<span style="color:#75715e"># intialize parameters</span>
</span></span><span style="display:flex;"><span>		self<span style="color:#f92672">.</span>w <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>zeros(dim, <span style="color:#ae81ff">1</span>, dtype<span style="color:#f92672">=</span>torch<span style="color:#f92672">.</span>float)<span style="color:#f92672">.</span>to(device)
</span></span><span style="display:flex;"><span>		self<span style="color:#f92672">.</span>b <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>scalar_tensor(<span style="color:#ae81ff">0</span>)<span style="color:#f92672">.</span>to(device)
</span></span><span style="display:flex;"><span>		self<span style="color:#f92672">.</span>grads <span style="color:#f92672">=</span> {<span style="color:#e6db74">&#34;dw&#34;</span>: torch<span style="color:#f92672">.</span>zeros(dim, <span style="color:#ae81ff">1</span>, dtype<span style="color:#f92672">=</span>torch<span style="color:#f92672">.</span>float)<span style="color:#f92672">.</span>to(device), 
</span></span><span style="display:flex;"><span>					  <span style="color:#e6db74">&#34;db&#34;</span>: torch<span style="color:#f92672">.</span>scalar_tensor(<span style="color:#ae81ff">0</span>)<span style="color:#f92672">.</span>to(device)}
</span></span><span style="display:flex;"><span>		self<span style="color:#f92672">.</span>lr <span style="color:#f92672">=</span> lr<span style="color:#f92672">.</span>to(device)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>	<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, x): 
</span></span><span style="display:flex;"><span>		<span style="color:#75715e">## compute forward</span>
</span></span><span style="display:flex;"><span>		z <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>mm(self<span style="color:#f92672">.</span>w<span style="color:#f92672">.</span>T, x) 
</span></span><span style="display:flex;"><span>		a <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>sigmoid(z) 
</span></span><span style="display:flex;"><span>		<span style="color:#66d9ef">return</span> a
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>	<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">sigmoid</span>(self, z):
</span></span><span style="display:flex;"><span>		<span style="color:#66d9ef">return</span> <span style="color:#ae81ff">1</span><span style="color:#f92672">/</span> (<span style="color:#ae81ff">1</span> <span style="color:#f92672">+</span> torch<span style="color:#f92672">.</span>exp(<span style="color:#f92672">-</span>z))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>	<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">backward</span>(self, x, yhat, y): 
</span></span><span style="display:flex;"><span>		<span style="color:#75715e">## compute backward</span>
</span></span><span style="display:flex;"><span>		self<span style="color:#f92672">.</span>grads[<span style="color:#e6db74">&#34;dw&#34;</span>] <span style="color:#f92672">=</span> (<span style="color:#ae81ff">1</span><span style="color:#f92672">/</span>x<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">1</span>]) <span style="color:#f92672">*</span> torch<span style="color:#f92672">.</span>mm(x, (yhat <span style="color:#f92672">-</span> y)<span style="color:#f92672">.</span>T) 
</span></span><span style="display:flex;"><span>		self<span style="color:#f92672">.</span>grads[<span style="color:#e6db74">&#34;db&#34;</span>] <span style="color:#f92672">=</span> (<span style="color:#ae81ff">1</span><span style="color:#f92672">/</span>x<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">1</span>]) <span style="color:#f92672">*</span> torch<span style="color:#f92672">.</span>sum(yhat <span style="color:#f92672">-</span> y)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>	<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">optimize</span>(self):
</span></span><span style="display:flex;"><span>		<span style="color:#75715e">## optimization step</span>
</span></span><span style="display:flex;"><span>		self<span style="color:#f92672">.</span>w <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>w <span style="color:#f92672">-</span> self<span style="color:#f92672">.</span>lr <span style="color:#f92672">*</span> self<span style="color:#f92672">.</span>grads[<span style="color:#e6db74">&#34;dw&#34;</span>]
</span></span><span style="display:flex;"><span>		self<span style="color:#f92672">.</span>b <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>b <span style="color:#f92672">-</span> self<span style="color:#f92672">.</span>lr <span style="color:#f92672">*</span> self<span style="color:#f92672">.</span>grads[<span style="color:#e6db74">&#34;db&#34;</span>]
</span></span></code></pre></div></li>
</ul>

        </p>
        
    </div>

    <div class="prev-next">
        
    </div>

    
    
    <svg id="btt-button" class="arrow-logo" xmlns="http://www.w3.org/2000/svg" height="1em" viewBox="0 0 384 512" onclick="topFunction()" title="Go to top">
        
        <path d="M177 159.7l136 136c9.4 9.4 9.4 24.6 0 33.9l-22.6 22.6c-9.4 9.4-24.6 9.4-33.9 0L160 255.9l-96.4 96.4c-9.4 9.4-24.6 9.4-33.9 0L7 329.7c-9.4-9.4-9.4-24.6 0-33.9l136-136c9.4-9.5 24.6-9.5 34-.1z"/>
    </svg>
    
    <script>
        let backToTopButton = document.getElementById("btt-button");

        window.onscroll = function() {
            scrollFunction()
        };

        function scrollFunction() {
            if (document.body.scrollTop > 20 || document.documentElement.scrollTop > 20) {
                backToTopButton.style.display = "block";
            } else {
                backToTopButton.style.display = "none";
            }
        }

        function topFunction() {
            smoothScrollToTop();
        }

        function smoothScrollToTop() {
            const scrollToTop = () => {
                const c = document.documentElement.scrollTop || document.body.scrollTop;
                if (c > 0) {
                    window.requestAnimationFrame(scrollToTop);
                    window.scrollTo(0, c - c / 8);
                }
            };
            scrollToTop();
        }
    </script>
    
    
    <div id="tex">
        <script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [['$$','$$']],
    processEscapes: true,
    processEnvironments: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
    TeX: { equationNumbers: { autoNumber: "AMS" },
         extensions: ["AMSmath.js", "AMSsymbols.js"] }
  }
  });
  MathJax.Hub.Queue(function() {
    
    
    
    var all = MathJax.Hub.getAllJax(), i;
    for(i = 0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script> 
    </div>
    
</div>

<aside class="post-toc">
    <nav id="toc">
        <nav id="TableOfContents">
  <ul>
    <li>
      <ul>
        <li>
          <ul>
            <li><a href="#예제--logistic-regression">예제 — Logistic Regression</a></li>
          </ul>
        </li>
      </ul>
    </li>
  </ul>
</nav>
    </nav>
</aside>


    

        </main><footer class="footer">
    
    

    
    <span>&copy; 2024 OMIN</span>
    
</footer>
</body>
</html>
