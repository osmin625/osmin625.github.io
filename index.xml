<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>오뚝이 개발자</title>
    <link>https://osmin625.github.io/</link>
    <description>Recent content on 오뚝이 개발자</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>ko</language>
    <lastBuildDate>Mon, 18 Mar 2024 05:59:00 +0900</lastBuildDate>
    <atom:link href="https://osmin625.github.io/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Notion2Hugo blog 자동화</title>
      <link>https://osmin625.github.io/posts/notion-to-hugo-blog-%EC%9E%90%EB%8F%99%ED%99%94/</link>
      <pubDate>Mon, 18 Mar 2024 05:59:00 +0900</pubDate>
      <guid>https://osmin625.github.io/posts/notion-to-hugo-blog-%EC%9E%90%EB%8F%99%ED%99%94/</guid>
      <description>나는 노션을 꽤 오래 전부터 사용해왔다.&#xA;공부하며 노션에 기록하는 것이 습관화되어 있는데, 이렇게 정리해 둔 게시글만 500개가 넘는다.&#xA;애초부터 노션에 정리해 둔 글을 보다 쉽게 블로그에 업로드하고자 호환성이 좋은 깃허브 블로그를 사용하고 있으나, 학습 내용을 기록하는 공간이 두 개로 구분되었을 때 둘 모두를 관리하는 건 생각보다 쉽지 않다는 것을 깨달았다.&#xA;아무튼, 이번에는 Hugo 블로그에 Notion에서 작성한 게시글을 보다 쉽게 업로드하기 위해 자동화를 시도해보았다.&#xA;기존에는 post 작성을 끝낸 후 build하고 push하는 과정을 쉘 커맨드로 간편하게 구성해두었다.</description>
    </item>
    <item>
      <title>Hugo Blog category 404 해결 방법</title>
      <link>https://osmin625.github.io/posts/hugo_blog_category_404_%E1%84%92%E1%85%A2%E1%84%80%E1%85%A7%E1%86%AF_%E1%84%87%E1%85%A1%E1%86%BC%E1%84%87%E1%85%A5%E1%86%B8/</link>
      <pubDate>Wed, 31 Jan 2024 07:50:00 +0900</pubDate>
      <guid>https://osmin625.github.io/posts/hugo_blog_category_404_%E1%84%92%E1%85%A2%E1%84%80%E1%85%A7%E1%86%AF_%E1%84%87%E1%85%A1%E1%86%BC%E1%84%87%E1%85%A5%E1%86%B8/</guid>
      <description>Hugo 블로그를 사용하며 분명이 업로드한 포스팅이 보이지 않는 경우가 가끔 발생한다.&#xA;나는 이번에 블로그를 hugo로 이전하며 카테고리 페이지를 직접 구현했는데, 이 때 카테고리가 제대로 동작하지 않는 문제를 마주했다.&#xA;이는 카테고리 front matter에 대문자가 포함된 경우 hugo의 urlize의 동작이 jekyll의 urlize와 달라서 발생하는 문제로 보인다.&#xA;category → 404 found 기존에 Jekyll에서는 front-matter에 categories로 대문자를 마음껏 사용했었는데, Hugo 블로그로 옮기고 나니 대문자가 인식이 되지 않는다.&#xA;Hugo와 Jekyll의 urlize 차이점 jekyll에서의 urlize — 대문자는 그대로 유지, 띄어쓰기는 ‘-’로 변경</description>
    </item>
    <item>
      <title>Taylor polynomials</title>
      <link>https://osmin625.github.io/posts/Taylor-Polynomials/</link>
      <pubDate>Sat, 11 Nov 2023 18:04:00 +0900</pubDate>
      <guid>https://osmin625.github.io/posts/Taylor-Polynomials/</guid>
      <description>테일러 근사 복잡한 형태의 미분 가능한 함수 $f(x)$를 다항식의 합으로 근사하는 것&#xA;$a$를 포함하는 구간에서 함수 $f$가 무한 미분이 가능 할 때&#xA;$$&#xD;\begin{aligned}f(x) &amp;amp; =\sum_{n=0}^{\infty} \frac{f^{(n)}(a)}{n !}(x-a)^n \\&amp;amp; =f(a)+\frac{f^{\prime}(a)}{1 !}(x-a)+\frac{f^{\prime \prime}(a)}{2 !}(x-a)^2+\frac{f^{\prime \prime \prime}(a)}{3 !}(x-a)^3+\ldots\end{aligned}&#xD;$$&#xA;를 테일러 급수라고 한다.&#xA;$f(x)$를 임의의 수 $a$에 대해 정리하는 과정이라 이해하면 편하다.&#xA;수식이 이렇게 생긴 이유&#xA;$f(x)$를 $a$에 대해 정리하고 싶어 식을 $f(x) = t_n(x-a)^n + t_{n-1}(x-a)^{n-1 }+\dots + t_1(x-a)^1$와 같이 정의했을 때,</description>
    </item>
    <item>
      <title>[논문 리뷰] LeakGAN: Long Text Generation via Adversarial Training with Leaked Information</title>
      <link>https://osmin625.github.io/posts/LeakGAN/</link>
      <pubDate>Sat, 07 Oct 2023 02:40:00 +0900</pubDate>
      <guid>https://osmin625.github.io/posts/LeakGAN/</guid>
      <description>✔️ 간단 요약&#xA;Sparsity와 Non-Informative를 효과적으로 해결한다.&#xA;분별망에게 스파이를 심어 생성망이 분별망을 더 잘 속일 수 있도록 구성한다.&#xA;Hierarchical RL architecture (MANAGER, WORKER)&#xA;MANAGER (LSTM)&#xA;중재자 역할 D로부터 고수준 feature representation을 받음 → Leakage WORKER (LSTM)&#xA;$s_t$를 인코딩한 후, MANAGER가 넘겨준 Goal 임베딩과 결합한다. (내적) D가 넘겨준 guiding signal은 scalar 보상 값으로도 쓰이고, 문장 생성 과정에서 Goal 임베딩으로도 쓰인다. {: .prompt-info } logit, temperature parameter, highway network(gate), leakgan의 3가지 학습 방법, CNN for text classification, truncated normalization</description>
    </item>
    <item>
      <title>추천 시스템 개요</title>
      <link>https://osmin625.github.io/posts/Recsys-Overview/</link>
      <pubDate>Wed, 27 Sep 2023 13:53:00 +0900</pubDate>
      <guid>https://osmin625.github.io/posts/Recsys-Overview/</guid>
      <description>Naver BoostCamp AI Tech에서 학습한 내용을 재구성했습니다.&#xA;해당 게시글은 지속적으로 업데이트할 예정입니다.&#xA;노션에 정리했던 내용을 복습하며 블로그에 조금씩 업로드하고 있습니다.&#xA;추천 시스템 추천 시스템 평가 패러다임 Rule Base 인기도 기반 추천&#xA;연관 분석(Association Analysis)&#xA;CBF: Content Based Filtering 1. Vectorizer — 아이템 특성을 벡터 형태로 어떻게 표현하는가 TF-IDF&#xA;TF-IDF 기반 추천&#xA;BM25&#xA;Word2Vec&#xA;2. Similarity — 특성화된 아이템이 서로 얼마나 비슷한가 Similarity&#xA;Distance&#xA;CF: Collaborative Filtering(협업 필터링) NBCF: Neighborhood-based CF(이웃 기반 협업 필터링) MBCF: Model based Collaborative Filtering(모델 기반 협업 필터링) ML based CF</description>
    </item>
    <item>
      <title>Entropy(엔트로피)</title>
      <link>https://osmin625.github.io/posts/entropy_%E1%84%8B%E1%85%A6%E1%86%AB%E1%84%90%E1%85%B3%E1%84%85%E1%85%A9%E1%84%91%E1%85%B5/</link>
      <pubDate>Fri, 22 Sep 2023 07:11:00 +0900</pubDate>
      <guid>https://osmin625.github.io/posts/entropy_%E1%84%8B%E1%85%A6%E1%86%AB%E1%84%90%E1%85%B3%E1%84%85%E1%85%A9%E1%84%91%E1%85%B5/</guid>
      <description>간단 요약&#xA;확률 분포의 불확실성을 의미한다.&#xA;정보 이론에서의 엔트로피는 (최적화된 전략 하에서의) 질문 개수에 대한 기댓값이다.&#xA;스무 고개로 정답 맞추기를 진행할 때 질문이 많이 필요할수록 불확실성이 높은 것이다.&#xA;확률 분포의 무작위성(불확실성)을 측정하는 함수&#xA;$$&#xD;H(X)=\sum_{i=1}^n p_i\left(\log \frac{1}{p_i}\right)=-\sum_{i=1}^n p_i \log p_i&#xD;$$&#xA;entropy 공식은 왜 이렇게 생겼을까?&#xA;스무 고개로 정답 맞추기를 진행할 때 확률 분포가 불확실할수록 필요한 질문의 개수가 늘어난다.&#xA;이 때, 전체 경우를 양분하는 질문의 개수는 $log_2$를 통해 파악할 수 있다.</description>
    </item>
    <item>
      <title>Cross-Entropy(=Log loss, 교차 엔트로피)</title>
      <link>https://osmin625.github.io/posts/cross_entropy_log_loss_%E1%84%80%E1%85%AD%E1%84%8E%E1%85%A1_%E1%84%8B%E1%85%A6%E1%86%AB%E1%84%90%E1%85%B3%E1%84%85%E1%85%A9%E1%84%91%E1%85%B5/</link>
      <pubDate>Fri, 22 Sep 2023 06:54:00 +0900</pubDate>
      <guid>https://osmin625.github.io/posts/cross_entropy_log_loss_%E1%84%80%E1%85%AD%E1%84%8E%E1%85%A1_%E1%84%8B%E1%85%A6%E1%86%AB%E1%84%90%E1%85%B3%E1%84%85%E1%85%A9%E1%84%91%E1%85%B5/</guid>
      <description>두 확률 분포 P와 Q가 다른 정도를 측정하는 함수&#xA;$$&#xD;H(P, Q)=-\sum_{i=1, k} P\left(e_i\right) \log Q\left(e_i\right)&#xD;$$&#xA;최선의 전략이 아닐 때의 질문 개수의 기댓값&#xA;엔트로피를 최적화된 전략 하에서 질문 개수에 대한 기댓값이라고 설명했다.&#xA;하지만 현실 문제의 대부분의 경우 최선의 전략을 찾기 어렵다.&#xA;스무 고개에서 최적의 전략이 각 사건의 확률에 의해 결정되듯이, 전략은 곧 확률 분포라고 이해할 수 있다.&#xA;결국 엔트로피는 전략$(\log P_i)$과 사건$(P_i)$의 분포가 동일한 상태를 의미하고,&#xA;교차 엔트로피는 전략$(\log Q_i)$과 사건$(P_i)$의 분포가 다른 상태를 의미한다.</description>
    </item>
    <item>
      <title>Polynomial Interpolation(보간 다항식)</title>
      <link>https://osmin625.github.io/posts/Polynomial-Interpolation/</link>
      <pubDate>Tue, 12 Sep 2023 23:39:00 +0900</pubDate>
      <guid>https://osmin625.github.io/posts/Polynomial-Interpolation/</guid>
      <description>Linear Interpolation 가장 간단한 보간법&#xA;두 점을 이은 직선의 방정식을 근사 함수로 사용한다.&#xA;데이터 점들 사이의 간격이 작을수록 더 좋은 근삿값을 얻는다.&#xA;$$&#xD;\mathrm{g}(x)=\frac{f\left(x_{i+1}\right)-f\left(x_i\right)}{x_{i+1}-x_i}\left(x-x_i\right)+f\left(x_i\right)&#xD;$$&#xA;Polynomial interpolation (n+1)개의 점이 주어진 경우 n차 이하의 유일한 다항식을 구할 수 있다.&#xA;Q. n+1개의 점으로 찾을 수 있는 n차 다항식은 왜 유일한가?&#xA;방데르몽드 행렬&#xA;각 행의 초항이 1인 등비수열로 이루어진 행렬&#xA;$$&#xD;V=\left(\begin{array}{ccccc}1 &amp;amp; \alpha_1 &amp;amp; \alpha_1^2 &amp;amp; \cdots &amp;amp; \alpha_1^{n-1} \\&#xD;1 &amp;amp; \alpha_2 &amp;amp; \alpha_2^2 &amp;amp; \cdots &amp;amp; \alpha_2^{n-1} \\&#xD;1 &amp;amp; \alpha_3 &amp;amp; \alpha_3^2 &amp;amp; \cdots &amp;amp; \alpha_3^{n-1} \\&#xD;\vdots &amp;amp; \vdots &amp;amp; \vdots &amp;amp; &amp;amp; \vdots \\&#xD;1 &amp;amp; \alpha_m &amp;amp; \alpha_m^2 &amp;amp; \cdots &amp;amp; \alpha_m^{n-1}\end{array}\right)&#xD;$$</description>
    </item>
    <item>
      <title>네이버 부스트캠프 AI Tech 회고 2 — Upstage 기업연계 프로젝트</title>
      <link>https://osmin625.github.io/posts/%EB%84%A4%EC%9D%B4%EB%B2%84-%EB%B6%80%EC%8A%A4%ED%8A%B8%EC%BA%A0%ED%94%84-AI-Tech-%ED%9A%8C%EA%B3%A0-2-Upstage-%EA%B8%B0%EC%97%85%EC%97%B0%EA%B3%84-%ED%94%84%EB%A1%9C%EC%A0%9D%ED%8A%B8/</link>
      <pubDate>Tue, 08 Aug 2023 18:30:00 +0900</pubDate>
      <guid>https://osmin625.github.io/posts/%EB%84%A4%EC%9D%B4%EB%B2%84-%EB%B6%80%EC%8A%A4%ED%8A%B8%EC%BA%A0%ED%94%84-AI-Tech-%ED%9A%8C%EA%B3%A0-2-Upstage-%EA%B8%B0%EC%97%85%EC%97%B0%EA%B3%84-%ED%94%84%EB%A1%9C%EC%A0%9D%ED%8A%B8/</guid>
      <description>최종 프로젝트는 변성윤 마스터님(유튜버 카일스쿨)의 Product Serving 강좌와 함께 진행됐다.&#xA;내가 강의를 통해 최종 프로젝트의 흐름에 대해 정리한 바로는 AI를 활용하는 End-to-End 서비스를 구축하는 것이 1차 목표이고, 2차 목표로는 사용자 피드백을 받아 서비스를 개선하는 것이다.&#xA;다만 우리 팀은 기업 연계 프로젝트를 진행하여 AI 활용 서비스를 개발하는 것에 초점을 두지 않았다.&#xA;업스테이지 기업연계 프로젝트 전체 트랙에서 지원을 받아 최종적으로 뽑힌 팀이 기업이 제안한 주제로 프로젝트를 진행할 수 있다.&#xA;지원 배경 기연프에 지원하기 위해선 팀원 모두가 기업에서 제안한 프로젝트 주제가 마음에 들어야 한다.</description>
    </item>
    <item>
      <title>5F 8월 1주차 주간회고록</title>
      <link>https://osmin625.github.io/posts/5F-%EC%A3%BC%EA%B0%84%ED%9A%8C%EA%B3%A0%EB%A1%9D-230806/</link>
      <pubDate>Sun, 06 Aug 2023 01:22:00 +0900</pubDate>
      <guid>https://osmin625.github.io/posts/5F-%EC%A3%BC%EA%B0%84%ED%9A%8C%EA%B3%A0%EB%A1%9D-230806/</guid>
      <description>8월 2일 네트워킹 데이 때 기업에서 채용담당자 분들과 면담할 기회가 있었다.&#xA;어떤 캠퍼가 기업관계자분께 *“신입 개발자에게 가장 중요한 것이 무엇이냐”*고 물었고, 이에 대한 대답으로&#xA;문제를 잘 정의하고, 다양한 솔루션을 시도하는 역량&#xA;이 중요하다고 말씀해주셨다.&#xA;이 말을 캠퍼 50명과 함께 듣고 있었는데, 마치 나를 해주는 충고처럼 느껴졌다.&#xA;아무래도 PM으로 프로젝트를 주도하다보니, 문제 정의와 다양한 솔루션 제안에서 많은 고민을 했기 때문에 더 크게 와닿았던 것이 아닐까 생각한다.&#xA;그 충고를 듣고 보니 나는 일부 솔루션에 매몰되는 성향이 있었다.</description>
    </item>
    <item>
      <title>네이버 부스트캠프 AI Tech 회고 1 — 강의, 대회</title>
      <link>https://osmin625.github.io/posts/%EB%84%A4%EC%9D%B4%EB%B2%84-%EB%B6%80%EC%8A%A4%ED%8A%B8%EC%BA%A0%ED%94%84-AI-Tech-%ED%9A%8C%EA%B3%A0-1-%EA%B0%95%EC%9D%98-%EB%8C%80%ED%9A%8C/</link>
      <pubDate>Sat, 29 Jul 2023 01:24:00 +0900</pubDate>
      <guid>https://osmin625.github.io/posts/%EB%84%A4%EC%9D%B4%EB%B2%84-%EB%B6%80%EC%8A%A4%ED%8A%B8%EC%BA%A0%ED%94%84-AI-Tech-%ED%9A%8C%EA%B3%A0-1-%EA%B0%95%EC%9D%98-%EB%8C%80%ED%9A%8C/</guid>
      <description>{: lqip=&amp;quot;/imgs/boostcamp_logo.png&amp;quot; }&#xA;드디어! 최종 프로젝트를 제출하면서, 길고 길었던 부스트캠프 AI Tech 5기의 모든 일정이 종료됐다.&#xA;부스트캠프 전체에 대한 회고도 하면서, 기업연계 프로젝트에 대한 회고도 담아보고자 한다.&#xA;처음에는 하나의 포스트로 작성했는데, 쓰다보니 너무 길어져 구분하여 글을 작성했다.&#xA;왜 부스트캠프 추천 트랙에 지원했나? AI 부스트캠프 지원자를 모집할 당시(22년 하반기), 나는 카카오 DS 최종면접을 준비하고 있었다.&#xA;1년간 통계학과와 수학과를 넘나들며 나름 AI의 수학적 지식기반을 쌓았(다고 생각했)고, 여러 경진대회를 경험하고 AI 관련 과목을 수강하면서 점점 자신감이 차오르던 시기였다.</description>
    </item>
    <item>
      <title>Autoregression</title>
      <link>https://osmin625.github.io/posts/Autoregression/</link>
      <pubDate>Wed, 28 Jun 2023 16:10:00 +0900</pubDate>
      <guid>https://osmin625.github.io/posts/Autoregression/</guid>
      <description>회귀 분석의 관점에서 과거의 데이터를 보고 현재 또는 미래의 결과를 예측하는 것&#xA;즉, Regression을 자기 자신에게 적용하는 것&#xA;$$&#xD;y_1, \ldots, y_n \rightarrow y_{n+1}&#xD;$$&#xA;$$&#xD;\mathrm{MSE}=\frac{1}{n} \sum_{i=1}^n\left(f\left(y_1, \ldots y_i\right)-y_{i+1}\right)^2&#xD;$$&#xA;종류 Moving Average(이동평균) 가장 간단한 방법&#xA;최신 트렌드를 반영하기 위해 최근 K개의 평균을 향후 예측에 활용한다. K의 값에 따라 경향성을 다르게 모델링할 수 있다. K가 커질수록 최신 트렌드의 반영 정도가 줄어든다. 평균 뿐만 아니라 다양한 형태로 Moving Average의 모델링이 가능하다.</description>
    </item>
    <item>
      <title>Book Rating Prediction Wrap-up Report</title>
      <link>https://osmin625.github.io/posts/Book-Rating-Prediction-Wrap-up-Report/</link>
      <pubDate>Fri, 26 May 2023 21:14:00 +0900</pubDate>
      <guid>https://osmin625.github.io/posts/Book-Rating-Prediction-Wrap-up-Report/</guid>
      <description>1. 프로젝트 개요 개요&#xA;본 프로젝트에서는 소비자들의 책 구매 결정에 도움을 주기 위해 개인화된 상품을 추천하는 문제를 해결하였습니다.&#xA;본 프로젝트에서는 크게 세 가지 작업을 수행하였습니다.&#xA;주어진 데이터셋을 바탕으로 데이터의 내용을 분석하고 시각화하는 과정을 통해 인사이트를 도출하였습니다. 주어진 모델 예제들을 변형하고 조합하여 다양한 모델에 대한 성능을 실험하였습니다. 앞서 명시한 작업들을 통해 최고 성능을 내는 모델을 발굴하였습니다. 본 프로젝트를 수행하면서 부스트코스 RecSys 트랙에서 학습하였던 머신러닝/딥러닝 모델을 이해하고 이를 PyTorch 구현하면서 실생활 문제에 적용하는 경험을 하였습니다.</description>
    </item>
    <item>
      <title>인기도 기반 추천이란?</title>
      <link>https://osmin625.github.io/posts/%EC%9D%B8%EA%B8%B0%EB%8F%84-%EA%B8%B0%EB%B0%98-%EC%B6%94%EC%B2%9C/</link>
      <pubDate>Tue, 11 Apr 2023 23:44:00 +0900</pubDate>
      <guid>https://osmin625.github.io/posts/%EC%9D%B8%EA%B8%B0%EB%8F%84-%EA%B8%B0%EB%B0%98-%EC%B6%94%EC%B2%9C/</guid>
      <description>간단 요약&#xA;가장 인기있는 아이템을 규칙을 기반으로 추천한다.&#xA;인기도의 척도&#xA;- 조회수, 평균 평점, 리뷰 개수, 좋아요/싫어요 수&#xA;예시 네이버 쇼핑 랭킹 순 다음 뉴스, 댓글 추천 레딧 Hot 추천 Score 계산 방법 Most Popular: 조회수가 가장 많은 아이템 최신성을 고려하지 않으면 한번 조회수가 높은 아이템이 계속 추천되게 된다.&#xA;Score Formula&#xA;가장 많이 조회된 뉴스를 추천하기&#xA;좋아요가 가장 많은 게시글을 추천하기&#xA;Hacker News Formula&#xA;뉴스 추천 서비스&#xA;$$&#xD;score = \frac{pageviews -1}{(age + 2)^{gravity}}&#xD;$$</description>
    </item>
    <item>
      <title>DeepFM</title>
      <link>https://osmin625.github.io/posts/DeepFM/</link>
      <pubDate>Tue, 04 Apr 2023 17:38:00 +0900</pubDate>
      <guid>https://osmin625.github.io/posts/DeepFM/</guid>
      <description>DeepFM: A Factorization-Machine based Neural Network for CTR Prediction&#xA;Wide &amp;amp; Deep 모델과 달리 두 요소(wide, deep)가 입력값을 공유하도록 한 end-to-end 방식의 논문&#xA;Background 추천 시스템에서는 implicit feature interaction을 학습하는 것이 중요하다.&#xA;예시) 식사 시간에 배달앱 다운로드 수 증가 (order-2 interaction)&#xA;10대 남성은 슈팅/RPG게임을 선호 (order-3 interaction)&#xA;기존 모델들은 low-나 high-order interaction 중 어느 한 쪽에만 강하다.&#xA;Wide &amp;amp; Deep 모델은 이 둘을 통합하여 문제 해결&#xA;하지만 wide component에 feature engineering(=Cross-Product Transformation)이 필요하다.</description>
    </item>
    <item>
      <title>WDN: Wide &amp; Deep Network</title>
      <link>https://osmin625.github.io/posts/WDN-Wide-Deep-Network/</link>
      <pubDate>Tue, 04 Apr 2023 17:38:00 +0900</pubDate>
      <guid>https://osmin625.github.io/posts/WDN-Wide-Deep-Network/</guid>
      <description>Wide &amp;amp; Deep Learning for Recommender Systems&#xA;선형적인 모델(Wide)과 비선형적인 모델(Deep)을 결합하여 기존 모델들의 장점을 모두 취하고자 한 논문&#xA;등장 배경 추천시스템에서 해결해야 할 두 가지 과제&#xA;Memorization — 학습데이터에 자주 등장하는 패턴은 모델이 암기해야 한다.&#xA;함께 빈번히 등장하는 아이템 혹은 특성(feature) 관계를 과거 데이터로부터 학습하는 것&#xA;Logistic Regression과 같은 선형 모델&#xA;대규모 추천 시스템 및 검색 엔진에서 사용해왔다. 확장 및 해석이 용이하다. 학습 데이터에 없는 feature 조합에 취약하다. Generalization — 학습데이터에 발생하지 않는 패턴을 적절하게 표현해야 한다.</description>
    </item>
    <item>
      <title>FFM: Field-aware Factorization Machine</title>
      <link>https://osmin625.github.io/posts/FFM-Field-aware-Factorization-Machine/</link>
      <pubDate>Mon, 03 Apr 2023 14:17:00 +0900</pubDate>
      <guid>https://osmin625.github.io/posts/FFM-Field-aware-Factorization-Machine/</guid>
      <description>개요 Field-aware Factorization Machines for CTR Prediction&#xA;FM의 변형된 모델인 FFM을 제안하여 더 높은 성능을 보인 논문 FM은 예측 문제에 두루 적용 가능한 모델로, 특히 sparse 데이터로 구성된 CTR 예측에서 좋은 성능을 보인다.&#xA;Field-aware Factorization Machine (FFM) FM을 발전시킨 모델&#xA;PITF 모델에서 아이디어를 얻었다.&#xA;PITF : Pairwise Interaction Tensor Factorization MF를 3차원으로 확장시킨 모델&#xA;PITF에서는 (user, item, tag) 3개의 필드에 대한 클릭률을 예측하기 위해&#xA;(user, item), (item, tag), (user, tag) 각각에 대해서 서로 다른 latent factor를 정의하여 계산</description>
    </item>
    <item>
      <title>FM: Factorization Machine</title>
      <link>https://osmin625.github.io/posts/FM-Factorization-Machine/</link>
      <pubDate>Mon, 03 Apr 2023 02:17:00 +0900</pubDate>
      <guid>https://osmin625.github.io/posts/FM-Factorization-Machine/</guid>
      <description>General Predictor에 Latent Factor Model을 추가한 모델.&#xA;Background Factorization Machines&#xA;SVM과 Factorization Model의 장점을 결합한 FM을 처음 소개한 논문&#xA;등장 배경&#xA;딥러닝이 등장하기 이전 SVM이 가장 많이 사용됐다.&#xA;매우 희소한 데이터가 많은 CF 환경에서는 SVM보다 MF 계열의 모델이 더 높은 성능을 내왔다.&#xA;SVM과 MF의 장점을 결합할 수 없을까? ⇒ FM 탄생.&#xA;MF 기반 모델의 한계 ⇒ User-Item 행렬 기반&#xA;즉, 특정 데이터 포맷에 특화되어 있다.&#xA;$X:$ (유저, 아이템) → $Y:$ (rating)으로 이루어진 데이터에 대해서만 적용이 가능하다.</description>
    </item>
    <item>
      <title>CBOW: Continous Bag of Word</title>
      <link>https://osmin625.github.io/posts/CBOW/</link>
      <pubDate>Fri, 31 Mar 2023 06:05:00 +0900</pubDate>
      <guid>https://osmin625.github.io/posts/CBOW/</guid>
      <description>Word2Vec을 학습하는 방법 중 하나.&#xA;앞뒤의 단어를 통해 중앙의 단어를 예측하는 방법&#xA;input — quick, brown, jumps, over output — fox 단어를 예측하기 위해 앞뒤로 몇 개의 단어(n)를 사용할지 정한다.&#xA;Multi-Class Classification&#xA;Input을 통해 One-Hot Vector의 각 원소가 0인지 1인지 예측한다.&#xA;학습 파라미터 $W_{V\times M}$: One-Hot Vector을 임베딩 벡터로 변환하는 행렬&#xA;$V$: 단어의 총 개수(One-Hot Vector의 크기) $M$: 임베딩 벡터의 크기 $W&#39;_{M\times V}$: 임베딩 벡터를 One-Hot Vector의 길이로 변환하는 행렬</description>
    </item>
    <item>
      <title>SG: Skip-Gram</title>
      <link>https://osmin625.github.io/posts/SG/</link>
      <pubDate>Fri, 31 Mar 2023 06:05:00 +0900</pubDate>
      <guid>https://osmin625.github.io/posts/SG/</guid>
      <description>Word2Vec을 학습하는 방법 중 하나.&#xA;CBOW가 뒤집어진 모델&#xA;CBOW와 입력층과 출력층이 반대로 구성되어 있다.&#xA;벡터의 평균을 구하는 과정이 없다.&#xA;CBOW보다 성능이 좋다고 알려져있다.&#xA;참고로 이 때의 성능은 학습 과정의 Loss가 아니라 임베딩 벡터의 표현력을 의미한다.&#xA;CBOW와 마찬가지로 Multi-Classification Model에 해당한다.</description>
    </item>
    <item>
      <title>SGNS: Skip-Gram with Negative Sampling</title>
      <link>https://osmin625.github.io/posts/SGNS/</link>
      <pubDate>Fri, 31 Mar 2023 06:05:00 +0900</pubDate>
      <guid>https://osmin625.github.io/posts/SGNS/</guid>
      <description>SG를 이진 분류 문제로 바꾼 모델&#xA;Negative Sampling 주변 단어가 아닌 단어를 Label 0으로 Sample에 포함시키는 것&#xA;Negative Sampling의 개수는 하이퍼파라미터에 해당한다.&#xA;학습 데이터가 적은 경우 5-20, 충분히 큰 경우 2-5가 적당하다. positive sample 하나당 k개 샘플링 중심 단어와 주변 단어가 각각 임베딩 벡터를 따로 가진다.&#xA;만약 input/output 혹은 word/context representation을 동일한 값으로 사용한다고 하면,&#xA;특정 단어, 가령 &amp;ldquo;dog&amp;quot;에 대해 P(dog|dog)가 현실적으로는 불가하지만 (한 문장에 &amp;ldquo;dog dog&amp;quot;를 연속으로 쓸 일은 없으니.</description>
    </item>
    <item>
      <title>MF: Matrix Factorization</title>
      <link>https://osmin625.github.io/posts/MF-Matrix-Factorization/</link>
      <pubDate>Thu, 30 Mar 2023 12:19:00 +0900</pubDate>
      <guid>https://osmin625.github.io/posts/MF-Matrix-Factorization/</guid>
      <description>간단 요약&#xA;행렬을 인수분해 후 재생성하여 결측치를 예측하는 방법&#xA;인수분해를 다양하게 시도하여 대상 행렬을 가장 잘 복구하는 최적의 하위행렬을 찾는 과정 {: .prompt-info } Latent Factors, Matrix Factorization, SGD&#xA;행렬을 인수분해 후 재생성하여 결측치를 예측하는 방법&#xA;대상 행렬을 두개의 하위 행렬로 분해한다.&#xA;User-Item 행렬을 User와 Item 각각에 대한 저차원의 latent factor 행렬로 분해한다.&#xA;두 하위 행렬을 다시 곱해서 대상 행렬과 동일한 크기의 단일 행렬로 만든다.&#xA;위의 과정에서 기존 행렬의 빈공간이 채워진다.</description>
    </item>
    <item>
      <title>SVD: Singular Value Decomposition(특이값 분해)</title>
      <link>https://osmin625.github.io/posts/SVD-Singular-Value-Decomposition%ED%8A%B9%EC%9D%B4%EA%B0%92-%EB%B6%84%ED%95%B4/</link>
      <pubDate>Thu, 30 Mar 2023 00:19:00 +0900</pubDate>
      <guid>https://osmin625.github.io/posts/SVD-Singular-Value-Decomposition%ED%8A%B9%EC%9D%B4%EA%B0%92-%EB%B6%84%ED%95%B4/</guid>
      <description>간단 요약&#xA;2차원 행렬을 두 개의 잠재요인 행렬과 하나의 대각행렬로 분해하는 기법 {: .prompt-info } eigen vector, eigen value&#xA;2차원 행렬 분해 기법 유저 잠재요인 행렬 ⇒ 유저 임베딩 잠재요인 대각행렬 ⇒ 임베딩의 중요도 아이템 잠재요인 행렬 ⇒ 아이템 임베딩 차원축소 기법 행렬을 대각화하는 방법 모든 m x n 행렬에 대해 적용 가능 Rating Matrix $R$ 에 대해 유저와 아이템의 잠재 요인을 포함할 수 있는 행렬로 분해한다.&#xA;Full SVD 기존 행렬을 온전하게 3개의 행렬로 분해한다.</description>
    </item>
    <item>
      <title>NBCF: Neighborhood-based CF(이웃 기반 협업 필터링)</title>
      <link>https://osmin625.github.io/posts/nbcf-neighborhood-based-cf/</link>
      <pubDate>Wed, 29 Mar 2023 14:03:00 +0900</pubDate>
      <guid>https://osmin625.github.io/posts/nbcf-neighborhood-based-cf/</guid>
      <description>Neighborhood-based CF 혹은 Memory Based CF라고 부르기도 한다.&#xA;사용자 또는 아이템 간의 similarity 값을 계산하고 이를 rating prediction 또는 top-K ranking에 활용하는 방법&#xA;Similarity를 계산하기 위한 Metric으로 Jaccard, Cosine, Pearson 등을 활용한다.&#xA;장점&#xA;구현이 간단하고 이해하기 쉽다. Similarity를 활용하기 때문에 추천의 이유에 대한 직관적인 설명을 제공한다. 최적화나 훈련 과정이 필요 없다. 단점&#xA;Sparsity(희소성) 문제&#xA;NBCF를 적용하려면 적어도 sparsity ratio가 99.5%를 넘지 않는 것이 좋다.&#xA;데이터가 충분하지 않다면 유사도 계산이 부정확해진다. ⇒ 성능 저하</description>
    </item>
    <item>
      <title>연관 분석(Association Analysis) 정리</title>
      <link>https://osmin625.github.io/posts/%EC%97%B0%EA%B4%80-%EB%B6%84%EC%84%9D/</link>
      <pubDate>Tue, 28 Mar 2023 08:31:00 +0900</pubDate>
      <guid>https://osmin625.github.io/posts/%EC%97%B0%EA%B4%80-%EB%B6%84%EC%84%9D/</guid>
      <description>연관 규칙 분석(Association Rule Analysis) 추천 시스템의 가장 고전적인 방법론&#xA;장바구니 분석, 서열 분석이라고도 불린다.&#xA;상품의 구매, 조회 등 하나의 연속된 거래들 사이의 규칙을 발견하기 위해 적용하는 방법&#xA;즉, 사용자의 장바구니 내에 포함된 상품들의 규칙을 발견하기 위해 적용하는 방법&#xA;유저 정보(유저 행동 정보)를 활용하는 분석 방법&#xA;규칙&#xA;IF {condition} THEN {result}&#xA;{condition} → {result}&#xA;연관 규칙&#xA;규칙 가운데 일부 기준(빈번함의 기준)을 만족하는 것&#xA;IF {antecedent} THEN {consequent}&#xA;빈번하게 발생하는 규칙을 의미한다.</description>
    </item>
    <item>
      <title>추천 시스템 평가 패러다임</title>
      <link>https://osmin625.github.io/posts/%EC%B6%94%EC%B2%9C-%EC%8B%9C%EC%8A%A4%ED%85%9C-%ED%8F%89%EA%B0%80-%EC%A7%80%ED%91%9C/</link>
      <pubDate>Mon, 27 Mar 2023 11:15:00 +0900</pubDate>
      <guid>https://osmin625.github.io/posts/%EC%B6%94%EC%B2%9C-%EC%8B%9C%EC%8A%A4%ED%85%9C-%ED%8F%89%EA%B0%80-%EC%A7%80%ED%91%9C/</guid>
      <description>MAP, NDCG&#xA;비즈니스 / 서비스 관점&#xA;추천 시스템 적용으로 인해 매출, PV(Page View) 증가&#xA;추천 아이템으로 인해 유저 CTR(노출 대비 클릭)의 상승&#xA;품질 관점&#xA;정확성(Accuracy)&#xA;연관성(Relevance): 추천된 아이템이 유저에게 관련이 있는가?&#xA;다양성(Diversity): 추천된 Top-K 아이템에 얼마나 다양한 아이템이 추천되는가?&#xA;신뢰성(Confidence) : 추천 결과를 제공하는 시스템이 신뢰할 만한가?&#xA;표준편차가 적은 추천 시스템일수록 더 높은 Confidence를 가진다.&#xA;신뢰성(Trust) : 사용자가 추천 결과에 얼마나 믿음을 가지는가?&#xA;추천 결과에 설명이 추가된다면 사용자가 추천 결과를 더 믿게 된다.</description>
    </item>
    <item>
      <title>추천 시스템이란?</title>
      <link>https://osmin625.github.io/posts/%EC%B6%94%EC%B2%9C-%EC%8B%9C%EC%8A%A4%ED%85%9C/</link>
      <pubDate>Mon, 27 Mar 2023 10:36:00 +0900</pubDate>
      <guid>https://osmin625.github.io/posts/%EC%B6%94%EC%B2%9C-%EC%8B%9C%EC%8A%A4%ED%85%9C/</guid>
      <description>정보 필터링(IF) 기술의 일종. 특정 사용자가 관심 가질 만한 정보를 추천하는 것. Background 기존&#xA;유저가 원하는 것을 검색하여 이에 맞는 아이템 결과를 보여주는 Pull 방식&#xA;추천 시스템&#xA;유저가 원하는 것을 유추하여 제시하는 Push 방식&#xA;유저가 자신의 니즈를 쿼리로 표현하지 않아도 된다.&#xA;다양한 종류의 아이템들을 유저에게 노출시킬 수 있다.&#xA;추천 시스템의 필요성&#xA;과거에는 유저가 접할 수 있는 상품, 컨텐츠가 제한적&#xA;TV 채널, 영화관, 백화점, 신문 등&#xA;웹/모바일 환경에 의해 다양한 상품, 컨텐츠 등장 → 정보 과다.</description>
    </item>
    <item>
      <title>모델에 데이터를 먹이는 방법(PyTorch Datasets &amp; DataLoaders)</title>
      <link>https://osmin625.github.io/posts/PyTorch-Datasets-DataLoaders/</link>
      <pubDate>Tue, 14 Mar 2023 10:13:00 +0900</pubDate>
      <guid>https://osmin625.github.io/posts/PyTorch-Datasets-DataLoaders/</guid>
      <description>모델에 데이터를 먹이는 방법&#xA;1. Dataset 모아놓은 데이터에 대해 Dataset이라는 클래스를 통해 시작, 길이, mapstyle 등을 선언해준다.&#xA;__getitems__() : 하나의 데이터를 불러올 때 어떤 식으로 데이터를 반환할 지를 선언해준다.&#xA;데이터 입력 형태를 정의하는 클래스&#xA;데이터를 입력하는 방식의 표준화&#xA;Image, Text, Audio 등에 따라 다르게 입력이 정의된다.&#xA;데이터의 형태에 따라 각 함수를 다르게 정의한다.&#xA;모든 것을 데이터 생성 시점에 처리할 필요는 없다.&#xA;image의 Tensor 변화는 학습에 필요한 시점에 변환해주면 된다.&#xA;데이터 셋에 대한 표준화된 처리 방법 제공이 필요하다.</description>
    </item>
    <item>
      <title>PyTorch의 Backward에 대해 알아보자.</title>
      <link>https://osmin625.github.io/posts/Backward/</link>
      <pubDate>Mon, 13 Mar 2023 22:13:00 +0900</pubDate>
      <guid>https://osmin625.github.io/posts/Backward/</guid>
      <description>✔️ 간단 요약&#xA;forward함수를 정의하면 자동으로 정의된다.&#xA;동작 과정&#xA;tensor(loss에 해당)가 포함된 식을 미분한다. 미분 값을 tensor에 저장한다.&#xA;{: .prompt-info } Autograd, loss, optimizer, nn.Module tensor(loss에 해당)가 포함된 식을 미분한다.&#xA;Tensor 객체의 backward 함수에는 default로 Autograd 설정이 되어 있기 때문에, 미분 수식을 따로 작성하지 않아도 자동으로 미분이 가능하다.&#xA;기본 예제&#xA;w = torch.tensor(2.0, requires_grad=True) y = w**2 z = 10*y + 25 z.backward() w.grad() # output : tensor(40.) $$&#xD;w = 2\\&#xD;y = w^2\\&#xD;z = 10\times y + 25\\&#xD;z = 10 \times w^2 + 25\\&#xD;{dz\over dw} = 20 \times w = 40&#xD;$$</description>
    </item>
    <item>
      <title>PyTorch 모델 정의하기 - nn.Module</title>
      <link>https://osmin625.github.io/posts/PyTorch-%EB%AA%A8%EB%8D%B8-%EC%A0%95%EC%9D%98%ED%95%98%EA%B8%B0/</link>
      <pubDate>Mon, 13 Mar 2023 22:09:00 +0900</pubDate>
      <guid>https://osmin625.github.io/posts/PyTorch-%EB%AA%A8%EB%8D%B8-%EC%A0%95%EC%9D%98%ED%95%98%EA%B8%B0/</guid>
      <description>간단 요약&#xA;반복되는 Layer을 만들기 위한 Torch의 가장 기본적인 신경망 모듈&#xA;매개변수를 캡슐화하는 간편한 방법&#xA;GPU로 이동, 내보내기(exporting), 불러오기(loading) 등의 작업을 위한 헬퍼(helper)를 제공한다.&#xA;{: .prompt-info }&#xA;DL 모델은 모두 Layer의 반복이며, 블록 반복의 연속이다.&#xA;Module에서 정의하는 것&#xA;Input&#xA;Output&#xA;Forward&#xA;(Backward)&#xA;이 때, Backward는 자동 미분이 되기 때문에, 해당되는 weight의 값들을 내보내준다.&#xA;즉, weight가 학습의 대상이 되고, 이를 parameter(tensor)로 정의한다.&#xA;일반적으로는 직접 지정해줄 필요가 없다.&#xA;example import torch from torch.</description>
    </item>
    <item>
      <title>PyTorch에서 weight를 저장하는 객체 - nn.Parameter</title>
      <link>https://osmin625.github.io/posts/PyTorch-nn-Parameter/</link>
      <pubDate>Mon, 13 Mar 2023 22:09:00 +0900</pubDate>
      <guid>https://osmin625.github.io/posts/PyTorch-nn-Parameter/</guid>
      <description>간단 요약&#xA;학습의 대상이 되는 Weight를 정의한다.&#xA;Tensor 객체의 상속 객체 {: .prompt-info }&#xA;Tensor 객체와 매우 비슷하다.&#xA;nn.Module의 attribute가 될 때는 required_grad = True로 자동으로 지정되어 AutoGrad의 대상이 된다.&#xA;대부분의 Layer에는 weights 값들이 지정되어 있기 때문에, 직접 지정할 일은 드물다. 그래도 직접 지정하는 법을 알아보자.&#xA;nn.Module로 만든 $\tt xw +b$라는 선형 모델을 살펴본다.&#xA;$\tt xw +b$ class MyLinear(nn.Module): def __init__ (self, in_features, out_features, bias=True): super(). init () **self.in_features = in_features self.</description>
    </item>
    <item>
      <title>Tensor에 대해 알아보자.</title>
      <link>https://osmin625.github.io/posts/Tensor/</link>
      <pubDate>Mon, 13 Mar 2023 10:44:00 +0900</pubDate>
      <guid>https://osmin625.github.io/posts/Tensor/</guid>
      <description>간단 요약&#xA;autograd 연산을 지원하는 다차원 배열&#xA;tensor에 대한 미분값을 가진다.&#xA;reshape보다 view를 쓰는 것이 좋다. squeeze와 unsqueeze의 차이 mm, dot, matmul 차이&#xA;{: .prompt-info } 신경망의 가중치(매개변수)를 텐서로 표현한다.&#xA;다차원 Arrays를 표현하는 PyTorch 클래스&#xA;numpy의 ndarray와 호환된다.&#xA;TensorFlow의 Tensor와도 동일&#xA;Tensor을 생성하는 함수도 거의 동일&#xA;numpy — ndarray&#xA;import numpy as np n_array = np.arange(10).reshape(2,5) print(n_array) print(&amp;#34;n_dim :&amp;#34;, n_array.ndim, &amp;#34;shape :&amp;#34;, n_array.shape) pytorch — tensor&#xA;import torch t_array = torch.</description>
    </item>
    <item>
      <title>PyTorch 개요</title>
      <link>https://osmin625.github.io/posts/Pytorch-overview/</link>
      <pubDate>Mon, 13 Mar 2023 09:58:00 +0900</pubDate>
      <guid>https://osmin625.github.io/posts/Pytorch-overview/</guid>
      <description>Naver BoostCamp AI Tech에서 학습한 내용을 재구성했습니다.&#xA;해당 게시글은 지속적으로 업데이트할 예정입니다.&#xA;{: .prompt-info }&#xA;구현 개요(PyTorch) 1. 데이터 준비 Tensor PyTorch Datasets &amp;amp; DataLoaders 2. 모델 정의 (torch.nn.Module) PyTorch 모델 불러오기&#xA;Input size, Output size 정의 nn.Parameter Forward 연산 정의 Backward 연산 정의 3. 하이퍼 파라미터 지정 Hyperparameter Tuning 4. 모델 평가 기준 및 Optimizer 설정 모델 평가 기준 : loss를 어떻게 계산할 것인가? 손실 함수(Loss Function) Optimizer 설정 5.</description>
    </item>
    <item>
      <title>PyTorch 모델 저장하고 불러오기</title>
      <link>https://osmin625.github.io/posts/PyTorch-%EB%AA%A8%EB%8D%B8-%EB%B6%88%EB%9F%AC%EC%98%A4%EA%B8%B0/</link>
      <pubDate>Mon, 13 Mar 2023 09:58:00 +0900</pubDate>
      <guid>https://osmin625.github.io/posts/PyTorch-%EB%AA%A8%EB%8D%B8-%EB%B6%88%EB%9F%AC%EC%98%A4%EA%B8%B0/</guid>
      <description>model.save() 학습의 결과를 저장하기 위한 함수&#xA;모델 형태(architecture)와 파라미터를 저장&#xA;모델 학습 중간 과정의 저장을 통해 최선의 결과 모델을 선택&#xA;만들어진 모델을 외부 연구자와 공유하여 학습 재연성 향상&#xA;# Print model&amp;#39;s state_dict print(&amp;#34;Model&amp;#39;s state_dict:&amp;#34;) # state dict: 모델의 파라미터를 표시 for param_tensor in model.state_dict(): print(param_tensor, &amp;#34;\t&amp;#34;, model.state_dict()[param_tensor].size()) ## 방법 1. # 모델의 파라미터만 저장하기 **torch.save**(model.**state_dict()**, os.path.join(MODEL_PATH, &amp;#34;model.pt&amp;#34;)) # 모델은.pt 파일로 저장한다. # dict type으로 저장된다. new_model = TheModelClass() # 모델의 Architecture가 동일한 경우 파라미터만 저장하고 불러온다.</description>
    </item>
    <item>
      <title>모델의 성능이 더이상 오르지 않을 때 (Hyper-Parameter Tuning)</title>
      <link>https://osmin625.github.io/posts/Hyperparameter_tuning/</link>
      <pubDate>Mon, 13 Mar 2023 09:58:00 +0900</pubDate>
      <guid>https://osmin625.github.io/posts/Hyperparameter_tuning/</guid>
      <description>하이퍼 파라미터&#xA;모델 스스로 학습하지 않는 값.&#xA;사람이 직접 지정해주어야 한다.&#xA;결과를 개선하고 싶을 때 모델을 바꾸기&#xA;중요하지만, 이미 높은 성능의 모델이 공개되어있기 때문에 상대적으로 덜 중요.&#xA;데이터를 바꾸기 → 성능 개선을 위해 가장 중요하다.&#xA;하이퍼 파라미터 Tuning&#xA;약간의 성능 개선이 간절한 경우 수행한다.&#xA;마지막 0.01의 성능 개선이라도 필요한 경우 사용한다.&#xA;generalization 등 적용&#xA;Hyperparameter Tuning 가장 기본적인 방법 - grid vs random&#xA;grid&#xA;적절한 하이퍼파라미터를 찾을 때, 값들을 일정한 범위를 정해 선택하는 것.</description>
    </item>
    <item>
      <title>나를 위한 Docker 총정리</title>
      <link>https://osmin625.github.io/posts/docker/</link>
      <pubDate>Mon, 13 Feb 2023 13:00:00 +0900</pubDate>
      <guid>https://osmin625.github.io/posts/docker/</guid>
      <description>눈송이 서버 가상화 VM(Virtual Machine) Container Docker Image Docker Container Background — 왜 도커를 써야하나? 눈송이 서버들(Snowflake Servers)&#xA;모든 눈송이의 모양이 다르듯, 서버들도 모두 다르다.&#xA;서버가 여러개인 경우, 서버 구성 시기에 따라 운영체제, 컴파일러, 설치된 패키지 등의 버전이 달라지게 된다.&#xA;눈송이 서버를 방지하기 위해, 새로 서버를 구성할 때 다양한 방식으로 서버 운영 기록을 저장해 둔다.&#xA;가상화&#xA;특정 소프트웨어 환경을 만든 후, Local 서버나 Production 서버에서 그대로 활용하는 방법.&#xA;어느 운영체제에서나 동일한 환경으로 프로그램 실행 가능</description>
    </item>
    <item>
      <title>1 x 1 Convolution</title>
      <link>https://osmin625.github.io/posts/1x1-Convolution/</link>
      <pubDate>Fri, 13 Jan 2023 15:39:00 +0900</pubDate>
      <guid>https://osmin625.github.io/posts/1x1-Convolution/</guid>
      <description>차원 축소를 위해 활용한다.&#xA;이미지에서 단 하나의 픽셀만 보기 때문에, 이미지에서 영역을 살펴보는 의미는 없다.&#xA;다만, 1 * 1 Convolution을 이용하여 기존 Spatial Dimension을 그대로 유지한 채, 채널의 개수를 128개에서 32개로 줄인다.&#xA;![1x1 Conv_1](/imgs/1x1 Conv_1.png)&#xA;이를 통해 NN의 층을 더 깊게 쌓으면서도, 채널의 수를 줄여서 파라미터의 수를 줄일 수 있다.&#xA;1 * 1 Convolution을 사용하지 않는 경우&#xA;![1x1 Conv_2](/imgs/1x1 Conv_2.png)&#xA;1 * 1 Convolution을 사용한 경우&#xA;![1x1 Conv_3](/imgs/1x1 Conv_3.png)&#xA;파라미터 수를 147,456개에서 40,960개로 효과적으로 줄일 수 있다.</description>
    </item>
    <item>
      <title>Alexnet 모델의 파라미터 수 계산</title>
      <link>https://osmin625.github.io/posts/Alexnet-%EB%AA%A8%EB%8D%B8%EC%9D%98-%ED%8C%8C%EB%9D%BC%EB%AF%B8%ED%84%B0-%EC%88%98-%EA%B3%84%EC%82%B0%ED%95%B4%EB%B3%B4%EA%B8%B0/</link>
      <pubDate>Fri, 13 Jan 2023 15:15:00 +0900</pubDate>
      <guid>https://osmin625.github.io/posts/Alexnet-%EB%AA%A8%EB%8D%B8%EC%9D%98-%ED%8C%8C%EB%9D%BC%EB%AF%B8%ED%84%B0-%EC%88%98-%EA%B3%84%EC%82%B0%ED%95%B4%EB%B3%B4%EA%B8%B0/</guid>
      <description>Conv Layer Layer 1 파라미터 수 = 11 * 11 * 3 * 48 * 2 ⇒ 35k&#xA;입력 : 224 * 224 * 3&#xA;filter : 11 * 11 * (3)&#xA;3은 생략되어 있지만, 입력 크기와 동일한 채널을 가질 것이기 때문에 3으로 유추할 수 있다.&#xA;모델 이미지상 커널이 위 아래로 두 개이기 때문에 * 2를 했다.&#xA;gpu 메모리 용량 등의 이유로 이처럼 구성하는 경우가 많다.&#xA;Layer 2 파라미터 수 = 5 * 5 * 48 * 128 * 2 ⇒ 307k</description>
    </item>
    <item>
      <title>Kullback-Leibler (KL) Divergence</title>
      <link>https://osmin625.github.io/posts/kullback_leibler_kl_divergence/</link>
      <pubDate>Mon, 12 Dec 2022 12:34:00 +0900</pubDate>
      <guid>https://osmin625.github.io/posts/kullback_leibler_kl_divergence/</guid>
      <description>간단 요약&#xA;Cross Entropy - Entropy&#xA;두 분포의 차이, 정보량을 의미한다.&#xA;metric이 아니다.&#xA;엔트로피의 상대성에 대해 이야기한다.&#xA;유도 과정&#xA;$$&#xD;\begin{aligned}H(p, q) &amp;amp; =-\sum_i p_i \log q_i \\&amp;amp; =-\sum_i p_i \log q_i-\sum_i p_i \log p_i+\sum_i p_i \log p_i \\&amp;amp; =H(p)+\sum_i p_i \log p_i-\sum_i p_i \log q_i \\&amp;amp; =H(p)+\sum_i p_i \log \frac{p_i}{q_i}\end{aligned}&#xD;$$&#xA;이 때, $H(p,q) - H(p)$로 정리되는 다음 수식을 KL-Divergence 혹은 Relative Entropy라고 부른다.&#xA;$$&#xD;\sum_i p_i \log \frac{p_i}{q_i}=H(p, q)-H(p)&#xD;$$</description>
    </item>
    <item>
      <title>LSTM: Long Short Term Memory</title>
      <link>https://osmin625.github.io/posts/LSTM/</link>
      <pubDate>Sun, 04 Dec 2022 02:11:00 +0900</pubDate>
      <guid>https://osmin625.github.io/posts/LSTM/</guid>
      <description>RNN의 장기문맥 의존성을 해결하기 위해 탄생한 모델&#xA;선별적 게이트라는 개념으로 선별 기억 능력을 확보한다.&#xA;그림은 이해를 돕기위해 O,X로 표현했지만, 실제로는 게이트는 0~1 사이의 실수값으로 열린 정도를 조절한다.&#xA;게이트의 여닫는 정도는 가중치로 표현되며 가중치는 학습으로 알아낸다.&#xA;가중치&#xA;순환 신경망의 $\{U, V, W\}$에 4개를 추가하여 $\{U, U_i , U_o , W, W_i , W_o , V\}$&#xA;$i$ : 입력 게이트&#xA;$o$ : 출력 게이트&#xA;다양한 구조 설계가 가능하다.&#xA;Model Concept Cell State LSTM의 핵심</description>
    </item>
    <item>
      <title>Optimizer란?</title>
      <link>https://osmin625.github.io/posts/Optimizer/</link>
      <pubDate>Sat, 03 Dec 2022 13:05:00 +0900</pubDate>
      <guid>https://osmin625.github.io/posts/Optimizer/</guid>
      <description>간단 요약&#xA;Loss의 미분값을 파라미터에 어떻게 반영할 지에 대한 방법&#xA;반영 방법 : loss의 미분 값을 파라미터에 어떻게 반영할 것인가? Learning rate : 한 번에 얼마나 반영할 것인가?&#xA;{: .prompt-info } Background: Gradient Descent(GD)에서의 Issue 1. Local minima, Saddle point {: w=&amp;ldquo;700&amp;rdquo; h=&amp;ldquo;400&amp;rdquo; }&#xA;실제로는 Local Minima보단 안장점(saddle point)이 문제인 경우가 더 많다.&#xA;local minima가 되기 위해선 모든 변수 방향에서 loss가 증가해야 하는데, 이는 흔치 않다.&#xA;{: w=&amp;ldquo;400&amp;rdquo; h=&amp;ldquo;250&amp;rdquo; }</description>
    </item>
    <item>
      <title>손실 함수(Loss Function)에 대해 알아보자.</title>
      <link>https://osmin625.github.io/posts/Loss-function/</link>
      <pubDate>Sat, 19 Nov 2022 00:04:00 +0900</pubDate>
      <guid>https://osmin625.github.io/posts/Loss-function/</guid>
      <description>✔️ 간단 요약&#xA;신경망의 학습 중 받는 벌점의 기준&#xA;회귀와 분류 문제에서 다른 loss function을 사용한다.&#xA;{: .prompt-info } Gradient, MAE, MSE, RMSE&#xA;Loss : 예측 값과 실제 값의 차이&#xA;신경망의 학습 중 오답에 대해 받는 벌점&#xA;두 값의 차이는 단순히 뺄셈의 절댓값을 의미하는 것은 아니며, 상황에 따라 다양하게 나타난다.&#xA;ex) 정답과 완전히 동떨어진 대답을 하면 더 많은 벌점을 받는다.&#xA;Loss Function 신경망이 벌점을 받는 기준&#xA;신경망의 학습 과정에서 가중치 $\mathbf w$를 평가하는 함수.</description>
    </item>
    <item>
      <title>MICE : Multiple Imputation Chained Equation</title>
      <link>https://osmin625.github.io/posts/MICE/</link>
      <pubDate>Tue, 18 Oct 2022 07:41:00 +0900</pubDate>
      <guid>https://osmin625.github.io/posts/MICE/</guid>
      <description>Multiple Imputation Chained Equation(다중 산입 연립 방정식)&#xA;MICE 접근 방식에는 MI에서 언급된 동일한 개념이 적용된다.&#xA;값들은 각 방식에 따라 산입된 후 완전한 데이터셋에 대한 분석이 진행되고 결과가 합쳐진다. 다만 차이점으로, MI에서는 모든 변수에 대해 동시에 산입되지만, MICE에서는 각 변수의 값이 순차적으로 산입된다.&#xA;Process 누락된 데이터의 양이 가장 적은 변수가 가장 먼저 산입된다.&#xA;가장 첫 변수는 mean replacement(평균 대체) 방법으로 채워진다.&#xA;이후, 채워진 변수는 다른 변수를 채울 때 함께 예측 변수로 사용된다.</description>
    </item>
    <item>
      <title>나를 위한 Git Branch 총정리</title>
      <link>https://osmin625.github.io/posts/git_branch/</link>
      <pubDate>Sat, 08 Oct 2022 22:49:00 +0900</pubDate>
      <guid>https://osmin625.github.io/posts/git_branch/</guid>
      <description>독립적으로 특정 작업을 진행할 때 사용한다.&#xA;팀으로 여러작업 동시에 작업 가능&#xA;여러 Branch를 합치는 방법 — Merge&#xA;merge 방법에 따른 차이 merge — branch의 모든 기록 보존, merge에 대한 기록 추가. 이 방법을 사용하면 merge에 대한 commit이 하나 생성되고 어느 시점에 merge를 진행했는지 쉽게 알 수 있다. branch가 늘어나고 여러 번의 merge가 생기게 되면, 그래프가 복잡해져 **커밋 히스토리(Commit History)**를 파악하기 더욱 어려워질 수 있다. squash and merge — 여러 commit 기록 하나로 합치기, merge기록 남기지 않기.</description>
    </item>
    <item>
      <title>나를 위한 Git Flow 총정리</title>
      <link>https://osmin625.github.io/posts/git_flow/</link>
      <pubDate>Wed, 28 Sep 2022 11:41:00 +0900</pubDate>
      <guid>https://osmin625.github.io/posts/git_flow/</guid>
      <description>Local Repository 중심 Branch 관리 방법론&#xA;기능이나 프로그램이 아닌, 개발자간의 약속&#xA;물론 이를 지원하는 모듈이 존재한다.&#xA;$ apt-get install git-flow 장점&#xA;신중하다. 단점&#xA;복잡하다. 배포할 때 거쳐야 할 단계가 많다. 단계별로 관리하는 사람이 다른 경우, 특정 지점에서 병목이 발생할 수 있다. 총 5가지의 branch를 사용해서 개발이 진행된다.&#xA;master&#xA;배포의 기준이 되는 branch.&#xA;Release Tag를 기록하는 branch.&#xA;제품을 배포하는 용도.&#xA;배포용 브랜치이므로 개발자가 해당 브랜치에 직접 커밋하거나, Release 이외의 branch에서 직접 Merge할 일이 없다.</description>
    </item>
    <item>
      <title>경사 하강법에 오차 역전파가 없다면 무슨 일이 일어날까?</title>
      <link>https://osmin625.github.io/posts/%EC%88%98%EC%B9%98-%EB%AF%B8%EB%B6%84/</link>
      <pubDate>Fri, 27 May 2022 20:54:00 +0900</pubDate>
      <guid>https://osmin625.github.io/posts/%EC%88%98%EC%B9%98-%EB%AF%B8%EB%B6%84/</guid>
      <description>손실 함수, Gradient Descent, Back Propagation&#xA;수치 미분 한 점에서의 기울기. 변화량을 의미한다.&#xA;경사 하강법을 사용하기 위해서는 미분값이 필요하다.&#xA;$$&#xD;{df(x)\over dx} = \lim_{h \to 0} {f(x+h) - f(x)\over h}&#xD;$$&#xA;수치 미분이 경사 하강법에 사용되는 방법 경사 하강법에서는 $f(x)$가 손실 함수이고, x가 현재의 가중치나 편향이 된다.&#xA;손실 함수는 대상 값과 예측 값의 오차를 의미하므로,&#xA;손실 함수에 대한 미분 값을 구한 후, 오차를 줄이는 방향으로 가중치와 편향을 수정할 수 있다.</description>
    </item>
    <item>
      <title>Back Propagation(오차 역전파 알고리즘)</title>
      <link>https://osmin625.github.io/posts/backpropagation/</link>
      <pubDate>Fri, 22 Apr 2022 23:21:00 +0900</pubDate>
      <guid>https://osmin625.github.io/posts/backpropagation/</guid>
      <description>이 알고리즘으로 인해 ML Network에서의 학습이 가능하다는 것이 알려져, 암흑기에 있던 신경망 학계가 다시 관심을 받게 되었다.&#xA;출력층에서 시작하여 역방향으로 오류를 전파한다는 뜻에서 오류 역전파라 부른다.&#xA;내가 뽑고자 하는 target값과 실제 모델이 계산한 출력의 차이를 계산한다. 오차값을 다시 뒤로 전파해가면서 각 노드가 가지고 있는 변수들을 갱신한다. 직관적인 이해는 끝났다. 이제 제대로 이해해보자.&#xA;오차 역전파가 중요한 이유를 알고 싶다면, 여기를 클릭하여 오차 역전파가 없을 시에 발생하는 문제점을 이해하자.&#xA;오차 역전파 신경망을 학습하는 방법.</description>
    </item>
    <item>
      <title>손실 함수에서 최적 해를 찾는 방법: Gradient Descent(경사 하강법)</title>
      <link>https://osmin625.github.io/posts/Gradient_Descent/</link>
      <pubDate>Fri, 22 Apr 2022 23:21:00 +0900</pubDate>
      <guid>https://osmin625.github.io/posts/Gradient_Descent/</guid>
      <description>손실 함수, 확률적 경사 하강법, 수치 미분, 배치 모드, 미니 배치 모드, 패턴 모드, Local Minima, Global Minima, Optimizer&#xA;Gradient Descent(경사하강법) 자연 과학과 공학에서 오랫동안 사용해온 최적화 방법&#xA;손실 함수의 최적 해를 찾기 위한 방법&#xA;1차 근삿값 발견을 위한 최적화 알고리즘&#xA;미분 값 $\partial J\over\partial w_1$의 반대 방향이 최적 해에 접근하는 방향이다.&#xA;따라서, 현재 가중치 $w_1$에 $-{\partial J\over\partial w_1}$을 더하면 최적 해에 가까워진다.&#xA;굳이 가까워질 필요 없이, 손실 함수를 미분해서 바로 극값을 찾으면 되지 않을까?</description>
    </item>
    <item>
      <title>Ranked Retreival 모델 구현(TF-IDF)</title>
      <link>https://osmin625.github.io/posts/TF-IDF-Implement/</link>
      <pubDate>Tue, 01 Mar 2022 12:37:00 +0900</pubDate>
      <guid>https://osmin625.github.io/posts/TF-IDF-Implement/</guid>
      <description>우선, 시작하기에 앞서 corpus 구성을 확인했다.&#xA;한글로 작성되었으며, 제목과 문서 내용으로 구성되어있는 것을 확인했다.&#xA;일단 데이터를 처리하기 위해 코드에서 파일을 열어야 하는데, 문서가 한글파일로 제공되었기 때문에 한글 문서를 txt 문서로 변환해주었다.&#xA;{: w=&amp;ldquo;200&amp;rdquo; h=&amp;ldquo;100&amp;rdquo; }&#xA;언어는 파이썬을 선택했다.&#xA;처음 프로젝트를 시작했을 때, 나는 colab 환경에서 파이썬 코드를 실행시키고자 했으므로 Google Drive에 corpus 파일을 업로드하고, 코드 작성을 시작했다.&#xA;수업 내용에서는 영어를 기준으로 다뤄왔었는데 한글을 토큰화하는 방법이 떠오르지 않았다.&#xA;다행스럽게도 한글 형태소 분석 라이브러리 Konlpy가 있어서 이를 활용하고자 했다.</description>
    </item>
    <item>
      <title>문서의 순위를 매기는 방법, TF-IDF</title>
      <link>https://osmin625.github.io/posts/TF-IDF/</link>
      <pubDate>Mon, 28 Feb 2022 20:41:00 +0900</pubDate>
      <guid>https://osmin625.github.io/posts/TF-IDF/</guid>
      <description>Background 이때까지 우리의 쿼리는 모두 Boolean을 활용한 것이었다.&#xA;Boolean은 자신들의 원하는 검색결과를 정확하게 아는 전문가들이 사용하기에 좋다.&#xA;또한 많은 정보를 탐색하는 전문가들에게 유용하다.&#xA;하지만 대부분의 사람들이 편하게 사용하기엔 적절하지 않다.&#xA;대부분의 유저들은 boolean 쿼리를 작성하지 못한다.&#xA;사용자들은 그렇게 많은 결과물이 필요하지 않다.&#xA;대부분 Boolean query는 문서가 너무 많이 나오거나, 너무 적게 나온다.&#xA;Query 1: “standard user dlink 650” → 200,000 hits&#xA;Query 2: “standard user dlink 650 no card found”: 0 hits</description>
    </item>
    <item>
      <title>1. 프로젝트 식별 및 관리</title>
      <link>https://osmin625.github.io/posts/%ED%94%84%EB%A1%9C%EC%A0%9D%ED%8A%B8-%EC%8B%9D%EB%B3%84-%EB%B0%8F-%EA%B4%80%EB%A6%AC/</link>
      <pubDate>Thu, 23 Dec 2021 16:22:00 +0900</pubDate>
      <guid>https://osmin625.github.io/posts/%ED%94%84%EB%A1%9C%EC%A0%9D%ED%8A%B8-%EC%8B%9D%EB%B3%84-%EB%B0%8F-%EA%B4%80%EB%A6%AC/</guid>
      <description>프로젝트를 어떻게 시작하느냐에 대한 관점&#xA;프로젝트를 시작하기 전, 어떻게 평가할 지, 그리고 어떻게 계획을 세울지를 설명한다.&#xA;프로젝트 식별 1. System Request 작성 Project Sponsor&#xA;프로젝트의 아이디어 제공자&#xA;프로젝트의 금전적 지원자 아님.&#xA;Business Need&#xA;Business requirements&#xA;Business Value : CEO의 입장에서 가장 먼저 보게된다.&#xA;프로젝트의 Business Value가 명확해야 한다. Economic feasibility부분을 미리 고민해봐야 한다. Special issues or constraints&#xA;프로젝트 마감 기간 등&#xA;ex) 크리스마스 시즌 전에 완성해야 한다.&#xA;2. Feasibility Analysis(타당성 분석) 프로젝트를 진행하기 전에 분석해야 한다.</description>
    </item>
    <item>
      <title>Theme Documentation - Advanced</title>
      <link>https://osmin625.github.io/posts/theme-documentation-advanced/</link>
      <pubDate>Wed, 06 May 2020 21:29:01 +0800</pubDate>
      <guid>https://osmin625.github.io/posts/theme-documentation-advanced/</guid>
      <description>Gokarna is an opinionated theme with a focus on minimalism and simplicity.&#xA;Content Types This theme supports two types of content types: post and page. To specify them, you need to add them in your markdown metadata.&#xA;Post This is the default blog post type which will be shown in your &amp;ldquo;Posts&amp;rdquo; section and who&amp;rsquo;s tags will be indexed. Basically, a normal blog post.&#xA;--- title: &amp;#34;Hello, world!&amp;#34; date: 2021-01-01Tdescription: &amp;#34;A blog post&amp;#34; image: &amp;#34;/path/to/image.</description>
    </item>
    <item>
      <title>Theme Documentation - Basics</title>
      <link>https://osmin625.github.io/posts/theme-documentation-basics/</link>
      <pubDate>Fri, 06 Mar 2020 21:29:01 +0800</pubDate>
      <guid>https://osmin625.github.io/posts/theme-documentation-basics/</guid>
      <description>Gokarna is an opinionated theme with a focus on minimalism and simplicity.&#xA;Installation The following steps are here to help you initialize your new website. If you don’t know Hugo at all, we strongly suggest you learn more about it by following this great documentation for beginners.&#xA;a. Create Your Project Hugo provides a new command to create a new website:&#xA;hugo new site my_website cd my_website b. Install the Theme The theme’s repository is: https://github.</description>
    </item>
    <item>
      <title></title>
      <link>https://osmin625.github.io/posts/1-epoch/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://osmin625.github.io/posts/1-epoch/</guid>
      <description></description>
    </item>
  </channel>
</rss>
