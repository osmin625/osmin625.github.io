<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Word2Vec on Robust dev O</title>
    <link>https://osmin625.github.io/tags/Word2Vec/</link>
    <description>Recent content in Word2Vec on Robust dev O</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>ko</language>
    <lastBuildDate>Fri, 31 Mar 2023 06:05:00 +0900</lastBuildDate>
    <atom:link href="https://osmin625.github.io/tags/Word2Vec/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>CBOW: Continous Bag of Word</title>
      <link>https://osmin625.github.io/posts/CBOW/</link>
      <pubDate>Fri, 31 Mar 2023 06:05:00 +0900</pubDate>
      <guid>https://osmin625.github.io/posts/CBOW/</guid>
      <description>Word2Vec을 학습하는 방법 중 하나.&#xA;앞뒤의 단어를 통해 중앙의 단어를 예측하는 방법&#xA;input — quick, brown, jumps, over output — fox 단어를 예측하기 위해 앞뒤로 몇 개의 단어(n)를 사용할지 정한다.&#xA;Multi-Class Classification&#xA;Input을 통해 One-Hot Vector의 각 원소가 0인지 1인지 예측한다.&#xA;학습 파라미터 $W_{V\times M}$: One-Hot Vector을 임베딩 벡터로 변환하는 행렬&#xA;$V$: 단어의 총 개수(One-Hot Vector의 크기) $M$: 임베딩 벡터의 크기 $W&#39;_{M\times V}$: 임베딩 벡터를 One-Hot Vector의 길이로 변환하는 행렬</description>
    </item>
    <item>
      <title>SG: Skip-Gram</title>
      <link>https://osmin625.github.io/posts/SG/</link>
      <pubDate>Fri, 31 Mar 2023 06:05:00 +0900</pubDate>
      <guid>https://osmin625.github.io/posts/SG/</guid>
      <description>Word2Vec을 학습하는 방법 중 하나.&#xA;CBOW가 뒤집어진 모델&#xA;CBOW와 입력층과 출력층이 반대로 구성되어 있다.&#xA;벡터의 평균을 구하는 과정이 없다.&#xA;CBOW보다 성능이 좋다고 알려져있다.&#xA;참고로 이 때의 성능은 학습 과정의 Loss가 아니라 임베딩 벡터의 표현력을 의미한다.&#xA;CBOW와 마찬가지로 Multi-Classification Model에 해당한다.</description>
    </item>
    <item>
      <title>SGNS: Skip-Gram with Negative Sampling</title>
      <link>https://osmin625.github.io/posts/SGNS/</link>
      <pubDate>Fri, 31 Mar 2023 06:05:00 +0900</pubDate>
      <guid>https://osmin625.github.io/posts/SGNS/</guid>
      <description>SG를 이진 분류 문제로 바꾼 모델&#xA;Negative Sampling 주변 단어가 아닌 단어를 Label 0으로 Sample에 포함시키는 것&#xA;Negative Sampling의 개수는 하이퍼파라미터에 해당한다.&#xA;학습 데이터가 적은 경우 5-20, 충분히 큰 경우 2-5가 적당하다. positive sample 하나당 k개 샘플링 중심 단어와 주변 단어가 각각 임베딩 벡터를 따로 가진다.&#xA;만약 input/output 혹은 word/context representation을 동일한 값으로 사용한다고 하면,&#xA;특정 단어, 가령 &amp;ldquo;dog&amp;quot;에 대해 P(dog|dog)가 현실적으로는 불가하지만 (한 문장에 &amp;ldquo;dog dog&amp;quot;를 연속으로 쓸 일은 없으니.</description>
    </item>
  </channel>
</rss>
