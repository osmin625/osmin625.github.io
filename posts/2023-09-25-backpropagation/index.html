<!DOCTYPE html>
<html lang="en" dir="ltr">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="description" content="이 알고리즘으로 인해 ML Network에서의 학습이 가능하다는 것이 알려져, 암흑기에 있던 신경망 학계가 다시 관심을 받게 되었다.
출력층에서 시작하여 역방향으로 오류를 전파한다는 뜻에서 오류 역전파라 부른다.
내가 뽑고자 하는 target값과 실제 모델이 계산한 출력의 차이를 계산한다. 오차값을 다시 뒤로 전파해가면서 각 노드가 가지고 있는 변수들을 갱신한다. 직관적인 이해는 끝났다. 이제 제대로 이해해보자.
오차 역전파가 중요한 이유를 알고 싶다면, 여기를 클릭하여 오차 역전파가 없을 시에 발생하는 문제점을 이해하자.
오차 역전파#신경망을 학습하는 방법.">
<meta name="theme-color" media="(prefers-color-scheme: light)" content="#ffffff">
<meta name="theme-color" media="(prefers-color-scheme: dark)" content="#343a40">
<meta name="color-scheme" content="light dark"><meta property="og:title" content="Back Propagation(오차 역전파 알고리즘)" />
<meta property="og:description" content="이 알고리즘으로 인해 ML Network에서의 학습이 가능하다는 것이 알려져, 암흑기에 있던 신경망 학계가 다시 관심을 받게 되었다.
출력층에서 시작하여 역방향으로 오류를 전파한다는 뜻에서 오류 역전파라 부른다.
내가 뽑고자 하는 target값과 실제 모델이 계산한 출력의 차이를 계산한다. 오차값을 다시 뒤로 전파해가면서 각 노드가 가지고 있는 변수들을 갱신한다. 직관적인 이해는 끝났다. 이제 제대로 이해해보자.
오차 역전파가 중요한 이유를 알고 싶다면, 여기를 클릭하여 오차 역전파가 없을 시에 발생하는 문제점을 이해하자.
오차 역전파#신경망을 학습하는 방법." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://osmin625.github.io/posts/2023-09-25-backpropagation/" /><meta property="article:section" content="posts" />

<meta property="article:modified_time" content="2024-01-10T01:07:46+09:00" />

<title>Back Propagation(오차 역전파 알고리즘) | OMIN</title>
<link rel="manifest" href="/manifest.json">
<link rel="icon" href="/favicon.png" >
<link rel="stylesheet" href="/book.min.33a48f5432973b8ff9a82679d9e45d67f2c15d4399bd2829269455cfe390b5e8.css" integrity="sha256-M6SPVDKXO4/5qCZ52eRdZ/LBXUOZvSgpJpRVz&#43;OQteg=" crossorigin="anonymous"><!--
Made with Book Theme
https://github.com/alex-shpak/hugo-book
-->
  
</head>
<body dir="ltr">
  <input type="checkbox" class="hidden toggle" id="menu-control" />
  <input type="checkbox" class="hidden toggle" id="toc-control" />
  <main class="container flex">
    <aside class="book-menu">
      <div class="book-menu-content">
        
  <nav>
<h2 class="book-brand">
  <a class="flex align-center" href="/"><span>OMIN</span>
  </a>
</h2>













  












  
<ul>
  
  <li>
    <a href="/posts/"  >
        Blog
      </a>
  </li>
  
  <li>
    <a href="https://github.com/osmin625/"  target="_blank" rel="noopener">
        Github
      </a>
  </li>
  
</ul>






</nav>




  <script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script>


 
      </div>
    </aside>

    <div class="book-page">
      <header class="book-header">
        
  <div class="flex align-center justify-between">
    <label for="menu-control">
      
      <link rel="icon" href="/favicon.ico" type="image/x-icon">
    </label>
  
    <strong>Back Propagation(오차 역전파 알고리즘)</strong>
  
    <label for="toc-control">
      
      <img src="/svg/toc.svg" class="book-icon" alt="Table of Contents" />
      
    </label>
  </div>
  

  
  <aside class="hidden clearfix">
    
  
<nav id="TableOfContents">
  <ul>
    <li>
      <ul>
        <li><a href="#오차-역전파">오차 역전파</a>
          <ul>
            <li><a href="#연쇄-법칙chain-rule">연쇄 법칙(chain rule)</a></li>
            <li><a href="#순전파-역전파-국소적-계산">순전파, 역전파, 국소적 계산</a></li>
            <li><a href="#덧셈-노드에서의-역전파">덧셈 노드에서의 역전파</a></li>
            <li><a href="#곱셈-노드에서의-역전파">곱셈 노드에서의 역전파</a></li>
            <li><a href="#연쇄-법칙과-계산-그래프">연쇄 법칙과 계산 그래프</a></li>
            <li><a href="#결론">결론</a></li>
          </ul>
        </li>
      </ul>
    </li>
  </ul>
</nav>



  </aside>
  
 
      </header>

      
      
<article class="markdown book-post">
  <h1>
    <a href="/posts/2023-09-25-backpropagation/">Back Propagation(오차 역전파 알고리즘)</a>
  </h1>
  


  
  <div>
    
      <a href="/categories/AI-Knowledge/">AI Knowledge</a>, 
      <a href="/categories/Loss-Function/">Loss Function</a>
  </div>
  

  
  <div>
    
      <a href="/tags/Loss-Function/">Loss Function</a>, 
      <a href="/tags/Gradient-Descent/">Gradient Descent</a>, 
      <a href="/tags/Back-Propagation/">Back Propagation</a>
  </div>
  



<p><strong>이 알고리즘으로 인해 ML Network에서의 학습이 가능하다는 것이 알려져, 암흑기에 있던 신경망 학계가 다시 관심을 받게 되었다.</strong></p>
<p>출력층에서 시작하여 역방향으로 오류를 전파한다는 뜻에서 오류 역전파라 부른다.</p>
<ol>
<li>내가 뽑고자 하는 target값과 실제 모델이 계산한 출력의 차이를 계산한다.</li>
<li>오차값을 다시 뒤로 전파해가면서 각 노드가 가지고 있는 변수들을 갱신한다.</li>
</ol>
<p>
  <img src="bp.png" alt="bp" /></p>
<p>직관적인 이해는 끝났다. 이제 제대로 이해해보자.</p>
<p>오차 역전파가 중요한 이유를 알고 싶다면, 
  <a href="https://osmin625.github.io/posts/%EC%88%98%EC%B9%98-%EB%AF%B8%EB%B6%84/">여기</a>를 클릭하여 오차 역전파가 없을 시에 발생하는 문제점을 이해하자.</p>
<h2 id="오차-역전파">
  오차 역전파
  <a class="anchor" href="#%ec%98%a4%ec%b0%a8-%ec%97%ad%ec%a0%84%ed%8c%8c">#</a>
</h2>
<p>신경망을 학습하는 방법.</p>
<p>연쇄 법칙을 활용하여 수치 미분에서의 연산량을 대폭 감소시킨다.</p>
<p>수치 미분과 마찬가지로 손실 함수 위에서의 가중치의 기울기를 알게 해준다.</p>
<p>즉, 해당 가중치가 얼마나 오차에 영향을 끼치는지 알게 해준다.</p>
<p>(기울기에 대한 손실 함수의 미분)</p>
<p>특정 가중치 w에 대해, 오차 L 위에서의 기울기는 $\partial L\over \partial w$이다.</p>
<h3 id="연쇄-법칙chain-rule">
  연쇄 법칙(chain rule)
  <a class="anchor" href="#%ec%97%b0%ec%87%84-%eb%b2%95%ec%b9%99chain-rule">#</a>
</h3>
<p><strong>합성 함수의 미분은 합성 함수를 구성하는 각 함수의 미분의 곱으로 나타낼 수 있다.</strong></p>
<p>$$
{\partial z\over \partial x} = {\partial z\over \partial t}{\partial t\over \partial x}
$$</p>
<p>합성 함수</p>
<p>여러 함수로 구성된 함수</p>
<ul>
<li>ex) $z = (x + y)^2$
<ul>
<li>$z = t^2$</li>
<li>$t = x + y$</li>
</ul>
</li>
</ul>
<h3 id="순전파-역전파-국소적-계산">
  순전파, 역전파, 국소적 계산
  <a class="anchor" href="#%ec%88%9c%ec%a0%84%ed%8c%8c-%ec%97%ad%ec%a0%84%ed%8c%8c-%ea%b5%ad%ec%86%8c%ec%a0%81-%ea%b3%84%ec%82%b0">#</a>
</h3>
<p>ex) $f(x) = y$</p>
<p>
  <img src="bp1.png" alt="bp" /></p>
<p><strong>f에 x를 집어넣으면 y가 튀어나온다.</strong></p>
<p>위의 그림처럼 어딘가에서 갑자기 등장한 $L$이라는 함수를 $y$로 미분한 값($\partial L\over \partial y$)이 제공된다면?</p>
<p>우리는 $L$이라는 함수를 모르지만, $x$로 미분한 값을 알 수 있다.</p>
<p>연쇄법칙을 활용하면 된다.</p>
<p>$$
{\partial L\over \partial x} = {\partial L\over \partial y}{\partial y\over \partial x}
$$</p>
<p>우리는 $\partial L\over \partial y$를 알고 있으니, $\partial y\over \partial x$만 계산하면 된다.</p>
<p>${\partial y\over \partial x} = f&rsquo;(x)$이므로, x에 대한 함수인 $f(x)$를 미분하면 된다.</p>
<p>ex) $f(x) = x^2 ⇒ f&rsquo;(x) = 2x$</p>
<p>이 때, 위의 예시에서 왼쪽에서 오른쪽으로 진행하는 단계를 <strong>순전파(forward propagation)</strong>,</p>
<p>오른쪽에서 왼쪽으로 진행하는 단계를 **역전파(backward propagation)**이라고 한다.</p>
<p>국소적 계산</p>
<p>전체에서 어떤 일이 벌어지든 상관없이 자신과 관계된 정보만을 결과로 출력할 수 있다.</p>
<p>위의 예시처럼, 각 단계에서는 그저 $f(x)$의 미분값만 곱해서 하류로 흘러보낸 것이 전부다.</p>
<p>단순한 국소적 계산이 연결되어 전체를 구성하는 복잡한 계산을 수행하게 된다.</p>
<h3 id="덧셈-노드에서의-역전파">
  덧셈 노드에서의 역전파
  <a class="anchor" href="#%eb%8d%a7%ec%85%88-%eb%85%b8%eb%93%9c%ec%97%90%ec%84%9c%ec%9d%98-%ec%97%ad%ec%a0%84%ed%8c%8c">#</a>
</h3>
<p>ex) $z = x + y$</p>
<ul>
<li>$\partial z\over \partial x$ = 1</li>
<li>$\partial z\over \partial y$ = 1</li>
</ul>
<p>
  <img src="bp2.png" alt="bp" /></p>
<p>상류에서 내려온 신호인 $\partial L\over \partial z$에 $\partial z\over \partial x$를 곱함으로써 $\partial L\over \partial x$을 구했다.</p>
<p>단순한 덧셈 연산이기 때문에 미분값은 1이다.</p>
<p>즉, 덧셈에서는 상류에서 내려온 값을 그대로 하류로 전달한다.</p>
<h3 id="곱셈-노드에서의-역전파">
  곱셈 노드에서의 역전파
  <a class="anchor" href="#%ea%b3%b1%ec%85%88-%eb%85%b8%eb%93%9c%ec%97%90%ec%84%9c%ec%9d%98-%ec%97%ad%ec%a0%84%ed%8c%8c">#</a>
</h3>
<p>ex) $z = xy$</p>
<ul>
<li>${\partial z\over \partial x} = y$</li>
<li>${\partial z\over \partial y} = x$</li>
</ul>
<p>이므로,</p>
<p>
  <img src="bp3.png" alt="bp" />
이 된다.</p>
<p>즉, 단순한 곱셈 노드는 상류에서 내려온 신호를 교차시켜 곱한 후, 다시 하류로 보낸다.</p>
<p>하지만 본질은 미분이다.</p>
<p>$z = x^2$의 경우, 미분값은 $2x$가 되는 것을 명심하자.</p>
<h3 id="연쇄-법칙과-계산-그래프">
  연쇄 법칙과 계산 그래프
  <a class="anchor" href="#%ec%97%b0%ec%87%84-%eb%b2%95%ec%b9%99%ea%b3%bc-%ea%b3%84%ec%82%b0-%ea%b7%b8%eb%9e%98%ed%94%84">#</a>
</h3>
<p>
  <img src="bp4.png" alt="bp" /></p>
<p>
  <img src="bp5.png" alt="bp" /></p>
<p>처음에 예시로 들었던 함수 $z = (x+y)^2$를 그래프화 한 것이다.</p>
<h3 id="결론">
  결론
  <a class="anchor" href="#%ea%b2%b0%eb%a1%a0">#</a>
</h3>
<p>역전파는 오차(손실 함수)를 상류에서 하류로 내려보내면서 각 가중치가 손실 함수에서의 기울기를 알 수 있도록 해주는 기법이다.</p>
<p>이 때 수많은 노드와 복잡한 활성화 함수 등이 각 노드에서 국소적 계산을 한다.</p>
<p>이런 값들이 누적되고 확산되어 단 한번의 신경망 계산(역방향)만으로도 모든 가중치와 편향을 갱신할 수 있다.</p>
<p>곱셈 노드, 덧셈 노드뿐만 아니라 exp 노드, log 노드 등 수많은 노드들이 있지만,</p>
<p>결국 <strong>기본 원리는 상류 노드에 해당 노드에서의 미분 값을 찾아서 곱한 뒤, 하류로 흘려보내는 것</strong>이 전부이다.</p>
<p>그러므로 수치 미분보다는 당연히 빠르다.</p>
<p>
  <img src="bp6.png" alt="bp" /></p>
<ul>
<li>
<p>구현 코드</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">MulLayer</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># 곱셈 계층</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __init__(self) <span style="color:#f92672">-&gt;</span> <span style="color:#66d9ef">None</span>:
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>x <span style="color:#f92672">=</span> <span style="color:#66d9ef">None</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>y <span style="color:#f92672">=</span> <span style="color:#66d9ef">None</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, x, y):
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># 순전파, x와 y의 값을 저장해야만 backward때 사용할 수 있다.</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>x <span style="color:#f92672">=</span> x
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>y <span style="color:#f92672">=</span> y
</span></span><span style="display:flex;"><span>        out <span style="color:#f92672">=</span> x <span style="color:#f92672">*</span> y
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> out
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">backward</span>(self, dout):
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># 역전파로 상위 계층에서의 미분 값 * 반대 노드의 값을 출력한다.</span>
</span></span><span style="display:flex;"><span>        dx <span style="color:#f92672">=</span> dout <span style="color:#f92672">*</span> self<span style="color:#f92672">.</span>y
</span></span><span style="display:flex;"><span>        dy <span style="color:#f92672">=</span> dout <span style="color:#f92672">*</span> self<span style="color:#f92672">.</span>x
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> dx, dy
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">AddLayer</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># 덧셈 계층</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __init__(self) <span style="color:#f92672">-&gt;</span> <span style="color:#66d9ef">None</span>:
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">pass</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, x, y):
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># 순전파, x와 y 값을 저장하지 않아도 된다.</span>
</span></span><span style="display:flex;"><span>        out <span style="color:#f92672">=</span> x <span style="color:#f92672">+</span> y
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> out
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">backward</span>(self, dout):
</span></span><span style="display:flex;"><span>        dx <span style="color:#f92672">=</span> dout <span style="color:#f92672">*</span> <span style="color:#ae81ff">1</span>
</span></span><span style="display:flex;"><span>        dy <span style="color:#f92672">=</span> dout <span style="color:#f92672">*</span> <span style="color:#ae81ff">1</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> dx, dy
</span></span></code></pre></div></li>
</ul>
</article>
 
      

      <footer class="book-footer">
        
  <div class="flex flex-wrap justify-between">





</div>



  <script>(function(){function e(e){const t=window.getSelection(),n=document.createRange();n.selectNodeContents(e),t.removeAllRanges(),t.addRange(n)}document.querySelectorAll("pre code").forEach(t=>{t.addEventListener("click",function(){if(window.getSelection().toString())return;e(t.parentElement),navigator.clipboard&&navigator.clipboard.writeText(t.parentElement.textContent)})})})()</script>


 
        
      </footer>

      
  
 

      <label for="menu-control" class="hidden book-menu-overlay"></label>
    </div>

    
    <aside class="book-toc">
      <div class="book-toc-content">
        
  
<nav id="TableOfContents">
  <ul>
    <li>
      <ul>
        <li><a href="#오차-역전파">오차 역전파</a>
          <ul>
            <li><a href="#연쇄-법칙chain-rule">연쇄 법칙(chain rule)</a></li>
            <li><a href="#순전파-역전파-국소적-계산">순전파, 역전파, 국소적 계산</a></li>
            <li><a href="#덧셈-노드에서의-역전파">덧셈 노드에서의 역전파</a></li>
            <li><a href="#곱셈-노드에서의-역전파">곱셈 노드에서의 역전파</a></li>
            <li><a href="#연쇄-법칙과-계산-그래프">연쇄 법칙과 계산 그래프</a></li>
            <li><a href="#결론">결론</a></li>
          </ul>
        </li>
      </ul>
    </li>
  </ul>
</nav>


 
      </div>
    </aside>
    
  </main>

  
</body>
</html>












