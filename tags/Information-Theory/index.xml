<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Information Theory on 오뚝이 개발자</title>
    <link>https://osmin625.github.io/tags/Information-Theory/</link>
    <description>Recent content in Information Theory on 오뚝이 개발자</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>ko</language>
    <lastBuildDate>Fri, 22 Sep 2023 07:11:00 +0900</lastBuildDate>
    <atom:link href="https://osmin625.github.io/tags/Information-Theory/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Entropy(엔트로피)</title>
      <link>https://osmin625.github.io/posts/entropy_%E1%84%8B%E1%85%A6%E1%86%AB%E1%84%90%E1%85%B3%E1%84%85%E1%85%A9%E1%84%91%E1%85%B5/</link>
      <pubDate>Fri, 22 Sep 2023 07:11:00 +0900</pubDate>
      <guid>https://osmin625.github.io/posts/entropy_%E1%84%8B%E1%85%A6%E1%86%AB%E1%84%90%E1%85%B3%E1%84%85%E1%85%A9%E1%84%91%E1%85%B5/</guid>
      <description>간단 요약&#xA;확률 분포의 불확실성을 의미한다.&#xA;정보 이론에서의 엔트로피는 (최적화된 전략 하에서의) 질문 개수에 대한 기댓값이다.&#xA;스무 고개로 정답 맞추기를 진행할 때 질문이 많이 필요할수록 불확실성이 높은 것이다.&#xA;확률 분포의 무작위성(불확실성)을 측정하는 함수&#xA;$$&#xD;H(X)=\sum_{i=1}^n p_i\left(\log \frac{1}{p_i}\right)=-\sum_{i=1}^n p_i \log p_i&#xD;$$&#xA;entropy 공식은 왜 이렇게 생겼을까?&#xA;스무 고개로 정답 맞추기를 진행할 때 확률 분포가 불확실할수록 필요한 질문의 개수가 늘어난다.&#xA;이 때, 전체 경우를 양분하는 질문의 개수는 $log_2$를 통해 파악할 수 있다.</description>
    </item>
  </channel>
</rss>
