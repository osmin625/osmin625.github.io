[{"id":0,"href":"/posts/2023-05-14-MF-Matrix-Factorization/","title":"MF: Matrix Factorization","section":"Blog","content":" ê°„ë‹¨ ìš”ì•½\ní–‰ë ¬ì„ ì¸ìˆ˜ë¶„í•´ í›„ ì¬ìƒì„±í•˜ì—¬ ê²°ì¸¡ì¹˜ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ë°©ë²•\nì¸ìˆ˜ë¶„í•´ë¥¼ ë‹¤ì–‘í•˜ê²Œ ì‹œë„í•˜ì—¬ ëŒ€ìƒ í–‰ë ¬ì„ ê°€ì¥ ì˜ ë³µêµ¬í•˜ëŠ” ìµœì ì˜ í•˜ìœ„í–‰ë ¬ì„ ì°¾ëŠ” ê³¼ì • {: .prompt-info } Latent Factors, Matrix Factorization, SGD\ní–‰ë ¬ì„ ì¸ìˆ˜ë¶„í•´ í›„ ì¬ìƒì„±í•˜ì—¬ ê²°ì¸¡ì¹˜ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ë°©ë²•\nëŒ€ìƒ í–‰ë ¬ì„ ë‘ê°œì˜ í•˜ìœ„ í–‰ë ¬ë¡œ ë¶„í•´í•œë‹¤.\nUser-Item í–‰ë ¬ì„ Userì™€ Item ê°ê°ì— ëŒ€í•œ ì €ì°¨ì›ì˜ latent factor í–‰ë ¬ë¡œ ë¶„í•´í•œë‹¤.\në‘ í•˜ìœ„ í–‰ë ¬ì„ ë‹¤ì‹œ ê³±í•´ì„œ ëŒ€ìƒ í–‰ë ¬ê³¼ ë™ì¼í•œ í¬ê¸°ì˜ ë‹¨ì¼ í–‰ë ¬ë¡œ ë§Œë“ ë‹¤.\nìœ„ì˜ ê³¼ì •ì—ì„œ ê¸°ì¡´ í–‰ë ¬ì˜ ë¹ˆê³µê°„ì´ ì±„ì›Œì§„ë‹¤.\nì´ëŠ” í–‰ë ¬ì˜ ì„±ì§ˆì„ ì´ìš©í•œ ê²ƒì´ë‹¤.\nì¦‰, ê²°ì¸¡ê°’(ë¹„í‰ê°€ í•­ëª©)ì— ëŒ€í•´ ì„ì˜ë¡œ Imputationì„ ìˆ˜í–‰í•˜ì§€ ì•ŠëŠ”ë‹¤.\nì‹¤ì œë¡œ ê´€ì¸¡ëœ ê°’ë§Œ í™œìš©í•œë‹¤.\nìš”ì•½í•˜ë©´, ì¸ìˆ˜ë¶„í•´ë¥¼ ë‹¤ì–‘í•˜ê²Œ ì‹œë„í•˜ì—¬ ëŒ€ìƒ í–‰ë ¬ì„ ê°€ì¥ ì˜ ë³µêµ¬í•˜ëŠ” ìµœì ì˜ í•˜ìœ„í–‰ë ¬ì„ ì°¾ëŠ” ê³¼ì •ì´ë‹¤.\nê¸°ê³„ê°€ í•´ì„í•˜ê¸° ìœ„í•œ í–‰ë ¬, ì¦‰ ë¸”ë™ ë°•ìŠ¤ ëª¨ë¸ì— ë” ê°€ê¹ë‹¤.\nì¶”ì²œì‹œìŠ¤í…œì—ì„œ ë‘ ê°œì˜ í•˜ìœ„í–‰ë ¬ì€ ê°ê° ìœ ì € ì„ë² ë”©(User Latent Factors)ê³¼ ì•„ì´í…œ ì„ë² ë”©(Item Latent Factors)ì´ ëœë‹¤.\nê²°êµ­ MFë¥¼ í•™ìŠµí•˜ëŠ” ê²ƒì€ latent featureë“¤ì„ í•™ìŠµí•˜ëŠ” ê²ƒê³¼ ê°™ë‹¤.\në²”ì£¼í˜• featureì˜ ì ì¬ ìš”ì¸ì„ featureë¡œ ì‚¬ìš©í•˜ê³ ì í•  ê²½ìš° ì´ ì ì¬ ìš”ì¸ì„ ê°€ì¥ ì†ì‰½ê²Œ êµ¬í•  ìˆ˜ ìˆëŠ” ë°©ë²•ì´ë‹¤.\nLatent í–‰ë ¬ì„ ê°ê° P, Që¼ê³  í–ˆì„ ë•Œ, MFëŠ” Rating Matrixë¥¼ $P$ì™€ $Q$ë¡œ ë¶„í•´í•˜ì—¬ $R$ê³¼ ìµœëŒ€í•œ ìœ ì‚¬í•˜ê²Œ $\\hat R$ì„ ì¶”ë¡ í•œë‹¤.(ìµœì í™”)\n$$ R \\approx P \\times Q^T = \\widehat R $$\nëª¨ë¸ í•™ìŠµ ê³¼ì •\r#\rObject Functionì„ ì •ì˜í•˜ê³  ìµœì í™” ë¬¸ì œë¥¼ í‘¸ëŠ” ëª¨ë¸í•™ìŠµ ê³¼ì •ì„ ì‚´í´ë³´ì.\n$$ R \\approx P \\times Q^T = \\widehat R\\ P \\rightarrow |U| \\times k\\ Q \\rightarrow |I| \\times k $$\n![MF](/assets/post_imgs/Matrix Factorization.png)\nObjective Function\r#\rê¸°ë³¸ í˜•íƒœ\r#\r$$ \\tt \\min {P, Q} \\sum{\\text {observed } r_{u, i}}\\left(r_{u, i}-p_u^T q_i\\right)^2 $$\n$r_{u,i}:$ í•™ìŠµ ë°ì´í„°ì— ìˆëŠ” ìœ ì € $u$ì˜ ì•„ì´í…œ $i$ì— ëŒ€í•œ ì‹¤ì œ rating\n$p_u,q_i:$ ìœ ì €($u$)ì™€ ì•„ì´í…œ($i$)ì˜ latent vector\ní•™ìŠµì„ í†µí•´ ì•Œì•„ë‚´ê³ ì í•˜ëŠ” ëª©í‘œ\nìµœì í™” ë¬¸ì œë¥¼ í†µí•´ ê°±ì‹ ë˜ëŠ” íŒŒë¼ë¯¸í„°\n$\\widehat {r_{u,i}} = p_u^Tq_i$ : ì˜ˆì¸¡ëœ Rating Matrix\nì›ë³¸ ìˆ˜ì‹ì²˜ëŸ¼ $p \\times q^T$ ê°€ ì•„ë‹Œ $p^T \\times q$ ì¸ ì´ìœ \nìš°ì„ , $r_{u,i}$, $p_{u,x}$, ê·¸ë¦¬ê³  $q_{x,i}$ëŠ” ê°ê° ë‹¤ìŒì„ ì˜ë¯¸í•œë‹¤.($Q^T$ í—·ê°ˆë¦¬ì§€ ì•Šê²Œ ì£¼ì˜)\n$r_{u,i}$ : $R$ì˜ $(u,i)$ë²ˆì§¸ ì›ì†Œ\n$p_{u,x}$ : $P$ì˜ $(u,x)$ë²ˆì§¸ ì›ì†Œ\n$q_{x,i}$ : $Q^T$ì˜ $(x,i)$ë²ˆì§¸ ì›ì†Œ\në˜í•œ, í–‰ë ¬ ì—°ì‚°ì—ì„œ ë²¡í„°ëŠ” ë³´í†µ \u0026ldquo;ì—´ë²¡í„°(column vector)\u0026ldquo;ë¥¼ ì˜ë¯¸í•˜ë¯€ë¡œ\n$P$ì˜ $u$ë²ˆì§¸ í–‰($p_{u,1:k}$)ì€ $P^T$ì˜ $u$ë²ˆì§¸ ì—´($p^T_{1:k,u}$), ì¦‰ $p^T_u$ë¡œ í‘œí˜„í•  ìˆ˜ ìˆë‹¤.\në”°ë¼ì„œ, í•´ë‹¹ ê³µì‹ì—ì„œ $R$ì˜ $(u,i)$ë²ˆì§¸ ì›ì†Œì˜ ì¶”ì •ì¹˜ì¸ $\\widehat{r_{u,i}}$ëŠ”\n$P^T$ì˜ $u$ë²ˆì§¸ ì—´ë²¡í„°($p^T_u$)ì™€ $Q^T$ì˜ $i$ë²ˆì§¸ ì—´ë²¡í„°($q_i$)ë¥¼ ê³±í•œ ê°’ì´ë¼ê³  í•  ìˆ˜ ìˆë‹¤.\në¬¼ë¡ , $\\widehat{r_{u,i}}$ëŠ” $1 \\times 1$ í¬ê¸°ì˜ í–‰ë ¬ì´ë¯€ë¡œ ìš°ì¸¡ í•­ì— ì „ì¹˜ë¥¼ ì·¨í•´ë„ ê²°ê³¼ê°€ ë™ì¼í•˜ë‹¤.\n( $\\therefore \\widehat{r_{u,i}} = p^T_u \\cdot q_i = q^T_i \\cdot p_u$ )\nìµœì¢… í˜•íƒœ\r#\r$$ \\tt \\min {P, Q} \\sum{\\text {observed }r_{u,i}}\\left(r_{u, i}-p_u^T q_i\\right)^2 +{\\lambda\\left(\\left|p_u\\right|_2^2+\\left|q_i\\right|_2^2\\right)} $$\n$\\lambda$(ìƒìˆ˜)ë°° ëœ penalty termì€ L2 â€” Regularization(ê·œì œ)\ní•™ìŠµ ë°ì´í„°ì— ê³¼ì í•©ë˜ëŠ” ê²ƒì„ ë°©ì§€í•œë‹¤.\nMF í•™ìŠµ\r#\rSGD: Stochastic gradient descent(í™•ë¥ ì  ê²½ì‚¬ í•˜ê°•ë²•)\nMF ëª¨ë¸ì—ì„œì˜ SGD\nError $e_{ui}$\n$$ \\tt e_{u i}=r_{u i}-p_u^T q_i\n$$\nLoss $L$\n$$ \\tt L=\\sum\\left(r_{u, i}-p_u^T q_i\\right)^2+\\lambda\\left(\\left|p_u\\right|_2^2+\\left|q_i\\right|_2^2\\right) \\quad\n$$\nLossë¥¼ $p_u$ë¡œ ë¯¸ë¶„í•˜ì—¬ ìµœì†Ÿê°’ ê³„ì‚°\nGradient\n$$ {\\tt{\\frac{\\partial L}{\\partial p_u}=\\frac{\\partial\\left(r_{u i}-p_u^T q_i\\right)^2}{\\partial p_u}+\\frac{\\partial \\lambda\\left|p_u\\right|2^2}{\\partial p_u}}}\\\\tt{ \\=-2\\left(r{u i}-p_u^T q_i\\right) q_i+2 \\lambda p_u}\n$$\nì´ë¥¼ Error Termì„ í™œìš©í•˜ì—¬ ì•„ë˜ì™€ ê°™ì´ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆë‹¤.\n$$ \\tt \\frac{\\partial L}{\\partial p_u}=-2\\left(e_{u i} q_i-\\lambda p_u\\right) $$\nGradientì˜ ë°˜ëŒ€ ë°©í–¥ìœ¼ë¡œ $p_u$, $q_i$ë¥¼ ì—…ë°ì´íŠ¸\n$$ {\\tt \\p_u \\leftarrow p_u+\\eta \\cdot\\left(e_{u i} q_i-\\lambda p_u\\right)}\\ \\tt q_i \\leftarrow q_i+\\eta \\cdot\\left(e_{u i} p_u-\\lambda q_i\\right) $$\në¶€í˜¸ê°€ ë°”ë€ë‹¤.\nMF ê¸°ë°˜ ì¶”ì²œìœ¼ë¡œ ê°€ì¥ ë„ë¦¬ ì•Œë ¤ì§„ ë…¼ë¬¸\r#\rMatrix Factorization Techniques for Recommender Systems\nê¸°ë³¸ì ì¸ MFì— ë‹¤ì–‘í•œ í…Œí¬ë‹‰ì„ ì¶”ê°€í•˜ì—¬ ì„±ëŠ¥ì„ í–¥ìƒì‹œì¼°ë‹¤.\nAdding Biases\r#\rì–´ë–¤ ìœ ì €ëŠ” ëª¨ë“  ì˜í™”ì— ëŒ€í•´ í‰ì ì„ ë‚®ê²Œ ì¤„ ìˆ˜ë„ ìˆë‹¤.\nì•„ì´í…œë„ ë§ˆì°¬ê°€ì§€ë¡œ í¸í–¥ì´ ë°œìƒí•  ìˆ˜ ìˆë‹¤.\nâ‡’ ì „ì²´ í‰ê·  $\\mu$, ìœ ì €, ì•„ì´í…œì˜ biasë¥¼ ì¶”ê°€í•˜ì—¬ ì˜ˆì¸¡ ì„±ëŠ¥ì„ ë†’ì¸ë‹¤.\nê¸°ì¡´ ëª©ì  í•¨ìˆ˜\n$$ \\tt \\min {P, Q} \\sum{\\text {observed }r_{u,i}}\\left(r_{u, i}-p_u^T q_i\\right)^2+\\lambda\\left(\\left|p_u\\right|_2^2+\\left|q_i\\right|_2^2\\right) $$\nBiasê°€ ì¶”ê°€ëœ ëª©ì  í•¨ìˆ˜\n$$ {\\tt \\min {P, Q} \\sum{\\text {observed } r_{u,i}}\\left(r_{u, i}-{\\mu - b_u-b_i}-p_u^T q_i\\right)^2} \\+\\tt \\lambda\\left(|| p_u\\left|_2^2+|| q_i\\right|_2^2+{b_u^2+b_i^2}\\right) $$\në§ˆì°¬ê°€ì§€ë¡œ biasê°€ ê·œì œ termì— ì¶”ê°€ë˜ì–´ ê³¼ì í•©ë˜ì§€ ì•Šê²Œ í•œë‹¤.\nError\n$$ \\tt e_{u,i} = r_{u,i} - \\mu - b_u - b_i - p_u^Tq_i $$\nGradientì˜ ë°˜ëŒ€ë°©í–¥ìœ¼ë¡œ $\\tt b_u, b_i, x_u, y_i$ë¥¼ ì—…ë°ì´íŠ¸\n$$ \\begin{aligned}\u0026amp; {b_u \\leftarrow b_u+\\gamma \\cdot\\left(e_{u i}-\\lambda b_u\\right)} \\\u0026amp; {b_i \\leftarrow b_i+\\gamma \\cdot\\left(e_{u i}-\\lambda b_i\\right) } \\\u0026amp; p_u \\leftarrow p_u+\\gamma \\cdot\\left(e_{u i} q_i-\\lambda p_u\\right) \\\u0026amp; q_i \\leftarrow q_i+\\gamma \\cdot\\left(e_{u i} p_u-\\lambda q_i\\right)\\end{aligned} $$\nAdding Confidence Level\r#\rëª¨ë“  í‰ì ì´ ë™ì¼í•œ ì‹ ë¢°ë„ë¥¼ ê°–ì§€ ì•ŠëŠ”ë‹¤. â‡’ $r_{u,i}$ì— ëŒ€í•œ ì‹ ë¢°ë„ë¥¼ ì˜ë¯¸í•˜ëŠ” $c_{u,i}$ë¥¼ ì¶”ê°€\nëŒ€ê·œëª¨ ê´‘ê³  ì§‘í–‰ê³¼ ê°™ì´ íŠ¹ì • ì•„ì´í…œì´ ë§ì´ ë…¸ì¶œë˜ì–´ í´ë¦­ë˜ëŠ” ê²½ìš°\nìœ ì €ì˜ ì•„ì´í…œì— ëŒ€í•œ í‰ì ì´ ì •í™•í•˜ì§€ ì•Šì€ ê²½ìš°(implicit Feedback)\nê¸°ì¡´ ëª©ì  í•¨ìˆ˜\n$$ {\\tt \\min {P, Q} \\sum{\\text {observed } r_{u,i}}\\left(r_{u, i}-\\mu-b_u-b_i-p_u^T q_i\\right)^2} \\+\\tt\\lambda\\left(|| p_u\\left|_2^2+|| q_i\\right|_2^2+b_u^2+b_i^2\\right) $$\nConfidence Levelì´ ì¶”ê°€ëœ ëª©ì í•¨ìˆ˜\n$$ {\\tt \\min {P, Q} \\sum{\\text {observed } r_{u, i}} {c_{u, i}}\\left(r_{u, i}-\\mu-b_u-b_i-p_u^T q_i\\right)^2}\\+\\tt\\lambda\\left(|| p_u\\left|_2^2+|| q_i\\right|_2^2+b_u^2+b_i^2\\right) $$\nAdding Temporal Dynamics\r#\rì‹œê°„ì— ë”°ë¼ ë³€í•˜ëŠ” ìœ ì €, ì•„ì´í…œì˜ íŠ¹ì„±ì„ ë°˜ì˜í•˜ê³  ì‹¶ë‹¤.\nì•„ì´í…œì´ ì‹œê°„ì´ ì§€ë‚¨ì— ë”°ë¼ ì¸ê¸°ë„ê°€ ë–¨ì–´ì§„ë‹¤.\nìœ ì €ê°€ ì‹œê°„ì´ íë¥´ë©´ì„œ í‰ì ì„ ë‚´ë¦¬ëŠ” ê¸°ì¤€ì´ ì—„ê²©í•´ì§„ë‹¤.\nì‹œê°„ì„ ë°˜ì˜í•œ í‰ì  ì˜ˆì¸¡\ní•™ìŠµ íŒŒë¼ë¯¸í„°ê°€ ì‹œê°„ì„ ë°˜ì˜í•˜ë„ë¡ ëª¨ë¸ ì„¤ê³„\n$$ \\tt \\widehat r_{ui}(t) = \\mu + b_u(t) + b_i(t) + p^T_uq_i(t) $$\në‹¨ì \r#\rp, q ë³€ìˆ˜ê°€ 2ê°œë¼ì„œ ë¹ ë¥¸ê³„ì‚°ì´ ë¶ˆê°€ëŠ¥í•˜ë‹¤. SGDë¥¼ í•™ìŠµí•˜ëŠ” ê³¼ì •ì—ì„œ ì—…ë°ì´íŠ¸ë¥¼ ì—¬ëŸ¬ ë²ˆ í•˜ê¸° ë•Œë¬¸ì— ë¹ ë¥¸ ê³„ì‚°ì´ ë¶ˆê°€ëŠ¥í•˜ë‹¤ëŠ” ë‹¨ì ì´ ëˆ„ì ëœë‹¤. ìœ ì € ìˆ˜ì™€ ì•„ì´í…œ ìˆ˜ê°€ ì»¤ì§ˆìˆ˜ë¡ ë°˜ë³µë¬¸ìœ¼ë¡œ ì¸í•œ ì—°ì‚°ëŸ‰ ì¦ê°€ ì˜ˆì‹œ\rì—¬ê¸°ì„œëŠ” ì •ë‹µ íšŸìˆ˜ë¥¼ í–‰ë ¬ì˜ ê°’ìœ¼ë¡œ ì‚¬ìš©í•˜ì˜€ì§€ë§Œ ë¬¸ì œë³„ë¡œ ì‚¬ìš©í•œ ì‹œê°„ì˜ í‰ê·  í˜¹ì€ ë‹¤ë¥¸ featureë¥¼ ê°’ìœ¼ë¡œ í™œìš©í•œë‹¤ë©´ ë‹¤ë¥¸ ì ì¬ ìš”ì¸ ê°’ë“¤ì„ ì–»ì–´ë‚¼ ìˆ˜ ìˆë‹¤.\n$R \\approx PQ^{T}$ $R$ : ìœ ì €ë“¤ì— ëŒ€í•œ ë¬¸ì œë³„ ì •ë‹µ íšŸìˆ˜ í–‰ë ¬ $P$ : ìœ ì €ì™€ ì ì¬ ìš”ì¸ì˜ í–‰ë ¬ $Q$ : ë¬¸ì œì™€ ì ì¬ ìš”ì¸ì˜ í–‰ë ¬ ì°¸ê³ í•˜ë©´ ì¢‹ì€ ìë£Œ\nUnderstanding of Matrix Factorization (MF) and Singular Value Decomposition (SVD) - Medium Latent Matrix Factorization - Medium "},{"id":1,"href":"/posts/2023-05-20-1x1-Convolution/","title":"1 x 1 Convolution","section":"Blog","content":"ì°¨ì› ì¶•ì†Œë¥¼ ìœ„í•´ í™œìš©í•œë‹¤.\nì´ë¯¸ì§€ì—ì„œ ë‹¨ í•˜ë‚˜ì˜ í”½ì…€ë§Œ ë³´ê¸° ë•Œë¬¸ì—, ì´ë¯¸ì§€ì—ì„œ ì˜ì—­ì„ ì‚´í´ë³´ëŠ” ì˜ë¯¸ëŠ” ì—†ë‹¤.\në‹¤ë§Œ, 1 * 1 Convolutionì„ ì´ìš©í•˜ì—¬ ê¸°ì¡´ Spatial Dimensionì„ ê·¸ëŒ€ë¡œ ìœ ì§€í•œ ì±„, ì±„ë„ì˜ ê°œìˆ˜ë¥¼ 128ê°œì—ì„œ 32ê°œë¡œ ì¤„ì¸ë‹¤.\n![1x1 Conv_1](/assets/post_imgs/1x1 Conv_1.png)\nì´ë¥¼ í†µí•´ NNì˜ ì¸µì„ ë” ê¹Šê²Œ ìŒ“ìœ¼ë©´ì„œë„, ì±„ë„ì˜ ìˆ˜ë¥¼ ì¤„ì—¬ì„œ íŒŒë¼ë¯¸í„°ì˜ ìˆ˜ë¥¼ ì¤„ì¼ ìˆ˜ ìˆë‹¤.\n1 * 1 Convolutionì„ ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” ê²½ìš°\n![1x1 Conv_2](/assets/post_imgs/1x1 Conv_2.png)\n1 * 1 Convolutionì„ ì‚¬ìš©í•œ ê²½ìš°\n![1x1 Conv_3](/assets/post_imgs/1x1 Conv_3.png)\níŒŒë¼ë¯¸í„° ìˆ˜ë¥¼ 147,456ê°œì—ì„œ 40,960ê°œë¡œ íš¨ê³¼ì ìœ¼ë¡œ ì¤„ì¼ ìˆ˜ ìˆë‹¤.\nëŒ€í‘œì ì¸ ì˜ˆë¡œ Bottleneck architecture êµ¬ì¡°ê°€ 1 * 1 Convolutionì„ ì´ìš©í•œë‹¤.\n1x1 convolutionì˜ ì˜ë¯¸ = ì°¨ì› ì¶•ì†Œ, ê·¸ë¦¬ê³  bottleneck (ì»¨ë³¼ë£¨ì…˜ê³¼ ë³´í‹€ë„¥)\n"},{"id":2,"href":"/posts/2023-10-12-LeakGAN/","title":"[ë…¼ë¬¸ ë¦¬ë·°] LeakGAN: Long Text Generation via Adversarial Training with Leaked Information","section":"Blog","content":" âœ”ï¸ ê°„ë‹¨ ìš”ì•½\nSparsityì™€ Non-Informativeë¥¼ íš¨ê³¼ì ìœ¼ë¡œ í•´ê²°í•œë‹¤.\në¶„ë³„ë§ì—ê²Œ ìŠ¤íŒŒì´ë¥¼ ì‹¬ì–´ ìƒì„±ë§ì´ ë¶„ë³„ë§ì„ ë” ì˜ ì†ì¼ ìˆ˜ ìˆë„ë¡ êµ¬ì„±í•œë‹¤.\nHierarchical RL architecture (MANAGER, WORKER)\nMANAGER (LSTM)\nì¤‘ì¬ì ì—­í•  Dë¡œë¶€í„° ê³ ìˆ˜ì¤€ feature representationì„ ë°›ìŒ â†’ Leakage WORKER (LSTM)\n$s_t$ë¥¼ ì¸ì½”ë”©í•œ í›„, MANAGERê°€ ë„˜ê²¨ì¤€ Goal ì„ë² ë”©ê³¼ ê²°í•©í•œë‹¤. (ë‚´ì ) Dê°€ ë„˜ê²¨ì¤€ guiding signalì€ scalar ë³´ìƒ ê°’ìœ¼ë¡œë„ ì“°ì´ê³ , ë¬¸ì¥ ìƒì„± ê³¼ì •ì—ì„œ Goal ì„ë² ë”©ìœ¼ë¡œë„ ì“°ì¸ë‹¤. {: .prompt-info } logit, temperature parameter, highway network(gate), leakganì˜ 3ê°€ì§€ í•™ìŠµ ë°©ë²•, CNN for text classification, truncated normalization\në°°ê²½\r#\r1. RNN â€“ ë¬¸ì¥ ìƒì„±ì„ ìœ„í•œ ê°€ì¥ ê¸°ë³¸ì ì¸ ë°©ë²•\r#\rì´ì „ì— ìƒì„±ëœ ë‹¨ì–´ë¥¼ í™œìš©í•˜ì—¬ ë‹¤ìŒ ë‹¨ì–´ë¥¼ ìƒì„±í•´ë‚´ëŠ” ë°©ì‹\nground-truth ë‹¨ì–´ë“¤ì˜ log-likelihoodë¥¼ ìµœëŒ€í™”í•œë‹¤.\nSupervising(Ground-Truthì— ëŒ€í•œ ì„¤ì •) í•„ìš”\ní•™ìŠµê³¼ ì¶”ë¡  ë‹¨ê³„ì˜ ë¶ˆì¼ì¹˜ì— ì˜í•´ í¸ì°¨ê°€ ë°œìƒ\ní•´ê²°ì±…ìœ¼ë¡œ Scheduled sampling approach ì œì•ˆ â†’ ì‹¤íŒ¨\n2. GAN â€“ ëª©ì ì€ ìƒì„±ë§ Gì˜ ì„±ëŠ¥ì„ ê°œì„ í•˜ëŠ” ê²ƒ\r#\rì´ë¥¼ ìœ„í•´ ìƒì„±ë¬¼ì˜ ì§„ìœ„ ì—¬ë¶€ë¥¼ í‰ê°€í•˜ëŠ” ë¶„ë³„ë§ Dì™€ ëŒ€ë¦½ Jenson-Shannon ê±°ë¦¬ í™œìš© ìƒì„±ë§ì˜ ì„±ëŠ¥ì´ ì¶©ë¶„íˆ ì¢‹ì•„ì§€ë©´ ë¶„ë³„ë§ ê°–ë‹¤ë²„ë¦¼ 3. GANì˜ í•œê³„ ë° í•´ê²°ì±…\r#\rì œí•œì ì¸ ìƒì„± ê°€ëŠ¥í•œ ë¬¸ì¥ì˜ ê¸¸ì´(ìµœëŒ€ 20ë‹¨ì–´)\nNon-informative guiding signal\në¬¸ì¥ â†’ ìŠ¤ì¹¼ë¼ ê°’(guiding signal)\në³€í™˜ ê³¼ì •ì—ì„œ Gê°€ í•™ìŠµí•˜ëŠ” ë¬¸ì¥ì˜ êµ¬ì¡° ë° ì˜ë¯¸ë¥¼ ë³´ì¥í•˜ì§€ ì•ŠìŒ.\nâ‡’ Dê°€ Gì—ê²Œ ì ìˆ˜ì™€ í•¨ê»˜ ì„ë² ë”©(feature representation)ì„ ì œê³µí•˜ì—¬ í•´ê²°\nGëŠ” Dì˜ feature representationì— ì¼ì¹˜í•˜ë„ë¡ ì„ë² ë”© í•™ìŠµ\nSparsity\nê¸´ ë¬¸ì¥ ìƒì„± ì‹œ binary guiding signalì„ í™œìš© â†’ ì „ì²´ ë¬¸ì¥ì´ ìƒì„±ë˜ì—ˆì„ ë•Œë§Œ ê°€ëŠ¥\nâ‡’ ë¬¸ì¥ ìƒì„±ì„ ì—¬ëŸ¬ ë‹¨ê³„(ê³„ì¸µ)ìœ¼ë¡œ êµ¬ë¶„í•˜ì—¬ signalì„ ë” ë§ì´ ì œê³µ\nSparsityê°€ ì¼ë¶€ í•´ê²°ë  ë¿ë§Œ ì•„ë‹ˆë¼, Taskê°€ ì‘ì•„ì ¸ ëª¨ë¸ í•™ìŠµ ìš©ì´\nbut, ë¬¸ì¥ ìƒì„± ë‹¨ê³„ì— ëŒ€í•œ ì‚¬ì „ ì •ì˜ê°€ í•„ìš”\nâ‡’ ëœë¤ ë¬¸ì¥ ìƒì„±ì—ëŠ” ì ìš© ë¶ˆê°€ëŠ¥\nì•„ì´ë””ì–´\r#\rSparsityì™€ Non-Informativeë¥¼ íš¨ê³¼ì ìœ¼ë¡œ í•´ê²°í•˜ê¸° ìœ„í•´ LeakGAN ì œì•ˆí•œë‹¤.\në¶„ë³„ë§ì—ê²Œ ìŠ¤íŒŒì´ë¥¼ ì‹¬ì–´ ìƒì„±ë§ì´ ë¶„ë³„ë§ì„ ë” ì˜ ì†ì¼ ìˆ˜ ìˆë„ë¡ êµ¬ì„±í•œë‹¤.\nMANAGER (LSTM)\r#\rì¤‘ì¬ì ì—­í•  Dë¡œë¶€í„° ê³ ìˆ˜ì¤€ feature representationì„ ë°›ëŠ”ë‹¤. â†’ Leakage ë”°ë¼ì„œ í•´ë‹¹ ì •ë³´ëŠ” ì „ì—­ì ìœ¼ë¡œ ê´€ë¦¬ëœë‹¤. ë¬¼ë¡  ê²Œì„ ì§„í–‰ ì¤‘ì—ëŠ” Gì—ê²Œ í•´ë‹¹ ì •ë³´ë¥¼ ì œê³µí•˜ì§€ ì•ŠëŠ”ë‹¤. í•´ë‹¹ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ Goal ì„ë² ë”© ìƒì„± í›„ WORKERì—ê²Œ ë„˜ê¸´ë‹¤. WORKER (LSTM)\r#\rí˜„ì¬ê¹Œì§€ ìƒì„±ëœ ë¬¸ì¥ì„ ì¸ì½”ë”©í•œ í›„, MANAGERê°€ ë„˜ê²¨ì¤€ Goal ì„ë² ë”©ê³¼ ê²°í•©í•œë‹¤. (ë‚´ì ) Dê°€ ë„˜ê²¨ì¤€ guiding signalì€ scalar ë³´ìƒ ê°’ìœ¼ë¡œë„ ì“°ì´ê³ , ë¬¸ì¥ ìƒì„± ê³¼ì •ì—ì„œ Goal ì„ë² ë”©ìœ¼ë¡œë„ ì“°ì¸ë‹¤.\nêµ¬ì²´ì  ë°©ë²•ë¡ \r#\rí…ìŠ¤íŠ¸ ìƒì„± ë¬¸ì œ â†’ Sequential Decision Making Process\n$s_t : t$ ì‹œì ê¹Œì§€ ìƒì„±ëœ ë‹¨ì–´ë“¤. $(x_1,\\dots, x_i, \\dots, x_t)$ $x_i:$ ë‹¨ì–´(token) ìƒì„±ë§ $G_\\theta$\r#\r$G_\\theta:$ íŒŒë¼ë¯¸í„°ê°€ $\\theta$ì¸ ìƒì„±ë§\n$s_t$ë¥¼ ì „ì²´ ì–´íœ˜ ë¶„í¬ì™€ ë§¤í•‘ì‹œí‚¨ë‹¤.\nex) $x_{t+1}$ì—ì„œ \u0026amp;G_\\theta(\\sdot | s_t)\u0026amp; í•™ìŠµ\në¶„ë³„ë§ì´ ìœ ì¶œí•´ì¤€ ì •ë³´ë¥¼ ê³„ì¸µ êµ¬ì¡°ë¥¼ í†µí•´ íš¨ê³¼ì ìœ¼ë¡œ í¬í•¨í•˜ì—¬ ë¬¸ì¥ì„ ìƒì„±í•œë‹¤.\nìƒì„±ë§ì˜ ê³„ì¸µ êµ¬ì¡°\r#\rDì˜ ìœ ì¶œëœ ì •ë³´ë¥¼ ì´ìš©í•˜ê¸° ìœ„í•œ MANAGER-WORKER ê³„ì¸µ êµ¬ì¡°\nMANAGER: $t$ì‹œì ë§ˆë‹¤ ì¶”ì¶œëœ $f_t$ë¥¼ í™œìš©í•´ $g_t$ ìƒì„±\n$f_t$ë¥¼ LSTMì— ì…ë ¥í•œ í›„ goal vector $g_t$ë¥¼ ìƒì„±í•œë‹¤.\n$$ \\begin{aligned}\\hat{g}t, h_t^M \u0026amp; =\\mathcal{M}\\left(f_t, h{t-1}^M ; \\theta_m\\right) \\g_t \u0026amp; =\\hat{g}_t /\\left|\\hat{g}_t\\right|\\end{aligned} $$\n$M:$ LSTM ëª¨ë¸\n$\\mathcal{M}:$ MANAGER ëª¨ë“ˆ\n$\\theta_m :$ $\\mathcal M$ì˜ íŒŒë¼ë¯¸í„°\n$h_t:$ tì‹œì ì˜ hidden state\nclass Manager(nn.Module):\rinit, init_params\ndef __init__(self, batch_size, hidden_dim, goal_out_size): super(Manager, self).__init__() self.batch_size = batch_size self.hidden_dim = hidden_dim self.goal_out_size = goal_out_size self.recurrent_unit = nn.LSTMCell( self.goal_out_size, #input size self.hidden_dim #hidden size ) self.fc = nn.Linear( self.hidden_dim, #in_features self.goal_out_size #out_features ) self.goal_init = nn.Parameter(torch.zeros(self.batch_size, self.goal_out_size)) self._init_params() def _init_params(self): for param in self.parameters(): nn.init.normal_(param, std=0.1) self.goal_init.data = truncated_normal( self.goal_init.data.shape ) forward\ndef forward(self, f_t, h_m_t, c_m_t): \u0026#34;\u0026#34;\u0026#34; f_t = feature of CNN from discriminator leaked at time t, it is input into LSTM h_m_t = ouput of previous LSTMCell c_m_t = previous cell state \u0026#34;\u0026#34;\u0026#34; h_m_tp1, c_m_tp1 = self.recurrent_unit(f_t, (h_m_t, c_m_t)) sub_goal = self.fc(h_m_tp1) # í•˜ìœ„ í…ì„œì˜ p-normì´ ê°’ maxnormë³´ë‹¤ ë‚®ë„ë¡ # ì°¨ì›ì— ë”°ë¼ ì…ë ¥ì˜ ê° í•˜ìœ„ í…ì„œê°€ ì •ê·œí™”ë˜ëŠ” í…ì„œë¥¼ ë°˜í™˜í•œë‹¤. sub_goal = torch.renorm(sub_goal, 2, 0, 1.0) return sub_goal, h_m_tp1, c_m_tp1 WORKER: MANAGERì˜ $g_t$ë¥¼ í† ëŒ€ë¡œ ë³´ìƒì„ ë†’ì´ëŠ” ë‹¤ìŒ ë‹¨ì–´ ìƒì„±\nMANAGERì˜ $g_t$ë¥¼ í¬í•¨í•˜ê¸° ìœ„í•´ ê°€ì¤‘ì¹˜ í–‰ë ¬ $W_\\psi$ë¡œ ìµœê·¼ cê°œì˜ ëª©í‘œë“¤ì— ëŒ€í•œ ì„ í˜• ë³€í™˜ì„ ìˆ˜í–‰í•œë‹¤.\nì´ë¥¼ í†µí•´ kì°¨ì›ì˜ goal embedding vector $w_t$ë¥¼ ì–»ëŠ”ë‹¤.\n$$ w_t=\\psi\\left(\\sum_{i=1}^c g_{t-i}\\right)=W_\\psi\\left(\\sum_{i=1}^c g_{t-i}\\right) $$\n$\\psi:$ ì„ í˜• ë³€í™˜(í–‰ë ¬ ê³±ì…ˆ) $$ \\begin{aligned}O_t, h_t^W \u0026amp; =\\mathcal{W}\\left(x_t, h_{t-1}^W ; \\theta_w\\right) \\G_\\theta\\left(\\cdot \\mid s_t\\right) \u0026amp; =\\operatorname{softmax}\\left(O_t \\cdot w_t / \\alpha\\right) \\end{aligned} $$\n$\\mathcal W:$ WORKER ëª¨ë“ˆ\n$x_t:$ input. (t ì‹œì ì˜ ë‹¨ì–´)\n$\\theta_w :$ $\\mathcal W$ì˜ íŒŒë¼ë¯¸í„°\n$O_t :$ í–‰ë ¬ ë‚´ì ìœ¼ë¡œ $w_t$ì™€ ì¶”ê°€ë¡œ ê²°í•©ëœ í–‰ë ¬. $|V| \\times k$\nëª¨ë“  ë‹¨ì–´ì— ëŒ€í•œ ë²¡í„° ì§‘í•©ì„ ì˜ë¯¸í•œë‹¤.\në”°ë¼ì„œ, $O_t \\cdot w_t$ ëŠ” ëª¨ë“  ë‹¨ì–´ì— ëŒ€í•´ logitì„ ê³„ì‚°í•œë‹¤.\n$\\alpha :$ generation entropyë¥¼ ì¡°ì ˆí•˜ê¸° ìœ„í•œ temperature parameter\nì¦‰, ìƒì„±ë˜ëŠ” ë¬¸ì¥ì˜ ì°¸ì‹ í•¨ì„ ì¡°ì ˆí•œë‹¤.\nsoftmaxë¥¼ í†µí•´ í˜„ì¬ê¹Œì§€ ìƒì„±ëœ ë‹¨ì–´ ì§‘í•© $s_t$ì—ì„œ ìµœì¢… action space ë¶„í¬ë¥¼ ê²°ì •í•œë‹¤.\nclass Worker(nn.Module):\rinit, init_params\ndef __init__(self, batch_size, vocab_size, embed_dim, hidden_dim, goal_out_size, goal_size): super(Worker, self).__init__() self.batch_size = batch_size self.vocab_size = vocab_size self.embed_dim = embed_dim self.hidden_dim = hidden_dim self.goal_out_size = goal_out_size self.goal_size = goal_size **self.emb = nn.Embedding(self.vocab_size, self.embed_dim)** **self.recurrent_unit = nn.LSTMCell(self.embed_dim, self.hidden_dim)** **self.fc = nn.Linear(self.hidden_dim, self.goal_size*self.vocab_size)** **self.goal_change = nn.Parameter(torch.zeros(self.goal_out_size, self.goal_size))** self._init_params() def _init_params(self): for param in self.parameters(): nn.init.normal_(param, std=0.1) forward\ndef forward(self, x_t, h_w_t, c_w_t): \u0026#34;\u0026#34;\u0026#34; x_t = last word h_w_t = last output of LSTM in Worker c_w_t = last cell state of LSTM in Worker \u0026#34;\u0026#34;\u0026#34; x_t_emb = self.emb(x_t) h_w_tp1, c_w_tp1 = self.recurrent_unit(x_t_emb, (h_w_t, c_w_t)) output_tp1 = self.fc(h_w_tp1) output_tp1 = output_tp1.view(self.batch_size, self.vocab_size, self.goal_size) return output_tp1, h_w_tp1, c_w_tp1 ìƒì„±ë§ G í•™ìŠµ\r#\rì•ì—ì„œ ì„¤ëª…í•œ Gì˜ ëª¨ë“  ê³¼ì •ì€ ë¯¸ë¶„ ê°€ëŠ¥í•œ êµ¬ì¡°ë¡œ ë˜ì–´ìˆë‹¤.\në”°ë¼ì„œ, REINFORCEì™€ ê°™ì€ policy gradient algorithmì„ ì ìš©í•˜ì—¬ ëª¨ë¸ì„ í•™ìŠµí•  ìˆ˜ ìˆë‹¤.\nLeakGAN ëª¨ë¸ì´ ìœ ì˜ë¯¸í•œ ì˜ë¯¸ íŒ¨í„´ì„ ì°¾ì„ ìˆ˜ ìˆë„ë¡ MANAGERì™€ WORKERëŠ” ê°œë³„ì ìœ¼ë¡œ í›ˆë ¨í•œë‹¤.\nMANAGER â€” ì‹ë³„ ê°€ëŠ¥í•œ feature spaceì—ì„œì˜ ì´ë™ ë°©í–¥ì„ ì˜ˆì¸¡í•˜ë„ë¡ í›ˆë ¨ëœë‹¤.\nMANAGERì˜ gradient\n$$ \\nabla_{\\theta_m}^{\\mathrm{adv}} g_t=-Q_{\\mathcal{F}}\\left(s_t, g_t\\right) \\nabla_{\\theta_m} d_{\\cos }\\left(f_{t+c}-f_t, g_t\\left(\\theta_m\\right)\\right) $$\n$Q_{\\mathcal{F}}\\left(s_t, g_t\\right)=Q\\left(\\mathcal{F}\\left(s_t\\right), g_t\\right)=Q\\left(f_t, g_t\\right)=\\mathbb{E}\\left[r_t\\right]$\nëª¬í…Œ ì¹´ë¥¼ë¡œ íƒìƒ‰ìœ¼ë¡œ ì¶”ì •í•œ í˜„ì¬ ì •ì±…ì— ëŒ€í•œ ë³´ìƒ ê¸°ëŒ“ê°’\n$d_{\\cos }:$ cosine similarity(similarityì¸ì§€ distanceì¸ì§€ í™•ì¸í•´ë³´ê¸°)\n$c$ë²ˆì˜ ì „í™˜ í›„ feature representationì˜ ë³€í™”$(f_{t+c} - f_t)$ì™€ ëª©ì  ë²¡í„° $g_t$ì˜ ì°¨ì´\nì†ì‹¤ í•¨ìˆ˜ì—ì„œëŠ” ë†’ì€ ë³´ìƒì„ ë‹¬ì„±í•˜ê¸° ìœ„í•´ $g_t$ê°€ íŠ¹ì§• ê³µê°„ì˜ ì „í™˜ê³¼ ì¼ì¹˜í•˜ë„ë¡ ê°•ì œí•œë‹¤.\n$$ \\begin{aligned}\u0026amp; \\nabla_{\\theta_w} \\mathbb{E}{s{t-1} \\sim G}\\left[\\sum_{x_t} r_t^I \\mathcal{W}\\left(x_t \\mid s_{t-1} ; \\theta_w\\right)\\right]\\ = \u0026amp; \\mathbb{E}{s{t-1} \\sim G, x_t \\sim \\mathcal{W}\\left(x_t \\mid s_{t-1}\\right)}\\left[r_t^I \\nabla_{\\theta_w} \\log \\mathcal{W}\\left(x_t \\mid s_{t-1} ; \\theta_w\\right)\\right]\\end{aligned} $$\nWORKER â€” MANAGERì˜ ì§€ì‹œë¥¼ ë”°ë¥´ë„ë¡ ë³´ìƒì´ ì£¼ì–´ì§„ë‹¤.\nREINFORCE ì•Œê³ ë¦¬ì¦˜ì„ í™œìš©í•˜ì—¬ ë³´ìƒì„ ìµœëŒ€í™”í•œë‹¤.\nì´ëŠ” $s_{t-1}$ ìƒíƒœì™€ í•¨ê»˜ WORKERê°€ ì·¨í•œ $x_t$ ì‘ì—…ì„ ìƒ˜í”Œë§í•˜ì—¬ ê·¼ì‚¬í•  ìˆ˜ ìˆë‹¤.\nWORKERì— ì œê³µë˜ëŠ” ë³´ìƒì€ ë‹¤ìŒê³¼ ê°™ì´ ì •ì˜ëœë‹¤.\n$$ r_t^I=\\frac{1}{c} \\sum_{i=1}^c d_{\\cos }\\left(f_t-f_{t-i}, g_{t-i}\\right) $$\nì‹¤ì œë¡œëŠ” $G_\\theta$ëŠ” ì ëŒ€ì  í•™ìŠµ ì „ì— ì‚¬ì „ í•™ìŠµì´ í•„ìš”í•˜ë‹¤.\nì‚¬ì „ í•™ìŠµ ì‹œ ì¼ê´€ì„±ì„ ìœ ì§€í•˜ê¸° ìœ„í•´ MANAGERì˜ ê¸°ìš¸ê¸°ë¥¼ í†µí•œ ë³„ë„ì˜ í›ˆë ¨ ì²´ê³„ë¥¼ ì‚¬ìš©í•œë‹¤.\n$$ \\nabla_{\\theta_m}^{\\mathrm{pre}} g_t=-\\nabla_{\\theta_m} d_{\\cos }\\left(\\hat{f}_{t+c}-\\hat{f}_t, g_t\\left(\\theta_m\\right)\\right) $$\n$\\hat{f}_t=\\mathcal{F}\\left(\\hat{s}t\\right), \\hat s_t, \\hat s{t + c}:$ ì‹¤ì œ í…ìŠ¤íŠ¸ì˜ ìƒíƒœ í•´ë‹¹ ìˆ˜ì‹ì€ ì•ì—ì„œ ì •ì˜í•œ MANAGER ë¯¸ë¶„ì‹ì—ì„œ $Q_{\\mathcal{F}}\\left(s_t, g_t\\right)$ê°€ $1$ì¸ ìƒíƒœì´ë‹¤.\nì‚¬ì „ í•™ìŠµì— ì‚¬ìš©ëœ ë°ì´í„°ëŠ” ëª¨ë‘ ì‹¤ì œ ë¬¸ì¥ì´ê¸° ë•Œë¬¸ì´ë‹¤.\nfeature spaceì—ì„œ ì‹¤ì œ ë¬¸ì¥ ìƒ˜í”Œì˜ ì „í™˜ì„ ëª¨ë°©í•˜ë„ë¡ í•™ìŠµëœë‹¤.\nMLE(Maximum Likelihood Estimation)ë¥¼ í†µí•´ í›ˆë ¨ëœë‹¤.\ní•™ìŠµ ê³¼ì •ì—ì„œ $G_\\theta$ì™€ $D_\\phi$ëŠ” ë²ˆê°ˆì•„ê°€ë©° í›ˆë ¨ëœë‹¤.\nìƒì„±ë§ì—ì„œë„ MANAGERì™€ WORKERëŠ” ë²ˆê°ˆì•„ê°€ë©° ì„œë¡œë¥¼ ê³ ì •í•œ ì±„ í›ˆë ¨ëœë‹¤.\në¶„ë³„ë§ $D_\\phi$\r#\r$D_\\phi:$ íŒŒë¼ë¯¸í„°ê°€ $\\phi$ì¸ ë¶„ë³„ë§\n1. Scalar Guiding Signal $D_\\phi(s)$ ì œê³µ\r#\rì „ì²´ ë¬¸ì¥ $s_T$ê°€ ìƒì„±ëœ í›„ ìƒì„±ë§ì´ íŒŒë¼ë¯¸í„°ë¥¼ ì¡°ì •í•  ë•Œ ê°€ì´ë“œ ì—­í• ì„ í•œë‹¤.\nì´ ë•Œ, $D_\\phi(s)$ëŠ” ë¬¸ì¥ì´ ê¸¸ì–´ì§ˆìˆ˜ë¡ ì •ë³´ëŸ‰ì´ ì ì–´ì§€ë¯€ë¡œ, ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ì¶”ê°€ì ì¸ ì •ë³´ $f_t$ë¥¼ ì œê³µí•œë‹¤.\nGuiding Signal(Leaked Features)\r#\r$$ D_\\phi(s)=\\operatorname{sigmoid}\\left(\\phi_l^{\\top} \\mathcal{F}\\left(s ; \\phi_f\\right)\\right)=\\operatorname{sigmoid}\\left(\\phi_l^{\\top} f\\right) $$\n$s:$ input. ìƒì„±ëœ ë¬¸ì¥. $\\mathcal F:$ CNN (íŠ¹ì§•ë§µ ì¶”ì¶œê¸°) $f : D_\\phi(s)$ ì˜ ë§ˆì§€ë§‰ Layerì—ì„œì˜ feature vector(ìœ ì¶œëœ ì •ë³´) $\\phi_l^{\\top}:$ ê°€ì¤‘ì¹˜ ë²¡í„° ì¦‰, $f$ì— ì˜í•´ Reward Valueê°€ ê²°ì •ë˜ê¸° ë•Œë¬¸ì—, ë³´ìƒì„ ë†’ì´ë„ë¡ feature(íŠ¹ì§•ë§µ)ì„ ë½‘ì•„ì•¼ í•œë‹¤.\nLeakGANì—ì„œëŠ” Feature Extractorë¡œ CNNì„ í™œìš©í•˜ì§€ë§Œ, LSTMì´ë‚˜ ë‹¤ë¥¸ ì‹ ê²½ë§ì„ í™œìš©í•˜ì—¬ êµ¬í˜„í•  ìˆ˜ë„ ìˆë‹¤.\n2. $f_t = s_t$ì—ì„œì˜ features\r#\r$f_t$ëŠ” ë¶„ë³„ë§ì´ ë¶„ë³„ì„ ìœ„í•´ì„œ ì“°ì´ëŠ” ì •ë³´ì´ê¸°ë„ í•˜ë‹¤.\në”°ë¼ì„œ, ì „ì—­ì ìœ¼ë¡œ ê´€ë¦¬ëœë‹¤.\n3. Learned Reward Functionì„ ì„¤ì •í•œë‹¤.\r#\rBlack Boxì¸ ê¸°ì¡´ RL ëª¨ë¸ë“¤ê³¼ ëŒ€ë¹„ëœë‹¤.\nclass Discriminator(nn.Module):\rtext ë¶„ë¥˜ë¥¼ ìœ„í•œ CNN ëª¨ë¸\nnum_filters (int): This is the output dim for each convolutional layer, which is the number of \u0026ldquo;filters\u0026rdquo; learned by that layer.\n__init__\ndef __init__(self, seq_len, num_classes, vocab_size, dis_emb_dim, filter_sizes, num_filters, start_token, goal_out_size, step_size, dropout_prob, l2_reg_lambda): super(Discriminator, self).__init__() self.seq_len = seq_len self.num_classes = num_classes self.vocab_size = vocab_size self.dis_emb_dim = dis_emb_dim self.filter_sizes = filter_sizes self.num_filters = num_filters self.start_token = start_token self.goal_out_size = goal_out_size self.step_size = step_size self.dropout_prob = dropout_prob self.l2_reg_lambda = l2_reg_lambda self.num_filters_total = sum(self.num_filters) #Building up layers **self.emb = nn.Embedding(self.vocab_size + 1, self.dis_emb_dim)** **self.convs = nn.ModuleList([ nn.Conv2d(1, num_f, (f_size, self.dis_emb_dim)) for f_size, num_f in zip(self.filter_sizes, self.num_filters) ])** **self.highway = nn.Linear(self.num_filters_total, self.num_filters_total)** #in_features = out_features = sum of num_festures self.dropout = nn.Dropout(p = self.dropout_prob) #Randomly zeroes some of the elements of the input tensor # with probability p using Bernouli distribution #Each channel will be zeroed independently onn every forward call **self.fc = nn.Linear(self.num_filters_total, self.num_classes)** highway\nclass Highway(nn.Module): #Highway Networks = Gating Function To Highway = y = xA^T + b def __init__(self, in_size, out_size): super(Highway, self).__init__() self.fc1 = nn.Linear(in_size, out_size) self.fc2 = nn.Linear(in_size, out_size) def forward(self, x): #highway = F.sigmoid(highway)*F.relu(highway) + (1. - transform)*pred # sets C = 1 - T g = F.relu(self.fc1) t = torch.sigmoid(self.fc2) out = g*t + (1. - t)*x return out tê°€ 1ì´ë©´ out = g\ntê°€ 0ì´ë©´ out = x\ntëŠ” torch.sigmoid(self.fc2)ì— ì˜í•´ ê²°ì •ë¨.\ntruncated_norm : ë‚œìˆ˜(ì ˆë‹¨ëœ ì •ê·œë¶„í¬)ë¡œ ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”ì— ì‚¬ìš©\nimport torch from scipy.stats import truncnorm import torch.nn as nn import torch.nn.functional as F import numpy as np def truncated_normal(shape, lower=-0.2, upper=0.2): size = 1 for dim in shape: size *= dim w_truncated = truncnorm.rvs(lower, upper, size=size) w_truncated = torch.from_numpy(w_truncated).float() w_truncated = w_truncated.view(shape) return w_truncated forward\ndef forward(self, x): \u0026#34;\u0026#34;\u0026#34; Argument: x: shape(batch_size * self.seq_len) type(Variable containing torch.LongTensor) Return: pred: shape(batch_size * 2) For each sequence in the mini batch, output the probability of it belonging to positive sample and negative sample. feature: shape(batch_size * self.num_filters_total) Corresponding to f_t in original paper score: shape(batch_size, self.num_classes) \u0026#34;\u0026#34;\u0026#34; #1. Embedding Layer #2. Convolution + maxpool layer for each filter size #3. Combine all the pooled features into a prediction #4. Add highway #5. Add dropout. This is when feature should be extracted #6. Final unnormalized scores and predictions emb = self.emb(x).unsqueeze(1) convs = [F.relu(conv(emb)).squeeze(3) for conv in self.convs] # [batch_size * num_filter * seq_len] pooled_out = [F.max_pool1d(conv, conv.size(2)).squeeze(2) for conv in convs] # [batch_size * num_filter] pred = torch.cat(pooled_out, 1) # batch_size * sum(num_filters) highway = self.highway(pred) highway = torch.sigmoid(highway)* F.relu(highway) + (1.0 - torch.sigmoid(highway))*pred features = self.dropout(highway) score = self.fc(features) pred = F.log_softmax(score, dim=1) #batch * num_classes return {\u0026#34;pred\u0026#34;:pred, \u0026#34;feature\u0026#34;:features, \u0026#34;score\u0026#34;: score} def l2_loss(self): W = self.fc.weight b = self.fc.bias l2_loss = torch.sum(W*W) + torch.sum(b*b) l2_loss = self.l2_reg_lambda * l2_loss return l2_loss í•™ìŠµ ê¸°ìˆ \r#\rBootstrapped Rescaled Activation\r#\rë°°ê²½\nSeqGANì˜ ì ëŒ€ì  í›ˆë ¨ ê³¼ì •ì—ì„œ, $D$ê°€ $G$ë³´ë‹¤ ë„ˆë¬´ ê°•í•œ ê²½ìš° ì‹¬ê°í•œ gradient ì†Œë©¸ ë¬¸ì œê°€ ë°œìƒí•œë‹¤.\nì¦‰, íŒŒë¼ë¯¸í„°ë¥¼ ê°±ì‹ í•˜ê¸°ì— ë³´ìƒì´ ë„ˆë¬´ ì‘ê¸° ë•Œë¬¸ì— $G$ì—ê²Œ ê°’ì„ ë„˜ê¸°ê¸° ì „ì— ìŠ¤ì¼€ì¼ ì¡°ì •ì´ í•„ìš”í•˜ë‹¤.\nRankGANë¡œë¶€í„° ì˜ê°ì„ ë°›ì€ rank ê¸°ë°˜ ë°©ë²•\në³´ìƒ í–‰ë ¬ : $R_{B\\times T}$\në‹¤ìŒ ìˆ˜ì‹ìœ¼ë¡œ $t$ë²ˆì§¸ ì—´ ë²¡í„° $R^t$ì˜ ìŠ¤ì¼€ì¼ì„ ì¬ì¡°ì •í•œë‹¤.\n$$ R_i^t=\\sigma\\left(\\delta \\cdot\\left(0.5-\\frac{\\operatorname{rank}(i)}{B}\\right)\\right) $$\n$\\text {rank}(i):$ ì—´ ë²¡í„°ì—ì„œ ië²ˆì§¸ ì›ì†Œì˜ ranking\n$\\delta :$ rescale ì‘ì—…ì˜ smoothnessë¥¼ ì¡°ì •í•˜ëŠ” í•˜ì´í¼ íŒŒë¼ë¯¸í„°\n$\\sigma(\\cdot) :$ í™œì„± í•¨ìˆ˜(ë…¼ë¬¸ì—ì„œëŠ” sigmoid)\në“±ê°„ê²© ì ìˆ˜ë¥¼ rank ê¸°ë°˜ìœ¼ë¡œ ë³´ë‹¤ íš¨ê³¼ì ì¸ ë¶„í¬ë¡œ ì¬êµ¬ì„±í•œë‹¤.\nì¥ì \nê° ë¯¸ë‹ˆ ë°°ì¹˜ì—ì„œ ë³´ìƒì˜ ê¸°ëŒ€ì™€ ë¶„ì‚°ì´ ì¼ì •í•˜ë‹¤.\nê°’ì„ ì•ˆì •ì‹œì¼œ ìˆ˜ì¹˜í˜• ë¶„ì‚°ì— ë¯¼ê°í•œ ì•Œê³ ë¦¬ì¦˜ì— ë„ì›€ì´ ëœë‹¤.\nëª¨ë“  ranking ë°©ë²•ê³¼ ë™ì¼í•˜ê²Œ, ëª¨ë¸ ìˆ˜ë ´ì„ ê°€ì†í™”í•˜ëŠ” gradient ì†Œì‹¤ ë¬¸ì œë¥¼ ë°©ì§€í•œë‹¤.\nInterleaved Training\r#\rì‚¬ì „ í›ˆë ¨ í›„ ì „ë¶€ GANìœ¼ë¡œ í•™ìŠµí•˜ëŠ” ëŒ€ì‹  ì¼ë¶€ëŠ” ì§€ë„ í•™ìŠµ(ex â€” MLE)ìœ¼ë¡œ, ì¼ë¶€ëŠ” ì ëŒ€ì  í•™ìŠµ(ex â€” GAN)ì„ ì ìš©í•œë‹¤.\nex) 1 epoch ì§€ë„ í•™ìŠµ + 15 epoch ì ëŒ€ì  í•™ìŠµ\nGANì´ local minimaë¥¼ ì œê±°í•˜ëŠ” ë° ë„ì›€ì„ ì¤€ë‹¤. mode collapseë¥¼ ì˜ˆë°©í•œë‹¤. ì‚½ì…ëœ ì§€ë„ í•™ìŠµì´ ìƒì„± ëª¨ë¸ì— ëŒ€í•´ ì•”ì‹œì  ê·œì œë¥¼ ìˆ˜í–‰í•˜ì—¬ MLE ê²°ê³¼ë¡œë¶€í„° ë„ˆë¬´ ë©€ë¦¬ ë–¨ì–´ì§€ëŠ” ê²ƒì„ ë°©ì§€í•œë‹¤.\nTemperature Control\r#\rë³¼ì¸ ë§Œ temperature $\\alpha$.\níƒí—˜ì™€ íƒì‚¬ì˜ ê· í˜•ì„ ë§ì¶”ëŠ” ë° ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ìš”ì†Œ\nëª¨ë¸ í›ˆë ¨ ì‹œ ë†’ì€ temperature ì„¤ì • ìƒ˜í”Œ ìƒì„±ì„ ìœ„í•´ ëª¨ë¸ ì ìš© ì‹œ ë‚®ì€ temperature ì„¤ì • Pseudo Code\r#\rí•„ìš”í•œ ìš”ì†Œ\r#\rê³„ì¸µ êµ¬ì¡° ìƒì„±ë§ $G(Î¸_m, Î¸_w)$\nMANAGERì™€ WORKERë¡œ êµ¬ì„±\në¶„ë³„ë§ $D(Ï†)$\nì´ì§„ ë¶„ë¥˜ê¸°\ní›ˆë ¨ ë°ì´í„° ì…‹\nì‹œí€€ìŠ¤ ë°ì´í„° ì§‘í•© $S = {X_1:T}$\nì•Œê³ ë¦¬ì¦˜ ë‹¨ê³„\r#\ríŒŒë¼ë¯¸í„° ì´ˆê¸°í™”\n$G(Î¸_m, Î¸_w), D(Ï†)$ë¥¼ ëœë¤ ê°€ì¤‘ì¹˜ $Î¸_m, Î¸_w, Ï†$ë¡œ ì´ˆê¸°í™”\nì‚¬ì „ í•™ìŠµ\n$D(Ï†)$ ì‚¬ì „ í•™ìŠµ\n$D(Ï†)$ë¥¼ ì‹œí€€ìŠ¤ ë°ì´í„° ì§‘í•© $S$ë¥¼ ì–‘ì„± ìƒ˜í”Œë¡œ, $G$ì—ì„œ ìƒì„±ëœ ì‹œí€€ìŠ¤ë¥¼ ìŒì„± ìƒ˜í”Œë¡œ ì‚¬ìš©í•˜ì—¬ ì‚¬ì „ í•™ìŠµí•œë‹¤.\nì´ë•Œ, $D(Ï†)$ëŠ” íŠ¹ì§• ì¶”ì¶œê¸°($\\mathcal F$)ì™€ ì¶œë ¥ ë ˆì´ì–´(sigmoid)ë¡œ êµ¬ì„±ëœë‹¤.\n$$ D_\\phi(s)=\\operatorname{sigmoid}\\left(\\phi_l^{\\top} \\mathcal{F}\\left(s ; \\phi_f\\right)\\right)=\\operatorname{sigmoid}\\left(\\phi_l^{\\top} f\\right) $$\n$G(Î¸_m, Î¸_w)$ ì‚¬ì „ í•™ìŠµ\n$D(Ï†)$ë¡œë¶€í„° ìœ ì¶œëœ ì •ë³´ë¥¼ ì‚¬ìš©í•˜ì—¬ í•™ìŠµí•œë‹¤.\nì‚¬ì „ í•™ìŠµì„ ìˆ˜ë ´í•  ë•Œê¹Œì§€ ë²ˆê°ˆì•„ ìˆ˜í–‰í•œë‹¤.\nì ëŒ€ì  í•™ìŠµ\nìƒì„±ë§ ë‹¨ê³„ (g-steps)\n$G(Î¸)$ë¥¼ ì‚¬ìš©í•˜ì—¬ ì‹œí€€ìŠ¤ $Y_1:T$ë¥¼ ìƒì„±\nê° $t$ì— ëŒ€í•´ $D(Ï†)$ë¡œë¶€í„° ìœ ì¶œëœ ì •ë³´ $f_t$ë¥¼ ì €ì¥\n$Q\\left(f_t, g_t\\right)=\\mathbb{E}\\left[r_t\\right]$ì„ í†µí•´ Monte Carlo Searchë¥¼ ì‚¬ìš©í•˜ì—¬ $Q(f_t, g_t)$ë¥¼ ì–»ì–´ë‚¸ë‹¤.\nMANAGERë¡œë¶€í„° ê³„ì‚°ëœ ë°©í–¥ $g_t$ë¥¼ ì–»ëŠ”ë‹¤.\nWORKER ë§¤ê°œë³€ìˆ˜ $Î¸_w, Ïˆ$, softmaxë¥¼ ê°±ì‹ í•œë‹¤.\n$$ \\begin{aligned} \u0026amp; \\nabla_{\\theta_w} \\mathbb{E}{s{t-1} \\sim G}\\left[\\sum_{x_t} r_t^I \\mathcal{W}\\left(x_t \\mid s_{t-1} ; \\theta_w\\right)\\right] \\ = \u0026amp; \\mathbb{E}{s{t-1} \\sim G, x_t \\sim \\mathcal{W}\\left(x_t \\mid s_{t-1}\\right)}\\left[r_t^I \\nabla_{\\theta_w} \\log \\mathcal{W}\\left(x_t \\mid s_{t-1} ; \\theta_w\\right)\\right] \\end{aligned} $$\nMANAGER ë§¤ê°œë³€ìˆ˜ $Î¸_m$ì„ ê°±ì‹ í•œë‹¤.\n$$ \\nabla_{\\theta_m}^{\\mathrm{adv}} g_t=-Q\\left(f_t, g_t\\right) \\nabla_{\\theta_m} d_{\\cos }\\left(\\mathcal{F}\\left(s_{t+c}\\right)-\\mathcal{F}\\left(s_t\\right), g_t\\left(\\theta_m\\right)\\right) $$\në¶„ë³„ë§ ë‹¨ê³„ (d-steps)\ní˜„ì¬ $G(Î¸_m, Î¸_w)$ë¥¼ ì‚¬ìš©í•˜ì—¬ ìŒì„± ì˜ˆì œë¥¼ ìƒì„±í•˜ê³  ì£¼ì–´ì§„ ì–‘ì„± ì˜ˆì œ Sì™€ ê²°í•©í•œë‹¤.\nk-epoch ë™ì•ˆ $D(Ï†)$ë¥¼ í›ˆë ¨í•œë‹¤.\n$$ D_\\phi(s)=\\operatorname{sigmoid}\\left(\\phi_l \\cdot \\mathcal{F}\\left(s ; \\phi_f\\right)\\right)=\\operatorname{sigmoid}\\left(\\phi_l, f\\right) $$\nLeakGANì´ ìˆ˜ë ´í•  ë•Œê¹Œì§€ ë°˜ë³µí•œë‹¤.\nì°¸ê³ \r#\rLong Text Generation via Adversarial Training with Leaked Information\nPapers with Code - Text Generation\nLeakGAN Implement code with PyTorch - github\n"},{"id":3,"href":"/posts/2023-06-19-%ED%94%84%EB%A1%9C%EC%A0%9D%ED%8A%B8-%EC%8B%9D%EB%B3%84-%EB%B0%8F-%EA%B4%80%EB%A6%AC/","title":"1. í”„ë¡œì íŠ¸ ì‹ë³„ ë° ê´€ë¦¬","section":"Blog","content":"í”„ë¡œì íŠ¸ë¥¼ ì–´ë–»ê²Œ ì‹œì‘í•˜ëŠëƒì— ëŒ€í•œ ê´€ì \ní”„ë¡œì íŠ¸ë¥¼ ì‹œì‘í•˜ê¸° ì „, ì–´ë–»ê²Œ í‰ê°€í•  ì§€, ê·¸ë¦¬ê³  ì–´ë–»ê²Œ ê³„íšì„ ì„¸ìš¸ì§€ë¥¼ ì„¤ëª…í•œë‹¤.\ní”„ë¡œì íŠ¸ ì‹ë³„\r#\r1. System Request ì‘ì„±\r#\rProject Sponsor\ní”„ë¡œì íŠ¸ì˜ ì•„ì´ë””ì–´ ì œê³µì\ní”„ë¡œì íŠ¸ì˜ ê¸ˆì „ì  ì§€ì›ì ì•„ë‹˜.\nBusiness Need\nBusiness requirements\nBusiness Value : CEOì˜ ì…ì¥ì—ì„œ ê°€ì¥ ë¨¼ì € ë³´ê²Œëœë‹¤.\ní”„ë¡œì íŠ¸ì˜ Business Valueê°€ ëª…í™•í•´ì•¼ í•œë‹¤. Economic feasibilityë¶€ë¶„ì„ ë¯¸ë¦¬ ê³ ë¯¼í•´ë´ì•¼ í•œë‹¤. Special issues or constraints\ní”„ë¡œì íŠ¸ ë§ˆê° ê¸°ê°„ ë“±\nex) í¬ë¦¬ìŠ¤ë§ˆìŠ¤ ì‹œì¦Œ ì „ì— ì™„ì„±í•´ì•¼ í•œë‹¤.\n2. Feasibility Analysis(íƒ€ë‹¹ì„± ë¶„ì„)\r#\rí”„ë¡œì íŠ¸ë¥¼ ì§„í–‰í•˜ê¸° ì „ì— ë¶„ì„í•´ì•¼ í•œë‹¤.\nTechnical feasibility\nê¸°ìˆ ì ìœ¼ë¡œ ê°œë°œì´ ê°€ëŠ¥í•œì§€ì— ëŒ€í•œ ë¶„ì„\në„ë©”ì¸ê³¼ì˜ ì¹œìˆ™ë„\nìš°ë¦¬ê°€ í‰ì†Œ ê°œë°œí•˜ë˜ ë¶„ì•¼ê°€ ì–¼ë§ˆë‚˜ í•´ë‹¹ ë¶„ì•¼ì™€ ê°€ê¹Œìš´ê°€ì— ëŒ€í•œ ê²ƒ\nê¸°ìˆ ì  ì¹œìˆ™ë„\ní”„ë¡œì íŠ¸ í¬ê¸°\nê¸°ì¡´ ì‹œìŠ¤í…œê³¼ì˜ ê²½ìŸì„±\nê¸°ì¡´ ì‹œìŠ¤í…œê³¼ì˜ í˜¸í™˜ì„±\nê¸°ì¡´ í”„ë¡œì íŠ¸ì™€ì˜ ë¹„êµ\nì™¸ë¶€ ì „ë¬¸ê°€ì˜ í‰ê°€\nEconomic feasibility\ní•´ë‹¹ í”„ë¡œì íŠ¸ê°€ ëˆì´ ë˜ëŠ”ì§€ì— ëŒ€í•œ ë¶„ì„\nNet present value(NPV)\nReturn on investment(ROI)\ní´ìˆ˜ë¡ ì¢‹ë‹¤.\nBreak even point(BEP) : ì†ìµë¶„ê¸°ì \në¹ ë¥¼ìˆ˜ë¡ ì¢‹ë‹¤.\nStep\nIdentify costs and benefits\nData Conversion Costs\nì‹œìŠ¤í…œ ë³€ê²½ì‹œ ê¸°ì¡´ì˜ ë°ì´í„°ë¥¼ ìƒˆë¡œìš´ ì‹œìŠ¤í…œìœ¼ë¡œ ì˜®ê¸°ëŠ” ë° ì†Œëª¨ë˜ëŠ” ë¹„ìš©\nTangible Benefits\nëˆìœ¼ë¡œ ì‰½ê²Œ í™˜ì‚° ê°€ëŠ¥í•œ ì´ìµ\nIntangible Benefits\nëˆìœ¼ë¡œ ì‰½ê²Œ í™˜ì‚°í•˜ê¸° ì–´ë ¤ìš´ ì´ìµ\nê°€ëŠ¥í•˜ë©´ Intangible Benefitì„ ëˆìœ¼ë¡œ í™˜ì‚°í•˜ì—¬ ìˆ˜ì¹˜í™”í•˜ëŠ” ê²ƒì´ ì¢‹ë‹¤. ê°€ì¥ ì–´ë µë‹¤. Assign Cost and Benefit Values\nëˆìœ¼ë¡œ í™˜ì‚°í•˜ê¸°\nDetermine Cash Flow\nì¬ë¬´ì¬í‘œ ë¶„ì„\nAssess Financial Viability\ní˜„ì¬ê°€ì¹˜ë¡œì˜ í™˜ì‚° : ë³µë¦¬ë¥¼ í™œìš©\nNPV \u0026gt; 0 â‡’ í”„ë¡œì íŠ¸ê°€ ê´œì°®ë‹¤.\nOrganizational feasibility\nìœ ì €ê°€ ì‚¬ìš©í•  ê²ƒì¸ê°€ì— ëŒ€í•œ ë¶„ì„\nStrategic alignment\nì´í•´ê´€ê³„ì ë¶„ì„(Stakeholder analysis)\nProject champion(s)\nOrganizational management\nì˜ˆì‚°ì— ëŒ€í•œ ë¶„ì„\nSystem users\n3. Project Selection Issues\r#\rPortfolio Management\níšŒì‚¬ ì „ì²´ì˜ í¬íŠ¸í´ë¦¬ì˜¤ ê´€ì ì—ì„œ ì§„í–‰í• ì§€ ì—¬ë¶€ë¥¼ ê²°ì •\níšŒì‚¬ì—ì„œ ì§„í–‰í•˜ëŠ” ë‹¤ë¥¸ í”„ë¡œì íŠ¸ì™€ì˜ ë°¸ëŸ°ìŠ¤ ë“±ì„ ê³ ë ¤, ìµœì¢…ì ìœ¼ë¡œ ì§„í–‰ì—¬ë¶€ë¥¼ ê²°ì •.\ní•´ë‹¹ ë°©ì‹ì´ ë°˜ë“œì‹œ íšŒì‚¬ ë‚´ì—ì„œ ì´ë¤„ì§€ì§€ ì•Šì„ ìˆ˜ë„ ìˆë‹¤.\ní”„ë¡œì íŠ¸ì˜ ìš”êµ¬ì‚¬í•­(RFP)ë§Œ ì‘ì„±í•˜ë©´, ì—¬ëŸ¬ ì™¸ë¶€ì—…ì²´ë“¤ì—ì„œ í”„ë¡œì íŠ¸ ì œì•ˆì„œ(proposal)ì„ ë°›ì•„ì„œ ì—…ì²´ë¥¼ ì„ íƒí•˜ê³ , ì™„ì„±ëœ í”„ë¡œì íŠ¸ë¥¼ ë„˜ê²¨ë°›ëŠ” ë°©ì‹ë„ ì¡´ì¬í•œë‹¤.\nRFPë¥¼ ì‘ì„±í•˜ê¸° ìœ„í•´ í”„ë¡œì íŠ¸ ê´€ë¦¬ì— ëŒ€í•œ ë‚´ìš©ì„ ì•Œì•„ë‘¬ì•¼ í•œë‹¤.\nì›í•˜ëŠ” í”„ë¡œì íŠ¸ì— ì†Œìš”ë˜ëŠ” ì‹œê°„, ë¹„ìš©, ë…¸ë™ë ¥ì„ ë¶„ì„í•  ìˆ˜ ìˆì–´ì•¼ í”„ë¡œì íŠ¸ë¥¼ í•©ë¦¬ì ìœ¼ë¡œ ìš”ì²­í•  ìˆ˜ ìˆë‹¤.\ní”„ë¡œì íŠ¸ ê´€ë¦¬\r#\rí”„ë¡œì íŠ¸ì˜ ê¸°ê°„ì´ ì–´ë–»ê²Œ ë˜ëŠ”ì§€, ë…¸ë™ë ¥ì€ ì–¼ë§ˆë‚˜ ë“¤ì–´ê°€ëŠ”ì§€ì— ëŒ€í•œ ê´€ì \n1. Identifying project size\r#\rë¹„ìš©ì„ ê²°ì •í•˜ëŠ” ì¤‘ìš”í•œ ìš”ì†Œ\nProject Estimation\nMathodology in use Actual previous projects Experienced developers í”„ë¡œì íŠ¸ì˜ ë¹„ìš©ê³¼ ì‹œê°„ê³¼ í¬ê¸°ëŠ” ì„œë¡œ ì—°ê´€ë˜ê¸° ë•Œë¬¸ì— ì‹œê°„ì˜ íë¦„ì— ë”°ë¼ ê³„ì† ë¶„ì„í•´ì•¼ í•œë‹¤.\nfunction points\nì—¬ê¸°ì„œ functionì€ í”„ë¡œê·¸ë˜ë° ì½”ë“œ ìƒì˜ functionì´ ì•„ë‹Œ ì§„ì§œ ê¸°ëŠ¥ì„ ì˜ë¯¸í•œë‹¤.\nê¸°ëŠ¥ ìœ í˜•\nInputs\nì‹œìŠ¤í…œì—ì„œ ì…ë ¥ì´ ëª‡ê°œë‚˜ í•„ìš”í•œì§€ì— ëŒ€í•œ ê²ƒ.\në§ˆì°¬ê°€ì§€ë¡œ ë‹¨ìˆœí•œ íŒŒë¼ë¯¸í„°ë¥¼ ì˜ë¯¸í•˜ëŠ” ê²ƒì´ ì•„ë‹Œ ì…ë ¥ ë°ì´í„°ì˜ ì¢…ë¥˜ë¥¼ ì˜ë¯¸í•œë‹¤.\nOutputs\nQueries\nì°¸ì¡°í•˜ëŠ” ë°ì´í„°ì˜ ê°œìˆ˜? ì¢€ ë” ì•Œì•„ë³¼ ê²ƒ.\nFiles\nProgram Interfaces\nì™¸ë¶€ Interfaceì„ ì˜ë¯¸í•œë‹¤.\nfunction point ì˜ˆì‹œ\ní•™ì ì„ ì¡°íšŒí•˜ëŠ” ê²ƒì´ ë‹¨ìˆœí•˜ê²Œ ì¶œë ¥ë§Œ í•œë‹¤ë©´ EOë¼ê³  ë³¼ ìˆ˜ ìˆì§€ë§Œ,\ní•™ì ì„ ê°€ê³µí•˜ì—¬ ì¶œë ¥í•˜ëŠ” ê²½ìš° EQê°€ ëœë‹¤.\nìœ„ì˜ í•™ìƒ í•™ì  ì¡°íšŒëŠ” EQì— í•´ë‹¹í•œë‹¤.(ì˜¤íƒ€ë¼ê³  ì§ì‘ë¨)\nLines of codes\nProjectì˜ code line ìˆ˜ë¥¼ ì¸¡ì •í•˜ë©´ ëŒ€ëµì ìœ¼ë¡œ ì•Œì•„ë‚¼ ìˆ˜ ìˆë‹¤.\nfunction pointë¥¼ ê³„ì‚°í•˜ë©´ ê¸°ëŠ¥ë³„ í•„ìš”í•œ ì½”ë“œìˆ˜ê°€ ë‚˜ì˜¨ë‹¤.\nPerson Months\ní•œ ì‚¬ëŒì´ í•œë‹¬ì— í•´ë‚´ëŠ” ì—…ë¬´ëŸ‰\në§Œì•½ 14 Person Monthsë¼ë©´ í•œ ì‚¬ëŒì´ 14ë‹¬ë™ì•ˆ í•´ì•¼í•˜ëŠ” ì‘ì—…ëŸ‰ì´ë¼ëŠ” ê²ƒ.\n14ëª…ì´ ì‘ì—…í•œë‹¤ê³  í•´ì„œ ì‘ì—…ê¸°ê°„ì´ 1ë‹¬ë¡œ ì¤„ì§€ëŠ” ì•ŠëŠ”ë‹¤.\nâ‡’ ì—…ë¬´ì—ëŠ” ì„ í›„ê´€ê³„ê°€ ì¡´ì¬í•˜ê¸° ë•Œë¬¸ì´ë‹¤.\n2. Creating and managing the workplan\r#\rí”„ë¡œì íŠ¸ì˜ í¬ê¸°ê°€ ê²°ì •ë˜ê³  ë‚œ í›„, PMì€ ê³„íšì„ ì§œì•¼í•œë‹¤.\nê³„íšì„ ì§œê¸° ìœ„í•´ì„œ\ní•´ì•¼í•˜ëŠ” ì—…ë¬´ë¥¼ ë‚˜ì—´í•´ì•¼ í•˜ê³ \nì‚¬ëŒì—ê²Œ ì—…ë¬´ë¥¼ í• ë‹¹í•´ì•¼ í•œë‹¤.\nì—…ë¬´ê°€ ë„ˆë¬´ í¬ë‹¤ë©´ ì—…ë¬´ë¥¼ ì„¸ë¶„í™”ì‹œì¼œì•¼ í•œë‹¤.\nWork Breakdown Structure\nworkplan\nTask name\nDuration of Dask\nCurrent task status\nTask dependencies\nMilestone (dates)\nworkplan ì˜ˆì‹œ\ní•˜ì§€ë§Œ workplanìœ¼ë¡œëŠ” íŒŒì•…í•˜ê¸°ê°€ ë¶ˆí¸í•˜ë‹¤. ì‹œê°í™”ê°€ í•„ìš”í•˜ë‹¤.\nGantt Chart\nê°„ë‹¨í•œ ëª¨ë¸\nì—…ë¬´ì˜ ê¸°ê°„ë§Œì„ í‘œì‹œí•´ë†“ì€ ë°”í˜• ê·¸ë˜í”„\nì •ë³´ê°€ ë„ˆë¬´ ì ë‹¤. ëˆ„ê°€ í•´ë‹¹ ì—…ë¬´ë¥¼ í•˜ëŠ”ì§€, dependencyì— ëŒ€í•œ ì •ë³´ ë“±ë“±ì´ ì—†ë‹¤. ì •ì‹ ëª¨ë¸\nCritical Path\nì–´ë–¤ ì—…ë¬´ê°€ ì¡°ê¸ˆì´ë¼ë„ ë°€ë¦°ë‹¤ë©´ ì „ì²´ ì—…ë¬´ê°€ ë°€ë¦¬ëŠ” ê²½ìš°\nì—…ë¬´ê°„ì˜ ê°„ê²©ì´ ì „í˜€ ì—†ëŠ” ê²½ìš°\nìœ„ì˜ í‘œì—ì„œ Alanì˜ pathê°€ Critical pathì— í•´ë‹¹í•œë‹¤.\në³´í†µ Critical PathëŠ” ê°€ì¥ ìœ ëŠ¥í•œ ì‚¬ëŒì´ ë§¡ê²Œ ëœë‹¤.\nAlanì´ ê°€ì¥ ìœ ëŠ¥í•˜ë‹¤ê³  ì§ì‘í•  ìˆ˜ ìˆë‹¤.\nPERT Chart\nX : ì§„í–‰ì™„ë£Œ \\ : ì§„í–‰ì¤‘ í”„ë¡œì íŠ¸ ì§„í–‰ì— ëŒ€í•œ ì˜ˆì¸¡ : ì •ë°€í•œ ì˜ˆì¸¡ì€ ì ˆëŒ€ ë¶ˆê°€ëŠ¥í•˜ë‹¤.\níƒœí’ì˜ ê²½ë¡œë¥¼ ì˜ˆì¸¡í•˜ëŠ” ëª¨ë¸ê³¼ í¡ì‚¬í•˜ê²Œ ì˜ˆìƒì´ ê°€ëŠ¥í•˜ë‹¤.\nê³¼ëŒ€í‰ê°€ : ê´œì°®ë‹¤. ê³¼ì†Œí‰ê°€ : êµ‰ì¥íˆ ìœ„í—˜í•˜ë‹¤. ë”°ë¼ì„œ, ì²˜ìŒ ì˜ˆìƒì„ í• ë•Œ ê³¼ëŒ€í‰ê°€ë¥¼ í•˜ëŠ” ê²ƒì´ ì¢‹ë‹¤.\n3. Staffing the project\r#\r4. Coordinating project activities\r#\r"},{"id":4,"href":"/posts/2023-08-06-5F-%EC%A3%BC%EA%B0%84%ED%9A%8C%EA%B3%A0%EB%A1%9D-230806/","title":"5F 8ì›” 1ì£¼ì°¨ ì£¼ê°„íšŒê³ ë¡","section":"Blog","content":"8ì›” 2ì¼ ë„¤íŠ¸ì›Œí‚¹ ë°ì´ ë•Œ ê¸°ì—…ì—ì„œ ì±„ìš©ë‹´ë‹¹ì ë¶„ë“¤ê³¼ ë©´ë‹´í•  ê¸°íšŒê°€ ìˆì—ˆë‹¤.\nì–´ë–¤ ìº í¼ê°€ ê¸°ì—…ê´€ê³„ìë¶„ê»˜ *â€œì‹ ì… ê°œë°œìì—ê²Œ ê°€ì¥ ì¤‘ìš”í•œ ê²ƒì´ ë¬´ì—‡ì´ëƒâ€*ê³  ë¬¼ì—ˆê³ , ì´ì— ëŒ€í•œ ëŒ€ë‹µìœ¼ë¡œ\në¬¸ì œë¥¼ ì˜ ì •ì˜í•˜ê³ , ë‹¤ì–‘í•œ ì†”ë£¨ì…˜ì„ ì‹œë„í•˜ëŠ” ì—­ëŸ‰\nì´ ì¤‘ìš”í•˜ë‹¤ê³  ë§ì”€í•´ì£¼ì…¨ë‹¤.\nì´ ë§ì„ ìº í¼ 50ëª…ê³¼ í•¨ê»˜ ë“£ê³  ìˆì—ˆëŠ”ë°, ë§ˆì¹˜ ë‚˜ë¥¼ í•´ì£¼ëŠ” ì¶©ê³ ì²˜ëŸ¼ ëŠê»´ì¡Œë‹¤.\nì•„ë¬´ë˜ë„ PMìœ¼ë¡œ í”„ë¡œì íŠ¸ë¥¼ ì£¼ë„í•˜ë‹¤ë³´ë‹ˆ, ë¬¸ì œ ì •ì˜ì™€ ë‹¤ì–‘í•œ ì†”ë£¨ì…˜ ì œì•ˆì—ì„œ ë§ì€ ê³ ë¯¼ì„ í–ˆê¸° ë•Œë¬¸ì— ë” í¬ê²Œ ì™€ë‹¿ì•˜ë˜ ê²ƒì´ ì•„ë‹ê¹Œ ìƒê°í•œë‹¤.\nê·¸ ì¶©ê³ ë¥¼ ë“£ê³  ë³´ë‹ˆ ë‚˜ëŠ” ì¼ë¶€ ì†”ë£¨ì…˜ì— ë§¤ëª°ë˜ëŠ” ì„±í–¥ì´ ìˆì—ˆë‹¤.\ní•˜ë‚˜ì˜ í•´ê²°ì±…ì´ ë³´ì´ë©´ ê·¸ í•´ê²°ì±…ì„ ì‹œë„í•´ë³´ê³ , ì•ˆë˜ë©´ ë‹¤ë¥¸ ë°©ë²•ì„ ê³ ë¯¼í•˜ê¸°ë³´ë‹¨, ê·¸ í•´ê²°ì±…ì„ ë˜ê²Œ í•˜ëŠë¼ ë˜ ì—ë„ˆì§€ë¥¼ ìŸëŠ”ë‹¤.\nì—¬ê¸°ê¹Œì§€ëŠ” íƒêµ¬ì‹¬ì´ê³  ê°œë°œ ì—­ëŸ‰ì„ í‚¤ìš°ëŠ”ë° ë„ì›€ì´ ë˜ê¸° ë•Œë¬¸ì— ë¬¸ì œê°€ ì—†ë‹¤.\ní•˜ì§€ë§Œ ë§Œì•½ í•˜ë‚˜ì˜ í•´ê²°ì±…ì„ í†µí•´ ëë‚´ ì„±ê³µí•˜ì§€ ëª»í•œ ê²½ìš°, ìµœì´ˆì˜ ë¬¸ì œ ìƒí™©ì„ í•´ê²°í•  ìˆ˜ ì—†ë‹¤ëŠ” ê²°ë¡ ì„ ë‚´ë ¤ë²„ë¦° ê²½í—˜ì´ ê½¤ ìˆë‹¤.\nì¦‰, ê¹Šê²Œ íƒêµ¬í•œ ì´í›„ì—ëŠ” ë˜ë‹¤ë¥¸ ì†”ë£¨ì…˜ì„ ì°¾ëŠ” ê²ƒì— í”¼ë¡œê°ì„ ëŠê»´ ë¬¸ì œ í•´ê²°ì„ í¬ê¸°í•´ë²„ë¦°ë‹¤.\nì´ëŸ° ìƒí™©ì„ ì–´ë–»ê²Œ í•´ê²°í• ê¹Œ? ë‚´ê°€ ìƒê°í•œ í•´ë‹µì€ ì´ë ‡ë‹¤.\nìµœì´ˆì— ë¬¸ì œ ìƒí™©ì„ ëª…í™•í•˜ê²Œ ì •ì˜í•˜ê³ , ì†”ë£¨ì…˜ì„ ì—¬ëŸ¬ ê°œ ë‚˜ì—´í•´ë³¸ í›„ì— ìˆ˜í–‰í•˜ì. ì´ë ‡ê²Œ ë˜ë©´ í•˜ë‚˜ì˜ ì†”ë£¨ì…˜ì´ ì‹¤íŒ¨í•˜ë”ë¼ë„, ë‚˜ë¨¸ì§€ ì†”ë£¨ì…˜ì´ ë‚¨ì•„ìˆë‹¤ëŠ” ì‚¬ì‹¤ì„ ë§ê°í•˜ì§€ ì•Šì„ ìˆ˜ ìˆë‹¤.\në˜í•œ, ë‹¤ë¥¸ ì†”ë£¨ì…˜ë“¤ì´ ë‚¨ì•„ìˆë‹¤ëŠ” ì‚¬ì‹¤ì„ ì¸ì§€í•˜ê³  ìˆê¸° ë•Œë¬¸ì—, í•˜ë‚˜ì˜ ì†”ë£¨ì…˜ì— ê³¼ë„í•œ ë…¸ë ¥ì„ íˆ¬ìí•˜ì§€ ì•Šì„ ìˆ˜ ìˆê² ë‹¤.\n"},{"id":5,"href":"/posts/2023-05-20-Alexnet-%EB%AA%A8%EB%8D%B8%EC%9D%98-%ED%8C%8C%EB%9D%BC%EB%AF%B8%ED%84%B0-%EC%88%98-%EA%B3%84%EC%82%B0%ED%95%B4%EB%B3%B4%EA%B8%B0/","title":"Alexnet ëª¨ë¸ì˜ íŒŒë¼ë¯¸í„° ìˆ˜ ê³„ì‚°","section":"Blog","content":"\rConv Layer\r#\rLayer 1 íŒŒë¼ë¯¸í„° ìˆ˜ = 11 * 11 * 3 * 48 * 2 â‡’ 35k\nì…ë ¥ : 224 * 224 * 3\nfilter : 11 * 11 * (3)\n3ì€ ìƒëµë˜ì–´ ìˆì§€ë§Œ, ì…ë ¥ í¬ê¸°ì™€ ë™ì¼í•œ ì±„ë„ì„ ê°€ì§ˆ ê²ƒì´ê¸° ë•Œë¬¸ì— 3ìœ¼ë¡œ ìœ ì¶”í•  ìˆ˜ ìˆë‹¤.\nëª¨ë¸ ì´ë¯¸ì§€ìƒ ì»¤ë„ì´ ìœ„ ì•„ë˜ë¡œ ë‘ ê°œì´ê¸° ë•Œë¬¸ì— * 2ë¥¼ í–ˆë‹¤.\ngpu ë©”ëª¨ë¦¬ ìš©ëŸ‰ ë“±ì˜ ì´ìœ ë¡œ ì´ì²˜ëŸ¼ êµ¬ì„±í•˜ëŠ” ê²½ìš°ê°€ ë§ë‹¤.\nLayer 2 íŒŒë¼ë¯¸í„° ìˆ˜ = 5 * 5 * 48 * 128 * 2 â‡’ 307k\nLayer 3 íŒŒë¼ë¯¸í„° ìˆ˜ = 3 * 3 * 128 * 2 * 192 * 2 â‡’ 663k\nLayer 4 íŒŒë¼ë¯¸í„° ìˆ˜ = 3 * 3 * 192 * 128 * 2 â‡’ 884k\nLayer 5 íŒŒë¼ë¯¸í„° ìˆ˜ = 3 * 3 * 192 * 128 * 2 â‡’ 442k\nDense Layer\r#\rLayer 6 íŒŒë¼ë¯¸í„° ìˆ˜ = 13 * 13 * 128 * 2 * 2048 * 2 â‡’ 117M\nLayer 7 íŒŒë¼ë¯¸í„° ìˆ˜ = 2048 * 2 * 2048 * 2 â‡’ 16M\nLayer 8 íŒŒë¼ë¯¸í„° ìˆ˜ = 2048 * 2 * 1000 â‡’ 4M\nDense Layerì˜ íŒŒë¼ë¯¸í„°ê°€ Conv Layerì˜ íŒŒë¼ë¯¸í„° ìˆ˜ì— ë¹„í•´ ì›”ë“±íˆ ë§ì€ ê²ƒì„ ë³¼ ìˆ˜ ìˆë‹¤.\nì„±ëŠ¥ì„ ì˜¬ë¦¬ê¸° ìœ„í•´ì„  íŒŒë¼ë¯¸í„°ë¥¼ ì¤„ì—¬ì•¼ í•œë‹¤.\në”°ë¼ì„œ, ë„¤íŠ¸ì›Œí¬ê°€ ë°œì „ë¨ì— ë”°ë¼ ë’·ë¶€ë¶„ì˜ Fully Connected Layerì„ ìµœëŒ€í•œ ì¤„ì´ê³ , ì•ì˜ Conv Layerì„ ê¹Šê²Œ ìŒ“ê²Œ ëœë‹¤.\n"},{"id":6,"href":"/posts/2023-06-29-Autoregression/","title":"Autoregression","section":"Blog","content":"íšŒê·€ ë¶„ì„ì˜ ê´€ì ì—ì„œ ê³¼ê±°ì˜ ë°ì´í„°ë¥¼ ë³´ê³  í˜„ì¬ ë˜ëŠ” ë¯¸ë˜ì˜ ê²°ê³¼ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ê²ƒ\nì¦‰, Regressionì„ ìê¸° ìì‹ ì—ê²Œ ì ìš©í•˜ëŠ” ê²ƒ\n$$ y_1, \\ldots, y_n \\rightarrow y_{n+1} $$\n$$ \\mathrm{MSE}=\\frac{1}{n} \\sum_{i=1}^n\\left(f\\left(y_1, \\ldots y_i\\right)-y_{i+1}\\right)^2 $$\nì¢…ë¥˜\r#\rMoving Average(ì´ë™í‰ê· )\r#\rê°€ì¥ ê°„ë‹¨í•œ ë°©ë²•\nìµœì‹  íŠ¸ë Œë“œë¥¼ ë°˜ì˜í•˜ê¸° ìœ„í•´ ìµœê·¼ Kê°œì˜ í‰ê· ì„ í–¥í›„ ì˜ˆì¸¡ì— í™œìš©í•œë‹¤. Kì˜ ê°’ì— ë”°ë¼ ê²½í–¥ì„±ì„ ë‹¤ë¥´ê²Œ ëª¨ë¸ë§í•  ìˆ˜ ìˆë‹¤. Kê°€ ì»¤ì§ˆìˆ˜ë¡ ìµœì‹  íŠ¸ë Œë“œì˜ ë°˜ì˜ ì •ë„ê°€ ì¤„ì–´ë“ ë‹¤. í‰ê·  ë¿ë§Œ ì•„ë‹ˆë¼ ë‹¤ì–‘í•œ í˜•íƒœë¡œ Moving Averageì˜ ëª¨ë¸ë§ì´ ê°€ëŠ¥í•˜ë‹¤.\n$$ f\\left(y_1, \\ldots, y_n\\right)=\\frac{1}{K} \\sum_{k=0}^{K-1} y_{n-k} $$\n$$ f\\left(y_1, \\ldots, y_{n+1}\\right)=\\frac{1}{K}\\left(K \\cdot f\\left(y_1, \\ldots, y_n\\right){-y_{n-K+1}}{+y_{n+1}}\\right) $$\n${-y_{n-K+1}}$: ê°€ì¥ ì˜¤ë˜ëœ ê°’ ì‚­ì œ ${+y_{n+1}}$ : ìµœê·¼ ê°’ ì¶”ê°€ Weighted Moving Average\r#\rë‹¨ìˆœíˆ í‰ê· ì´ ì•„ë‹ˆë¼ ìµœê·¼ ê°’ì— ëŒ€í•œ ê°€ì¤‘ì¹˜ë¥¼ ë” í¬ê²Œ ì£¼ëŠ” ë°©ë²•\nì„ í˜•ì ìœ¼ë¡œ ë¹„ì¤‘ì„ ì¡°ì ˆí•˜ëŠ” ê²ƒ ë¿ë§Œ ì•„ë‹ˆë¼ ì§€ìˆ˜, ë¡œê·¸ ë“±ì„ í™œìš©í•˜ì—¬ ë‹¤ì–‘í•œ ë°©ë²•ìœ¼ë¡œ ëª¨ë¸ë§ì´ ê°€ëŠ¥í•˜ë‹¤.\nì§€ìˆ˜ í•¨ìˆ˜ë¥¼ í™œìš©í•˜ì—¬ ìµœê·¼ ê°’ì˜ ë¹„ì¤‘ì„ ê¸°í•˜ê¸‰ìˆ˜ì ìœ¼ë¡œ ì¦ê°€ì‹œì¼°ë‹¤.\n$$ \\begin{gathered}f\\left(y_1\\right)=y_1 \\f\\left(y_1, \\ldots, y_{n+1}\\right)=\\alpha f\\left(y_1, \\ldots, y_n\\right)+(1-\\alpha) \\cdot y_{n+1}\\end{gathered} $$\nLearning-based Moving Average\r#\rWeighted moving averageì—ì„œì˜ ê°€ì¤‘ì¹˜ë¥¼ í•™ìŠµí•˜ëŠ” ë°©ë²•\n$$ f\\left(y_1, \\ldots, y_n\\right)=\\sum_{k=0}^{K-1} \\theta_k \\cdot y_{n-k} $$\nì£¼ê¸°ì ì¸ ë³€í™”ê°€ ìˆëŠ” êµí†µëŸ‰ ì˜ˆì¸¡, ì‹œì¦Œ ë³„ ìƒí’ˆ ì†Œë¹„ ì˜ˆì¸¡ ë“±ì— ì‚¬ìš©ëœë‹¤.\nì£¼ ë‹¨ìœ„, ì›” ë‹¨ìœ„ ë“±ì˜ ì£¼ê¸°ì„±ì„ ëª¨ë¸ë§í•˜ëŠ” ë° ë³´ë‹¤ ì í•©í•˜ë‹¤.\n"},{"id":7,"href":"/posts/2023-09-25-backpropagation/","title":"Back Propagation(ì˜¤ì°¨ ì—­ì „íŒŒ ì•Œê³ ë¦¬ì¦˜)","section":"Blog","content":"ì´ ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ ì¸í•´Â ML Networkì—ì„œì˜ í•™ìŠµì´ ê°€ëŠ¥í•˜ë‹¤ëŠ” ê²ƒì´ ì•Œë ¤ì ¸, ì•”í‘ê¸°ì— ìˆë˜Â ì‹ ê²½ë§Â í•™ê³„ê°€ ë‹¤ì‹œ ê´€ì‹¬ì„ ë°›ê²Œ ë˜ì—ˆë‹¤.\nì¶œë ¥ì¸µì—ì„œ ì‹œì‘í•˜ì—¬ ì—­ë°©í–¥ìœ¼ë¡œ ì˜¤ë¥˜ë¥¼ ì „íŒŒí•œë‹¤ëŠ” ëœ»ì—ì„œ ì˜¤ë¥˜ ì—­ì „íŒŒë¼ ë¶€ë¥¸ë‹¤.\në‚´ê°€ ë½‘ê³ ì í•˜ëŠ”Â targetê°’ê³¼ ì‹¤ì œ ëª¨ë¸ì´ ê³„ì‚°í•œÂ ì¶œë ¥ì˜ ì°¨ì´ë¥¼ ê³„ì‚°í•œë‹¤. ì˜¤ì°¨ê°’ì„ ë‹¤ì‹œ ë’¤ë¡œ ì „íŒŒí•´ê°€ë©´ì„œ ê° ë…¸ë“œê°€ ê°€ì§€ê³  ìˆëŠ” ë³€ìˆ˜ë“¤ì„ ê°±ì‹ í•œë‹¤. ì§ê´€ì ì¸ ì´í•´ëŠ” ëë‚¬ë‹¤. ì´ì œ ì œëŒ€ë¡œ ì´í•´í•´ë³´ì.\nì˜¤ì°¨ ì—­ì „íŒŒê°€ ì¤‘ìš”í•œ ì´ìœ ë¥¼ ì•Œê³  ì‹¶ë‹¤ë©´, ì—¬ê¸°ë¥¼ í´ë¦­í•˜ì—¬ ì˜¤ì°¨ ì—­ì „íŒŒê°€ ì—†ì„ ì‹œì— ë°œìƒí•˜ëŠ” ë¬¸ì œì ì„ ì´í•´í•˜ì.\nì˜¤ì°¨ ì—­ì „íŒŒ\r#\rì‹ ê²½ë§ì„ í•™ìŠµí•˜ëŠ” ë°©ë²•.\nì—°ì‡„ ë²•ì¹™ì„ í™œìš©í•˜ì—¬ ìˆ˜ì¹˜ ë¯¸ë¶„ì—ì„œì˜ ì—°ì‚°ëŸ‰ì„ ëŒ€í­ ê°ì†Œì‹œí‚¨ë‹¤.\nìˆ˜ì¹˜ ë¯¸ë¶„ê³¼ ë§ˆì°¬ê°€ì§€ë¡œ ì†ì‹¤ í•¨ìˆ˜ ìœ„ì—ì„œì˜ ê°€ì¤‘ì¹˜ì˜ ê¸°ìš¸ê¸°ë¥¼ ì•Œê²Œ í•´ì¤€ë‹¤.\nì¦‰, í•´ë‹¹ ê°€ì¤‘ì¹˜ê°€ ì–¼ë§ˆë‚˜ ì˜¤ì°¨ì— ì˜í–¥ì„ ë¼ì¹˜ëŠ”ì§€ ì•Œê²Œ í•´ì¤€ë‹¤.\n(ê¸°ìš¸ê¸°ì— ëŒ€í•œ ì†ì‹¤ í•¨ìˆ˜ì˜ ë¯¸ë¶„)\níŠ¹ì • ê°€ì¤‘ì¹˜ wì— ëŒ€í•´, ì˜¤ì°¨ L ìœ„ì—ì„œì˜ ê¸°ìš¸ê¸°ëŠ” $\\partial L\\over \\partial w$ì´ë‹¤.\nì—°ì‡„ ë²•ì¹™(chain rule)\r#\rí•©ì„± í•¨ìˆ˜ì˜ ë¯¸ë¶„ì€ í•©ì„± í•¨ìˆ˜ë¥¼ êµ¬ì„±í•˜ëŠ” ê° í•¨ìˆ˜ì˜ ë¯¸ë¶„ì˜ ê³±ìœ¼ë¡œ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆë‹¤.\n$$ {\\partial z\\over \\partial x} = {\\partial z\\over \\partial t}{\\partial t\\over \\partial x} $$\ní•©ì„± í•¨ìˆ˜\nì—¬ëŸ¬ í•¨ìˆ˜ë¡œ êµ¬ì„±ëœ í•¨ìˆ˜\nex) $z = (x + y)^2$ $z = t^2$ $t = x + y$ ìˆœì „íŒŒ, ì—­ì „íŒŒ, êµ­ì†Œì  ê³„ì‚°\r#\rex) $f(x) = y$\nfì— xë¥¼ ì§‘ì–´ë„£ìœ¼ë©´ yê°€ íŠ€ì–´ë‚˜ì˜¨ë‹¤.\nìœ„ì˜ ê·¸ë¦¼ì²˜ëŸ¼ ì–´ë”˜ê°€ì—ì„œ ê°‘ìê¸° ë“±ì¥í•œ $L$ì´ë¼ëŠ” í•¨ìˆ˜ë¥¼ $y$ë¡œ ë¯¸ë¶„í•œ ê°’($\\partial L\\over \\partial y$)ì´ ì œê³µëœë‹¤ë©´?\nìš°ë¦¬ëŠ” $L$ì´ë¼ëŠ” í•¨ìˆ˜ë¥¼ ëª¨ë¥´ì§€ë§Œ, $x$ë¡œ ë¯¸ë¶„í•œ ê°’ì„ ì•Œ ìˆ˜ ìˆë‹¤.\nì—°ì‡„ë²•ì¹™ì„ í™œìš©í•˜ë©´ ëœë‹¤.\n$$ {\\partial L\\over \\partial x} = {\\partial L\\over \\partial y}{\\partial y\\over \\partial x} $$\nìš°ë¦¬ëŠ” $\\partial L\\over \\partial y$ë¥¼ ì•Œê³  ìˆìœ¼ë‹ˆ, $\\partial y\\over \\partial x$ë§Œ ê³„ì‚°í•˜ë©´ ëœë‹¤.\n${\\partial y\\over \\partial x} = f\u0026rsquo;(x)$ì´ë¯€ë¡œ, xì— ëŒ€í•œ í•¨ìˆ˜ì¸ $f(x)$ë¥¼ ë¯¸ë¶„í•˜ë©´ ëœë‹¤.\nex) $f(x) = x^2 â‡’ f\u0026rsquo;(x) = 2x$\nì´ ë•Œ, ìœ„ì˜ ì˜ˆì‹œì—ì„œ ì™¼ìª½ì—ì„œ ì˜¤ë¥¸ìª½ìœ¼ë¡œ ì§„í–‰í•˜ëŠ” ë‹¨ê³„ë¥¼ ìˆœì „íŒŒ(forward propagation),\nì˜¤ë¥¸ìª½ì—ì„œ ì™¼ìª½ìœ¼ë¡œ ì§„í–‰í•˜ëŠ” ë‹¨ê³„ë¥¼ **ì—­ì „íŒŒ(backward propagation)**ì´ë¼ê³  í•œë‹¤.\nêµ­ì†Œì  ê³„ì‚°\nì „ì²´ì—ì„œ ì–´ë–¤ ì¼ì´ ë²Œì–´ì§€ë“  ìƒê´€ì—†ì´ ìì‹ ê³¼ ê´€ê³„ëœ ì •ë³´ë§Œì„ ê²°ê³¼ë¡œ ì¶œë ¥í•  ìˆ˜ ìˆë‹¤.\nìœ„ì˜ ì˜ˆì‹œì²˜ëŸ¼, ê° ë‹¨ê³„ì—ì„œëŠ” ê·¸ì € $f(x)$ì˜ ë¯¸ë¶„ê°’ë§Œ ê³±í•´ì„œ í•˜ë¥˜ë¡œ í˜ëŸ¬ë³´ë‚¸ ê²ƒì´ ì „ë¶€ë‹¤.\në‹¨ìˆœí•œ êµ­ì†Œì  ê³„ì‚°ì´ ì—°ê²°ë˜ì–´ ì „ì²´ë¥¼ êµ¬ì„±í•˜ëŠ” ë³µì¡í•œ ê³„ì‚°ì„ ìˆ˜í–‰í•˜ê²Œ ëœë‹¤.\në§ì…ˆ ë…¸ë“œì—ì„œì˜ ì—­ì „íŒŒ\r#\rex) $z = x + y$\n$\\partial z\\over \\partial x$ = 1 $\\partial z\\over \\partial y$ = 1 ìƒë¥˜ì—ì„œ ë‚´ë ¤ì˜¨ ì‹ í˜¸ì¸ $\\partial L\\over \\partial z$ì— $\\partial z\\over \\partial x$ë¥¼ ê³±í•¨ìœ¼ë¡œì¨ $\\partial L\\over \\partial x$ì„ êµ¬í–ˆë‹¤.\në‹¨ìˆœí•œ ë§ì…ˆ ì—°ì‚°ì´ê¸° ë•Œë¬¸ì— ë¯¸ë¶„ê°’ì€ 1ì´ë‹¤.\nì¦‰, ë§ì…ˆì—ì„œëŠ” ìƒë¥˜ì—ì„œ ë‚´ë ¤ì˜¨ ê°’ì„ ê·¸ëŒ€ë¡œ í•˜ë¥˜ë¡œ ì „ë‹¬í•œë‹¤.\nê³±ì…ˆ ë…¸ë“œì—ì„œì˜ ì—­ì „íŒŒ\r#\rex) $z = xy$\n${\\partial z\\over \\partial x} = y$ ${\\partial z\\over \\partial y} = x$ ì´ë¯€ë¡œ,\nì´ ëœë‹¤.\nì¦‰, ë‹¨ìˆœí•œ ê³±ì…ˆ ë…¸ë“œëŠ” ìƒë¥˜ì—ì„œ ë‚´ë ¤ì˜¨ ì‹ í˜¸ë¥¼ êµì°¨ì‹œì¼œ ê³±í•œ í›„, ë‹¤ì‹œ í•˜ë¥˜ë¡œ ë³´ë‚¸ë‹¤.\ní•˜ì§€ë§Œ ë³¸ì§ˆì€ ë¯¸ë¶„ì´ë‹¤.\n$z = x^2$ì˜ ê²½ìš°, ë¯¸ë¶„ê°’ì€ $2x$ê°€ ë˜ëŠ” ê²ƒì„ ëª…ì‹¬í•˜ì.\nì—°ì‡„ ë²•ì¹™ê³¼ ê³„ì‚° ê·¸ë˜í”„\r#\rì²˜ìŒì— ì˜ˆì‹œë¡œ ë“¤ì—ˆë˜ í•¨ìˆ˜ $z = (x+y)^2$ë¥¼ ê·¸ë˜í”„í™” í•œ ê²ƒì´ë‹¤.\nê²°ë¡ \r#\rì—­ì „íŒŒëŠ” ì˜¤ì°¨(ì†ì‹¤ í•¨ìˆ˜)ë¥¼ ìƒë¥˜ì—ì„œ í•˜ë¥˜ë¡œ ë‚´ë ¤ë³´ë‚´ë©´ì„œ ê° ê°€ì¤‘ì¹˜ê°€ ì†ì‹¤ í•¨ìˆ˜ì—ì„œì˜ ê¸°ìš¸ê¸°ë¥¼ ì•Œ ìˆ˜ ìˆë„ë¡ í•´ì£¼ëŠ” ê¸°ë²•ì´ë‹¤.\nì´ ë•Œ ìˆ˜ë§ì€ ë…¸ë“œì™€ ë³µì¡í•œ í™œì„±í™” í•¨ìˆ˜ ë“±ì´ ê° ë…¸ë“œì—ì„œ êµ­ì†Œì  ê³„ì‚°ì„ í•œë‹¤.\nì´ëŸ° ê°’ë“¤ì´ ëˆ„ì ë˜ê³  í™•ì‚°ë˜ì–´ ë‹¨ í•œë²ˆì˜ ì‹ ê²½ë§ ê³„ì‚°(ì—­ë°©í–¥)ë§Œìœ¼ë¡œë„ ëª¨ë“  ê°€ì¤‘ì¹˜ì™€ í¸í–¥ì„ ê°±ì‹ í•  ìˆ˜ ìˆë‹¤.\nê³±ì…ˆ ë…¸ë“œ, ë§ì…ˆ ë…¸ë“œë¿ë§Œ ì•„ë‹ˆë¼ exp ë…¸ë“œ, log ë…¸ë“œ ë“± ìˆ˜ë§ì€ ë…¸ë“œë“¤ì´ ìˆì§€ë§Œ,\nê²°êµ­Â ê¸°ë³¸ ì›ë¦¬ëŠ” ìƒë¥˜ ë…¸ë“œì— í•´ë‹¹ ë…¸ë“œì—ì„œì˜ ë¯¸ë¶„ ê°’ì„ ì°¾ì•„ì„œ ê³±í•œ ë’¤, í•˜ë¥˜ë¡œ í˜ë ¤ë³´ë‚´ëŠ” ê²ƒì´ ì „ë¶€ì´ë‹¤.\nê·¸ëŸ¬ë¯€ë¡œ ìˆ˜ì¹˜ ë¯¸ë¶„ë³´ë‹¤ëŠ” ë‹¹ì—°íˆ ë¹ ë¥´ë‹¤.\nêµ¬í˜„ ì½”ë“œ\nclass MulLayer: # ê³±ì…ˆ ê³„ì¸µ def __init__(self) -\u0026gt; None: self.x = None self.y = None def forward(self, x, y): # ìˆœì „íŒŒ, xì™€ yì˜ ê°’ì„ ì €ì¥í•´ì•¼ë§Œ backwardë•Œ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤. self.x = x self.y = y out = x * y return out def backward(self, dout): # ì—­ì „íŒŒë¡œ ìƒìœ„ ê³„ì¸µì—ì„œì˜ ë¯¸ë¶„ ê°’ * ë°˜ëŒ€ ë…¸ë“œì˜ ê°’ì„ ì¶œë ¥í•œë‹¤. dx = dout * self.y dy = dout * self.x return dx, dy class AddLayer: # ë§ì…ˆ ê³„ì¸µ def __init__(self) -\u0026gt; None: pass def forward(self, x, y): # ìˆœì „íŒŒ, xì™€ y ê°’ì„ ì €ì¥í•˜ì§€ ì•Šì•„ë„ ëœë‹¤. out = x + y return out def backward(self, dout): dx = dout * 1 dy = dout * 1 return dx, dy "},{"id":8,"href":"/posts/2023-08-16-Book-Rating-Prediction-Wrap-up-Report/","title":"Book Rating Prediction Wrap-up Report","section":"Blog","content":"\r1. í”„ë¡œì íŠ¸ ê°œìš”\r#\rê°œìš”\në³¸ í”„ë¡œì íŠ¸ì—ì„œëŠ” ì†Œë¹„ìë“¤ì˜ ì±… êµ¬ë§¤ ê²°ì •ì— ë„ì›€ì„ ì£¼ê¸° ìœ„í•´ ê°œì¸í™”ëœ ìƒí’ˆì„ ì¶”ì²œí•˜ëŠ” ë¬¸ì œë¥¼ í•´ê²°í•˜ì˜€ìŠµë‹ˆë‹¤.\në³¸ í”„ë¡œì íŠ¸ì—ì„œëŠ” í¬ê²Œ ì„¸ ê°€ì§€ ì‘ì—…ì„ ìˆ˜í–‰í•˜ì˜€ìŠµë‹ˆë‹¤.\nì£¼ì–´ì§„ ë°ì´í„°ì…‹ì„ ë°”íƒ•ìœ¼ë¡œ ë°ì´í„°ì˜ ë‚´ìš©ì„ ë¶„ì„í•˜ê³  ì‹œê°í™”í•˜ëŠ” ê³¼ì •ì„ í†µí•´ ì¸ì‚¬ì´íŠ¸ë¥¼ ë„ì¶œí•˜ì˜€ìŠµë‹ˆë‹¤. ì£¼ì–´ì§„ ëª¨ë¸ ì˜ˆì œë“¤ì„ ë³€í˜•í•˜ê³  ì¡°í•©í•˜ì—¬ ë‹¤ì–‘í•œ ëª¨ë¸ì— ëŒ€í•œ ì„±ëŠ¥ì„ ì‹¤í—˜í•˜ì˜€ìŠµë‹ˆë‹¤. ì•ì„œ ëª…ì‹œí•œ ì‘ì—…ë“¤ì„ í†µí•´ ìµœê³  ì„±ëŠ¥ì„ ë‚´ëŠ” ëª¨ë¸ì„ ë°œêµ´í•˜ì˜€ìŠµë‹ˆë‹¤. ë³¸ í”„ë¡œì íŠ¸ë¥¼ ìˆ˜í–‰í•˜ë©´ì„œ ë¶€ìŠ¤íŠ¸ì½”ìŠ¤ RecSys íŠ¸ë™ì—ì„œ í•™ìŠµí•˜ì˜€ë˜ ë¨¸ì‹ ëŸ¬ë‹/ë”¥ëŸ¬ë‹ ëª¨ë¸ì„ ì´í•´í•˜ê³  ì´ë¥¼ PyTorch êµ¬í˜„í•˜ë©´ì„œ ì‹¤ìƒí™œ ë¬¸ì œì— ì ìš©í•˜ëŠ” ê²½í—˜ì„ í•˜ì˜€ìŠµë‹ˆë‹¤.\ní™œìš© ì¥ë¹„ ë° ì¬ë£Œ(ê°œë°œ í™˜ê²½, í˜‘ì—… tool ë“±)\në¡œì»¬ í™˜ê²½: Windows, Mac ì„œë²„: Linux (Tesla V100) í˜‘ì—… íˆ´: Slack, Notion, Github ì‚¬ìš© ë²„ì „: Python == 3.8.5, Pandas == 2.0.0, Torch == 1.7.1 í”„ë¡œì íŠ¸ êµ¬ì¡° ë° ì‚¬ìš© ë°ì´í„°ì…‹ì˜ êµ¬ì¡°\në³¸ í”„ë¡œì íŠ¸ì—ì„œ í™œìš©ëœ ë°ì´í„°ì…‹ì€ í¬ê²Œ ì„¸ ê°€ì§€ í•˜ìœ„ ë°ì´í„°ì…‹ìœ¼ë¡œ ë‚˜ëˆŒ ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤.\nğŸ“š ì±…ê³¼ ê´€ë ¨ëœ ì •ë³´ë¥¼ ì €ì¥í•œ ë°ì´í„°ì…‹ (books.csv) - 149,570ê±´ì˜ ë°ì´í„° ğŸ‘¤ ì†Œë¹„ìì™€ ê´€ë ¨ëœ ì •ë³´ë¥¼ ì €ì¥í•œ ë°ì´í„°ì…‹ (users.csv) - 68,092ëª…ì˜ ë°ì´í„° ğŸ”¢ ì†Œë¹„ìê°€ ì‹¤ì œë¡œ ë¶€ì—¬í•œ í‰ì ì„ ì €ì¥í•œ ë°ì´í„°ì…‹ (ratings.csv) - 306,795ê±´ì˜ ë°ì´í„° ì‚¬ìš© ë°ì´í„°ì…‹ì˜ êµ¬ì¡°\ntrain_ratingsì˜ rating ë¶„í¬\n2. í”„ë¡œì íŠ¸ íŒ€ êµ¬ì„± ë° ì—­í• \r#\rì´ë¦„ ì—­í•  ê¹€ì§€ì—°_T5062 ë°ì´í„° EDA (ë¶„ì„ ë° ì‹œê°í™”), ë°ì´í„° ì „ì²˜ë¦¬, í•˜ì´í¼ íŒŒë¼ë¯¸í„° íŠœë‹, í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ ìŒì´ë ˆ_T5134 ë°ì´í„° ì „ì²˜ë¦¬, early stop ì ìš©, CNN_FM, DeepCoNNì— dropout ì ìš©, MLP activation function ì„ íƒì§€ ì¶”ê°€ ì˜¤ìŠ¹ë¯¼_T5126 í”„ë¡œì íŠ¸ ê´€ë¦¬, ë°ì´í„° EDA ë° ê²°ì¸¡ì¹˜ ì‚°ì…(Mice), í›„ì²˜ë¦¬, Bagging(Random Forest), Boosting(LGBM Regressor) ëª¨ë¸ íŠœë‹, ë°ì´í„° ì‹œê°í™”, ëª¨ë¸ ì•™ìƒë¸” ì¡°ì¬ì˜¤_T5204 ë°ì´í„° EDA \u0026amp; ì „ì²˜ë¦¬, K-Fold êµ¬í˜„, ë°ì´í„° ë¶„í¬ ë¶„ì„, í›„ì²˜ë¦¬(Post-processing) ìœ¤í•œë‚˜_T5133 ë² ì´ìŠ¤ë¼ì¸ ëª¨ë¸ ì‹¤í—˜, ë°ì´í„° ì „ì²˜ë¦¬, ë°ì´í„° ì‹œê°í™”, CatBoostRegressor , XGBRegressorëª¨ë¸ íŠœë‹, ëª¨ë¸ ì•™ìƒë¸” 3. í”„ë¡œì íŠ¸ ìˆ˜í–‰ ì ˆì°¨ ë° ë°©ë²•\r#\r4. í”„ë¡œì íŠ¸ ìˆ˜í–‰ ê²°ê³¼\r#\rìˆœìœ„ - ìµœì¢… ìˆœìœ„ Public 7ìœ„ (RMSE: 2.1207) / Private 7ìœ„ (RMSE: 2.1159)\níƒìƒ‰ì  ë¶„ì„ ë° ì „ì²˜ë¦¬\níŠ¹ìˆ˜ë¬¸ì ë° ì¤‘ë³µ ì œê±° location ë³€ìˆ˜ì˜ íŠ¹ìˆ˜ ë¬¸ìë¥¼ ì œê±°í•˜ê³ , category ë³€ìˆ˜ì˜ ëŒ€ì†Œë¬¸ìë¥¼ í†µì¼í•˜ì—¬ ì¤‘ë³µì„ ì œê±°í•˜ì˜€ìŠµë‹ˆë‹¤. Label Encoding CatBoostRegressorì˜ ê²½ìš°, ìˆ˜ì¹˜í˜• ë°ì´í„°ì¸ age ë³€ìˆ˜ë¥¼ ë²”ì£¼í™”í•˜ì—¬ ì‚¬ìš©í•˜ì˜€ìŠµë‹ˆë‹¤. ê²°ì¸¡ì¹˜ ì œê±° location_cityê°€ null ê°’ì´ ì•„ë‹ ë•Œ, location_stateì™€ location_countryê°€ ê²°ì¸¡ì¹˜ì¸ ê²½ìš°, cityë¥¼ ê¸°ì¤€ìœ¼ë¡œ ê²°ì¸¡ì¹˜ë¥¼ ì±„ì› ìŠµë‹ˆë‹¤. book_titleì´ nullì´ ì•„ë‹ ë•Œ, ì±… ì œëª©ì´ ê°™ì§€ë§Œ languageì™€ categoryê°€ ê²°ì¸¡ì¹˜ì¸ ê²½ìš°, book_titleì„ ê¸°ì¤€ìœ¼ë¡œ ê²°ì¸¡ì¹˜ë¥¼ ì±„ì› ìŠµë‹ˆë‹¤. Summary ë¶ˆìš©ì–´ë¥¼ ì œê±°í•˜ê³  word2vec ì„ë² ë”©ì„ ìƒì„±í•˜ì˜€ìŠµë‹ˆë‹¤. Feature Engineering ìƒìœ„ ì¹´í…Œê³ ë¦¬ ë³€ìˆ˜ ì¶”ê°€: 5ë²ˆ ì´ìƒ ë“±ì¥í•œ ì¹´í…Œê³ ë¦¬ì˜ ê²½ìš°ë¡œ ì´ë£¨ì–´ì§„ ìƒìœ„ ì¹´í…Œê³ ë¦¬ ë³€ìˆ˜ë¥¼ ì¶”ê°€í•˜ì˜€ìŠµë‹ˆë‹¤. Heavy User / Light User: ì±…ì„ ì½ì€ íšŸìˆ˜ê°€ 5ê¶Œ ì´ìƒì¸ ìœ ì €ë“¤ì„ Heavy User, ë‚˜ë¨¸ì§€ ìœ ì €ë“¤ì„ Light Userë¡œ êµ¬ë¶„í•˜ëŠ” ë³€ìˆ˜ë¥¼ ì¶”ê°€í•˜ì˜€ìŠµë‹ˆë‹¤. ì±… ì¶œíŒ íšŸìˆ˜: ê°™ì€ ì±… ì œëª©ì˜ isbnì˜ ê°œìˆ˜(ì¶œíŒ íšŸìˆ˜) ë³€ìˆ˜ë¥¼ ì¶”ê°€í•˜ì˜€ìŠµë‹ˆë‹¤. í•„ë“œ ìƒëµ Feature importance ë¶„ì„ì—ì„œ city/state fieldê°€ country fieldì— ë¹„í•´ ë†’ì€ ì¤‘ìš”ë„ë¥¼ ê°€ì§€ê³  ìˆë‹¤ëŠ” ê²°ê³¼ê°€ ë‚˜ì˜´ì— ë”°ë¼, ë„ì„œ ì„ íƒì— ë„ì‹œ/ì£¼ë³´ë‹¤ êµ­ê°€ê°€ ë” ë†’ì€ ì—°ê´€ì„±ì„ ê°€ì§ˆ ê±°ë¼ëŠ” ì§ê´€ê³¼ ì¶©ëŒí•˜ì—¬ ì˜ë¬¸ì„ ê°€ì¡ŒìŠµë‹ˆë‹¤. ì´í›„ city/state fieldë¥¼ ëˆ„ë½í•˜ê³  location ì •ë³´ë¥¼ countryë§Œ ì‚¬ìš©í•œ ê²°ê³¼ ì„±ëŠ¥ í–¥ìƒì„ ì–»ì—ˆìŠµë‹ˆë‹¤. ëª¨ë¸ ê°œìš”\nì‹¤í—˜ì— ì‚¬ìš©í•œ ëª¨ë¸ë“¤ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.\nCARs(ML)\nFM FFM CARs(DL)\nNCF DCN WDN CNN_FM - ì´ë¯¸ì§€ ë°ì´í„° ì¶”ê°€ í™œìš© DeepCoNN - Summary ë°ì´í„° ì¶”ê°€ í™œìš© Bagging\nRandomForest Boosting\nLGBM XGBoost CatBoost 3. ëª¨ë¸ ì„ ì • ë° ë¶„ì„\nìµœì¢… ëª¨ë¸ ì„ ì • ê¸°ì¤€ / ì•™ìƒë¸” ë“±\nFM\nContext-Aware Recommendation System ê³„ì—´ ML Model\nGeneral Predictor + Latent Factor Model\nRMSE : 2.354\nì„±ëŠ¥ì´ ê°€ì¥ ì˜ ë‚˜ì˜¨ CatBoostRegressorì™€ ì˜ˆì¸¡ ë¶„í¬ê°€ ê°€ì¥ ë‹¤ë¥¸ ëª¨ë¸ì„ ì•™ìƒë¸” í•´ì•¼ ê·¹ë‹¨ì ì¸ ê°’ë“¤ì´ ì¤‘í™”ë˜ì–´ robustí•œ ê²°ê³¼ê°€ ë‚˜ì˜¬ ê²ƒì´ë¼ê³  ê°€ì •í•˜ì˜€ê³ , CatBoostRegressorì™€ ê°€ì¥ ê²°ê³¼ ì˜ˆì¸¡ ë¶„í¬ê°€ ìƒì´í•œ ê²°ê³¼ë¥¼ ë³´ì˜€ë˜ FMì„ ì•™ìƒë¸”í•˜ì—¬ RMSE 2.1299 â†’ 2.1222ë¡œ í° ì„±ëŠ¥ í–¥ìƒ íš¨ê³¼ë¥¼ ë³´ì˜€ìŠµë‹ˆë‹¤.\nDeepCoNN\ní…ìŠ¤íŠ¸ ë°ì´í„° ì²˜ë¦¬ë¥¼ í•˜ëŠ” ë”¥ëŸ¬ë‹ ëª¨ë¸ë¡œ, ë‹¨ì¼ ëª¨ë¸ì˜ ì„±ëŠ¥ì€ RMSE 2.2228ì´ì—ˆìŠµë‹ˆë‹¤. summaryëŠ” ì±…ì˜ ë‚´ìš©ì„ ë‚˜íƒ€ë‚´ëŠ” featureì´ê¸° ë•Œë¬¸ì— ì±…ì˜ í‰ì ì„ ê²°ì •í•˜ëŠ” ë° ìœ ì˜ë¯¸í•  ê²ƒì´ë¼ê³  ê°€ì„¤ì„ ì„¸ìš°ê³  ì‹¤í—˜í•˜ì˜€ê³ , ìµœì¢…ì ìœ¼ë¡œ Deep_CoNNì„ ì•™ìƒë¸” ëª¨ë¸ì— í¬í•¨í•˜ì˜€ìŠµë‹ˆë‹¤. CatBoostRegressor\në”¥ëŸ¬ë‹ ê¸°ë°˜ì˜ ëª¨ë¸ì´ ì•„ë‹Œ Decision Tree ê¸°ë°˜ì˜ ëª¨ë¸ë¡œ, ê²°ì¸¡ì¹˜ê°€ ì¡´ì¬í•˜ëŠ” ë°ì´í„°ì—ì„œ ìƒëŒ€ì ìœ¼ë¡œ ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì˜€ìŠµë‹ˆë‹¤. ì£¼ì–´ì§„ ë°ì´í„°ì…‹ì—ì„œ ageì™€ year_of_publication ë³€ìˆ˜ë¥¼ ì œì™¸í•œ ëª¨ë“  ë³€ìˆ˜ë“¤ì´ ë²”ì£¼í˜• ë°ì´í„°ë¡œ êµ¬ì„±ë˜ì–´ ìˆì–´, Boosting ê¸°ë°˜ì˜ íšŒê·€ëª¨ë¸ì´ë©° ë²”ì£¼í˜• ë°ì´í„°ë¥¼ ì²˜ë¦¬í•˜ëŠ” ë° ê°•í•œ, CatBoostRegressorë¥¼ ì‚¬ìš©í•˜ê²Œ ë˜ì—ˆìŠµë‹ˆë‹¤. user embeddingê³¼ book embeddingì„ ì¶”ê°€í•˜ê³ , train ë°ì´í„°ì˜ rating ë¶„í¬ê°€ imbalanceí•˜ê¸° ë•Œë¬¸ì— ratingì„ ë¹„ìœ¨ëŒ€ë¡œ ë‚˜ëˆ„ì–´ trainê³¼ valid setì„ ë¶„ë¦¬í•˜ëŠ” Stratified 10 K-Fold ë°©ì‹ì„ ì‚¬ìš©í•˜ì—¬ RMSE 2.1997 â†’ 2.1306ìœ¼ë¡œ ì„±ëŠ¥ í–¥ìƒì„ í™•ì¸í•˜ì˜€ìŠµë‹ˆë‹¤. Optunaë¥¼ ì‚¬ìš©í•´ í•˜ì´í¼ íŒŒë¼ë¯¸í„°ë¥¼ íŠœë‹í•˜ì—¬ RMSE 2.1306 â†’ 2.1299ë¡œ ì„±ëŠ¥ í–¥ìƒì„ í™•ì¸í•˜ì˜€ìŠµë‹ˆë‹¤. ì•™ìƒë¸” (simple-weighted ensembleì‚¬ìš©)\nTrainì˜ ë¶„í¬ì™€ ê²°ê³¼ ë¹„êµí•˜ë©° weighted sumì„ ì ìš©í•˜ì˜€ìŠµë‹ˆë‹¤.\nCatBoostRegressor, DeepCoNN, FM ëª¨ë¸ì˜ ê²°ê³¼ ë¶„í¬\nìµœì¢…ì ìœ¼ë¡œ, CatBoostRegressor +Deep_CoNN + FM ì„ 7:1:2ì˜ ë¹„ìœ¨ë¡œ ê°€ì¤‘ì¹˜ë¥¼ ì£¼ì–´ ì•™ìƒë¸”í•¨ â†’ RMSE 2.1159(private ê¸°ì¤€)ìœ¼ë¡œ 7ë“±ì„ ë‹¬ì„±í•˜ì˜€ìŠµë‹ˆë‹¤.\nìµœì¢… Architecture - CatBoostRegressor + DeepCoNN + FM\nëª¨ë¸ í‰ê°€ ë° ê°œì„ , í›„ì²˜ë¦¬\nStratified K-fold\nBaseline ë° ì—¬ëŸ¬ Boosting ê³„ì—´ ëª¨ë¸ì— Stratified K-fold CVë¥¼ ì ìš©í•˜ì—¬, ì•½ê°„ì˜ RMSE ê°œì„ ê³¼ ëª¨ë¸ì˜ ì¼ë°˜í™” ì„±ëŠ¥ì„ í–¥ìƒì‹œì¼°ìŠµë‹ˆë‹¤.\nData Visualization\nì œì¶œ íšŸìˆ˜ê°€ ë¶€ì¡±í•œ ìƒí™©ì—ì„œ ëª¨ë¸ë³„ ì„±ëŠ¥ì„ íŒŒì•…í•˜ê¸° ìœ„í•´ ëª¨ë¸ì˜ ê²°ê³¼ë¬¼ì¸ Submission.csv íŒŒì¼ì˜ Rating ë¶„í¬ë¥¼ ê³ ë ¤í–ˆìŠµë‹ˆë‹¤.\nRMSEê°€ ë‚´ë¦¬ëŠ” ìƒí™©ì— ë°ì´í„°ì˜ ë¶„í¬ë¥¼ íŒŒì•…í•œ í›„, ë°ì´í„°ì˜ ë¶„í¬ë¥¼ ê³ ë ¤í•˜ì—¬ ê°œì„  ë°©í–¥ì„ íš¨ê³¼ì ìœ¼ë¡œ íŒë‹¨í•  ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤.\nPost Processing\ní‰ì  ì˜ˆì¸¡ ê²°ê³¼ê°€ 1~10ì˜ ë²”ìœ„ë¥¼ ì´ˆê³¼í•˜ëŠ” ê°’ë“¤ì„ ê°ê° 1, 10ìœ¼ë¡œ ì¡°ì •í•´ì£¼ì–´ ì„±ëŠ¥ì„ í–¥ìƒì‹œì¼°ìŠµë‹ˆë‹¤.\nì‹œì—° ê²°ê³¼\ní”„ë¡œì íŠ¸ ì§„í–‰ íë¦„ì— ë”°ë¥¸ ì„±ëŠ¥ ì¶”ì´\n5. ìì²´ í‰ê°€ ì˜ê²¬\r#\rë°°ìš´ ì  ë°ì´í„°ì…‹ EDA ë° ì „ì²˜ë¦¬ë¥¼ í†µí•´ ë°ì´í„°ë¥¼ ì •ì œí•˜ëŠ” ê²½í—˜ì„ ìŒ“ì„ ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤. ëª¨ë¸ê³¼ ê·¸ ì¡°í•©ì„ ì‹¤í—˜í•˜ë©° ì„±ëŠ¥ì˜ ì¦ê° ê²°ê³¼ì— ë”°ë¥¸ ì´ìœ ë¥¼ ì¶”ë¡ í•˜ëŠ” ê³¼ì •ì—ì„œ ë¶€ìŠ¤íŠ¸ì½”ìŠ¤ í•™ìŠµ ë‚´ìš©ì— ëŒ€í•´ ë” ê¹Šì´ ì´í•´í•  ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤. ì•„ì‰¬ìš´ ì  í•˜ë£¨ 10íšŒì˜ ë¦¬ë”ë³´ë“œ ì œì¶œ ê¶Œí•œì„ ì¶©ë¶„íˆ í™œìš©í•˜ì§€ ëª»í•œ ì ì´ ì•„ì‰¬ì›€ìœ¼ë¡œ ë‚¨ìŠµë‹ˆë‹¤. ë‹¨ì¼ ëª¨ë¸ì— ì§‘ì¤‘í•˜ë‹¤ê°€ Ensemble ì‹¤í—˜ì„ ì¶©ë¶„íˆ í•˜ì§€ ëª»í•œ ì ì´ ì•„ì‰½ìŠµë‹ˆë‹¤. ì‹œë„í•  ì  WandBë¥¼ ì ê·¹ì ìœ¼ë¡œ í™œìš©í•˜ì—¬ íŒŒë¼ë¯¸í„°ë¥¼ ì¡°ìœ¨í•´ë³´ê³  ì‹¶ìŠµë‹ˆë‹¤. Githubë¥¼ í†µí•œ í˜‘ì—…ì„ ë³´ë‹¤ ìœ ì—°í•˜ê²Œ ì§„í–‰í•˜ê³  ì‹¶ìŠµë‹ˆë‹¤. Weighted ensemble ì™¸ ë‹¤ì–‘í•œ ensembleì„ ì‹œë„í•˜ê³  ì‹¶ìŠµë‹ˆë‹¤. ê°œì¸ ë¦¬í¬íŠ¸\r#\rTeam\r#\rNotion ê¸°ë¡ì„ í†µí•´ ì„œë¡œ ìˆ˜í–‰í•˜ëŠ” ì‘ì—…ì— ëŒ€í•œ ê³µìœ ê°€ ì˜ ëë‹¤. ë°œìƒí•œ ë¬¸ì œì ë“¤ì„ ë¹ ë¥´ê²Œ ê³µìœ í•˜ê³  ì²˜ë¦¬í–ˆë‹¤. ì—­í•  ë¶„ë‹´ì€ ì—†ì—ˆìœ¼ë‚˜ ì„œë¡œ ë°°ë ¤í•˜ëŠ” ë§ˆìŒìœ¼ë¡œ ëŒ€íšŒê°€ ì˜ ì§„í–‰ëë‹¤. ëª¨ë¸ êµ¬í˜„ ì¼ì •ì„ ì œëŒ€ë¡œ í™•ì • ì§“ì§€ ì•Šì•„ ì‘ì—…ì´ ëŠ¦ì–´ì ¸ ë§ˆì§€ë§‰ì— ì œì¶œ íšŸìˆ˜ê°€ ë§ì´ ëª¨ìëë‹¤. Personal\r#\rProject Management â€” íŒ€ì›ë“¤ì—ê²Œ Git í™œìš©ë²•ì„ ì „íŒŒí–ˆë‹¤.\nEDA ë° ì•™ìƒë¸” ëª¨ë¸ ì„ íƒ â€” FM\nëŒ€íšŒ ë°ì´í„°ì…‹ì´ ì •í˜•ë°ì´í„°ë¼ëŠ” ì ì„ ê³ ë ¤í–ˆì„ ë•Œ, MLê³„ì—´ì„ ì•™ìƒë¸”ì— í™œìš©í•´ì•¼ ì „ì²´ ì„±ëŠ¥ì´ ì˜ ë‚˜ì˜¬ ê²ƒì´ë¼ ì§ì‘í–ˆë‹¤.\nì´ë¥¼ ìœ„í•´ ëŒ€íšŒë¥¼ ì‹œì‘í•˜ìë§ˆì FM ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ê°œì„ í•˜ëŠ” ê²ƒì— ì§‘ì¤‘í–ˆìœ¼ë©°, ê²°ê³¼ì ìœ¼ë¡œ ìµœì¢… ì œì¶œë¬¼ì—ë„ ê°œì„ í•œ FMëª¨ë¸ì´ í¬í•¨ë  ìˆ˜ ìˆì—ˆë‹¤.\ní™œìš©í•œ ëª¨ë¸ â€” FM, FFM, DeepCoNN, WDN, DCN, CNN_FM\nHyper Parameter Tuning â€” Batch_size, LR, Layer ê°œìˆ˜, Dropout ë“± ì¡°ì ˆ\në°ì´í„° ì „ì²˜ë¦¬ â€” Mice\nBoosting ì ìš© â€” Light GBM Regressor\nê²°ì¸¡ì¹˜ì— ì•½í•œ LGBM ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ë†’ì´ê¸° ìœ„í•´ MI ëª¨ë“ˆ MICEë¥¼ ì ‘ëª©í–ˆë‹¤.\nBagging ì ìš© â€” Random Forest Regressor\nData Visualization\nëŒ€íšŒ ë§ˆì§€ë§‰ë‚  ì œì¶œ íšŸìˆ˜ê°€ ë¶€ì¡±í•œ ìƒí™©ì—ì„œ ëª¨ë¸ë³„ ì„±ëŠ¥ì„ íŒŒì•…í•˜ê¸° ìœ„í•´ ëª¨ë¸ì˜ ê²°ê³¼ë¬¼ì¸ Submission.csv íŒŒì¼ì˜ Rating ë¶„í¬ë¥¼ ê³ ë ¤í–ˆë‹¤.\nRMSEê°€ ë‚´ë¦¬ëŠ” ìƒí™©ì— ë°ì´í„°ì˜ ë¶„í¬ë¥¼ íŒŒì•…í•œ í›„, ë°ì´í„°ì˜ ë¶„í¬ë¥¼ ê³ ë ¤í•˜ì—¬ ê°œì„  ë°©í–¥ì„ íš¨ê³¼ì ìœ¼ë¡œ íŒë‹¨í•  ìˆ˜ ìˆì—ˆë‹¤.\nEnsemble â€” ëª¨ë¸ë“¤ì˜ ì„±ëŠ¥ê³¼ ë°ì´í„°ì˜ ë¶„í¬ë¥¼ ê³ ë ¤í•˜ë©° sw ì•™ìƒë¸”ì„ ìˆ˜í–‰í–ˆë‹¤.\ní•œê³„ ë° êµí›ˆ\r#\rëª¨ë¸ ì„±ëŠ¥ì— ëŒ€í•´ ë‹¨í¸ì ì¸ ì •ë³´ë§Œ í™œìš©í•œ ê²ƒ\në‹¤ì–‘í•œ ëª¨ë¸ í‰ê°€ Metricì„ í™œìš©í•´ì•¼ê² ë‹¤.\në°°ì›€ë³´ë‹¤ëŠ”, ëŒ€íšŒ ì„±ì ì— ë” ì§‘ì¤‘í•œ ê²ƒ\nHyper Parameter Tuningë³´ë‹¤ëŠ”, ëª¨ë¸ ìì²´ì— ì§‘ì¤‘í•˜ì—¬, ì½”ë“œë¥¼ ìˆ˜ì •í•˜ê³  êµ¬ì¡°ë¥¼ ë³€ê²½í•´ë³´ì.\nBaselineì˜ êµ¬ì¡°ë¥¼ ìœ ì§€í•˜ê³ ì ë…¸ë ¥í•œ ê²ƒ\nBaselineì˜ í˜•íƒœë¥¼ ê³ ì§‘í•  í•„ìš”ëŠ” ì „í˜€ ì—†ë‹¤.\nì˜¤íˆë ¤ Baselineì˜ íë¦„ì„ ë²—ì–´ë˜ì§„ í›„ Baselineì„ ì™„ë²½í•˜ê²Œ ì´í•´í•  ìˆ˜ ìˆì—ˆë‹¤.\nì‹œê°„ ë¶„ë°°ë¥¼ ì œëŒ€ë¡œ í•˜ì§€ ëª»í•œ ê²ƒ\nLGBMì˜ ì„±ëŠ¥ê°œì„ ì„ ë¯¸ë£¨ë‹¤ê°€ ë§ˆì§€ë§‰ ë‚ ì˜ ì œì¶œ ìˆ˜ê°€ ë¶€ì¡±í•˜ì—¬ ì„±ëŠ¥ì„ ì œëŒ€ë¡œ íŒŒì•…í•˜ì§€ ëª»í–ˆë‹¤.\nì¼ì˜ ìš°ì„ ìˆœìœ„ë¥¼ ì œëŒ€ë¡œ ì„¤ì •í•œ í›„, ìš°ì„ ìˆœìœ„ì— ë§ê²Œ ì¼ì„ ì²˜ë¦¬í•˜ì.\në‹¤ìŒ í”„ë¡œì íŠ¸ì—ì„œ ì‹œë„í•´ë³´ê³  ì‹¶ì€ ì \r#\rHybrid Modelì„ êµ¬í˜„í•´ë³´ê¸°. Valid Loss ë¿ë§Œ ì•„ë‹ˆë¼ ë” ë‹¤ì–‘í•œ Metricìœ¼ë¡œ ëª¨ë¸ì„ í‰ê°€í•˜ê¸°. ë” ë‚˜ì€ ë°©ë²•ìœ¼ë¡œ Hyper Parameter Tuningì„ ìˆ˜í–‰í•˜ê¸°. Baselineì— ì–½ë§¤ì´ì§€ ì•Šê³ , ë°”ë¡œ ì§ì ‘ ëª¨ë¸ êµ¬í˜„í•˜ê¸°. "},{"id":9,"href":"/posts/2023-05-22-CBOW/","title":"CBOW: Continous Bag of Word","section":"Blog","content":" Word2Vecì„ í•™ìŠµí•˜ëŠ” ë°©ë²• ì¤‘ í•˜ë‚˜.\nì•ë’¤ì˜ ë‹¨ì–´ë¥¼ í†µí•´ ì¤‘ì•™ì˜ ë‹¨ì–´ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ë°©ë²•\ninput â€” quick, brown, jumps, over output â€” fox ë‹¨ì–´ë¥¼ ì˜ˆì¸¡í•˜ê¸° ìœ„í•´ ì•ë’¤ë¡œ ëª‡ ê°œì˜ ë‹¨ì–´(n)ë¥¼ ì‚¬ìš©í• ì§€ ì •í•œë‹¤.\nMulti-Class Classification\nInputì„ í†µí•´ One-Hot Vectorì˜ ê° ì›ì†Œê°€ 0ì¸ì§€ 1ì¸ì§€ ì˜ˆì¸¡í•œë‹¤.\ní•™ìŠµ íŒŒë¼ë¯¸í„°\r#\r$W_{V\\times M}$: One-Hot Vectorì„ ì„ë² ë”© ë²¡í„°ë¡œ ë³€í™˜í•˜ëŠ” í–‰ë ¬\n$V$: ë‹¨ì–´ì˜ ì´ ê°œìˆ˜(One-Hot Vectorì˜ í¬ê¸°) $M$: ì„ë² ë”© ë²¡í„°ì˜ í¬ê¸° $W\u0026rsquo;_{M\\times V}$: ì„ë² ë”© ë²¡í„°ë¥¼ One-Hot Vectorì˜ ê¸¸ì´ë¡œ ë³€í™˜í•˜ëŠ” í–‰ë ¬\ní•™ìŠµê³¼ì •\r#\rInput Layer\r#\rì£¼ë³€ì˜ ë‹¨ì–´ë¥¼ One-Hot Vectorë¡œ ì…ë ¥ë°›ëŠ”ë‹¤. ì…ë ¥ë°›ì€ One-Hot Vectorë¥¼ ì„ë² ë”© ë²¡í„°ë¡œ ë³€í™˜í•œë‹¤.($W_{V\\times M}$) Projection Layer\r#\rë³€í™˜ëœ ì„ë² ë”© ë²¡í„°ë“¤ì„ í‰ê· ë‚´ì–´ ì„ë² ë”© ë²¡í„° $v$ë¥¼ êµ¬í•œë‹¤. â†’ Word2Vec ì„ë² ë”© ë²¡í„° $v$ë¥¼ One-Hot Vectorì™€ ë™ì¼í•œ í¬ê¸°ë¡œ ë³€í™˜í•œë‹¤.($W\u0026rsquo;_{M\\times V}$)\në³€í™˜ëœ ë²¡í„° $z$ë¥¼ Output Layerë¡œ ë³´ë‚¸ë‹¤. Output Layer\r#\rSoftmax í•¨ìˆ˜ë¥¼ í†µí•´ ë²¡í„° $z$ë¥¼ í™•ë¥  ë²¡í„°ë¡œ ë³€í™˜í•œë‹¤.\nì¶œë ¥($\\hat y$)ì„ í‰ê°€í•˜ê¸° ìœ„í•´ ì¤‘ì•™ ë‹¨ì–´ì˜ One-Hot Vector($y$)ì™€ CEë¥¼ ê³„ì‚°í•œë‹¤.\n"},{"id":10,"href":"/posts/2023-05-24-DeepFM/","title":"DeepFM","section":"Blog","content":"DeepFM: A Factorization-Machine based Neural Network for CTR Prediction\nWide \u0026amp; Deep ëª¨ë¸ê³¼ ë‹¬ë¦¬ ë‘ ìš”ì†Œ(wide, deep)ê°€ ì…ë ¥ê°’ì„ ê³µìœ í•˜ë„ë¡ í•œ end-to-end ë°©ì‹ì˜ ë…¼ë¬¸\nBackground\r#\rì¶”ì²œ ì‹œìŠ¤í…œì—ì„œëŠ” implicit feature interactionì„ í•™ìŠµí•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•˜ë‹¤.\nì˜ˆì‹œ) ì‹ì‚¬ ì‹œê°„ì— ë°°ë‹¬ì•± ë‹¤ìš´ë¡œë“œ ìˆ˜ ì¦ê°€ (order-2 interaction)\n10ëŒ€ ë‚¨ì„±ì€ ìŠˆíŒ…/RPGê²Œì„ì„ ì„ í˜¸ (order-3 interaction)\nê¸°ì¡´ ëª¨ë¸ë“¤ì€ low-ë‚˜ high-order interaction ì¤‘ ì–´ëŠ í•œ ìª½ì—ë§Œ ê°•í•˜ë‹¤.\nWide \u0026amp; Deep ëª¨ë¸ì€ ì´ ë‘˜ì„ í†µí•©í•˜ì—¬ ë¬¸ì œ í•´ê²°\ní•˜ì§€ë§Œ wide componentì— feature engineering(=Cross-Product Transformation)ì´ í•„ìš”í•˜ë‹¤.\nì´ëŸ¬í•œ ë¬¸ì œë¥¼ í•´ê²°í•˜ê³ ì DeepFMì—ì„œëŠ” FMì„ wide componentë¡œ ì‚¬ìš©í•˜ì—¬ ì…ë ¥ê°’ì„ ê³µìœ í•œë‹¤.\nDeepFM = Factorization Machine + Deep Neural Network\nëª¨ë¸ êµ¬ì¡°\r#\rFM for low-order feature interaction\r#\rê¸°ì¡´ì˜ FMëª¨ë¸ê³¼ ì™„ì „íˆ ë™ì¼í•œ êµ¬ì¡°\nìˆ˜ì‹ì´ ë™ì¼í•˜ë‹¤.\nFM êµ¬ì¡°\n$$ \\hat{y}(\\mathrm{x})=w_0+\\sum_{i=1}^n w_i x_i{+\\sum_{i=1}^n \\sum_{j=i+1}^n\\left\\langle\\mathrm{v}_i, \\mathrm{v}_j\\right\\rangle x_i x_j} \\\nw_0 \\in \\mathbb{R}, \\quad w_i \\in \\mathbb{R}, \\quad \\mathrm{v}_i \\in \\mathbb{R}^k $$\norder-2 feature interactionì„ íš¨ê³¼ì ìœ¼ë¡œ ì¡ëŠ”ë‹¤.\nê° fieldê°€ í•˜ë‚˜ì˜ featureë¥¼ ì˜ë¯¸í•œë‹¤.\nëª¨ë‘ Sparseí•œ featureë¡œ êµ¬ì„±í•œë‹¤.\nAdditionìœ¼ë¡œ ì—°ê²°ëœ ì„ \n1ì°¨ Termì„ ì˜ë¯¸\nê°ê°ì˜ Featureì€ ë™ì¼í•œ ì°¨ì›ìœ¼ë¡œ ì„ë² ë”©ëœ í›„, ë‚´ì ì„ í†µí•´ featureê°„ interactionì„ í•™ìŠµí•œë‹¤.\nDNN for high-order feature interaction\r#\rëª¨ë“  featureë“¤ì€ ë™ì¼í•œ ì°¨ì›(k)ì˜ ì„ë² ë”©ìœ¼ë¡œ ì¹˜í™˜ëœë‹¤.\nì´ ë•Œ, ì„ë² ë”©ì— ì‚¬ìš©ë˜ëŠ” ê°€ì¤‘ì¹˜ëŠ” FM Componentì˜ ê°€ì¤‘ì¹˜($v_{ij}$)ì™€ ë™ì¼í•˜ë‹¤.\n$$ \\begin{aligned}\u0026amp; a^0=\\left[e_1, e_2, \\ldots, e_m\\right] \\\u0026amp; a^{(l+1)}=\\sigma\\left(W^l a^l+b^l\\right) \\\u0026amp; y_{D N N}=W^{|H|+1} a^{|H|}+b^{|H|+1}\\end{aligned} $$\nê° Embeddingì€ ëª¨ë‘ ì—°ê²°ë˜ì–´ ê°€ë¡œë¡œ ë¶™ê²Œ ëœë‹¤.\nì´ë ‡ê²Œ íƒ„ìƒí•œ ì„ë² ë”© ë²¡í„°ê°€ MLP Layerì˜ Input Layerê°€ ëœë‹¤.\nì´í›„, Lê°œì˜ Feed-Forward Networkë¥¼ ì§€ë‚˜ë©° ë§ˆì§€ë§‰ì— í´ë¦­ ì—¬ë¶€ë¥¼ Outputìœ¼ë¡œ ì œì¶œí•œë‹¤.\nì „ì²´ êµ¬ì¡°\r#\r$$ \\tt \\hat y = sigmoid(y_{FM} + y_{DNN}) $$\nFMê³¼ Deepì˜ ì¥ì ì„ ëª¨ë‘ ê°€ì§„ë‹¤.\níƒ€ ëª¨ë¸ê³¼ì˜ ë¹„êµ\r#\rFNN\r#\rFM ëª¨ë¸ì„ ì‚¬ìš©í•˜ì§€ë§Œ, End-to-End í•™ìŠµì´ ì•„ë‹ˆë‹¤.\nFMëª¨ë¸ì„ í™œìš©í•œ ì´í›„, ê·¸ ì„ë² ë”©ì„ ë‹¤ì‹œ ê°€ì§€ê³  ì™€ì„œ í™œìš©í•œë‹¤.\nì¦‰, Pre-trainingì´ í•„ìš”í•˜ë‹¤.\nPNN\r#\rDeepFMê³¼ í¡ì‚¬í•˜ì§€ë§Œ, Low-order Interaction(Memorizationë¶€ë¶„)ì´ ë¹ ì ¸ìˆë‹¤.\nì„±ëŠ¥\r#\r"},{"id":11,"href":"/posts/2023-05-25-FFM-Field-aware-Factorization-Machine/","title":"FFM: Field-aware Factorization Machine","section":"Blog","content":"\rê°œìš”\r#\rField-aware Factorization Machines for CTR Prediction\nFMì˜ ë³€í˜•ëœ ëª¨ë¸ì¸ FFMì„ ì œì•ˆí•˜ì—¬ ë” ë†’ì€ ì„±ëŠ¥ì„ ë³´ì¸ ë…¼ë¬¸ FMì€ ì˜ˆì¸¡ ë¬¸ì œì— ë‘ë£¨ ì ìš© ê°€ëŠ¥í•œ ëª¨ë¸ë¡œ, íŠ¹íˆ sparse ë°ì´í„°ë¡œ êµ¬ì„±ëœ CTR ì˜ˆì¸¡ì—ì„œ ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì¸ë‹¤.\nField-aware Factorization Machine (FFM)\r#\rFMì„ ë°œì „ì‹œí‚¨ ëª¨ë¸\nPITF ëª¨ë¸ì—ì„œ ì•„ì´ë””ì–´ë¥¼ ì–»ì—ˆë‹¤.\nPITF : Pairwise Interaction Tensor Factorization\r#\rMFë¥¼ 3ì°¨ì›ìœ¼ë¡œ í™•ì¥ì‹œí‚¨ ëª¨ë¸\nPITFì—ì„œëŠ” (user, item, tag) 3ê°œì˜ í•„ë“œì— ëŒ€í•œ í´ë¦­ë¥ ì„ ì˜ˆì¸¡í•˜ê¸° ìœ„í•´\n(user, item), (item, tag), (user, tag) ê°ê°ì— ëŒ€í•´ì„œ ì„œë¡œ ë‹¤ë¥¸ latent factorë¥¼ ì •ì˜í•˜ì—¬ ê³„ì‚°\nâ‡’ ì´ë¥¼ ì¼ë°˜í™”í•˜ì—¬ ì—¬ëŸ¬ ê°œì˜ í•„ë“œì— ëŒ€í•´ì„œ latent factorë¥¼ ì •ì˜í•œ ê²ƒì´ FFM\nFFMì˜ íŠ¹ì§•\r#\rì…ë ¥ ë³€ìˆ˜ë¥¼ í•„ë“œ(field)ë¡œ ë‚˜ëˆ„ì–´, í•„ë“œë³„ë¡œ ì„œë¡œ ë‹¤ë¥¸ latent factorë¥¼ ê°€ì§€ë„ë¡ factorizeí•œë‹¤.\nê¸°ì¡´ì˜ FMì€ í•˜ë‚˜ì˜ ë³€ìˆ˜ì— ëŒ€í•´ì„œ kê°œë¡œ factorizeí–ˆìœ¼ë‚˜ FFMì€ fê°œì˜ í•„ë“œì— ëŒ€í•´ ê°ê° kê°œë¡œ factorizeí•œë‹¤.\nField\nê°™ì€ ì˜ë¯¸ë¥¼ ê°–ëŠ” ë³€ìˆ˜ë“¤ì˜ ì§‘í•©ìœ¼ë¡œ ì„¤ì •\nìœ ì €: ì„±ë³„, ë””ë°”ì´ìŠ¤, ìš´ì˜ì²´ì œ\nì•„ì´í…œ: ê´‘ê³ , ì¹´í…Œê³ ë¦¬\nì»¨í…ìŠ¤íŠ¸: ì–´í”Œë¦¬ì¼€ì´ì…˜, ë°°ë„ˆ\nëª¨ë¸ì„ ì„¤ê³„í•  ë•Œ í•¨ê»˜ ì •ì˜\nCTR ì˜ˆì¸¡ì— ì‚¬ìš©ë˜ëŠ” í”¼ì³ëŠ” ì´ë³´ë‹¤ í›¨ì”¬ ë‹¤ì–‘í•˜ë‹¤.\në³´í†µ í”¼ì³ì˜ ê°œìˆ˜ë§Œí¼ í•„ë“œë¥¼ ì •ì˜í•˜ì—¬ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤.\nFFM ê³µì‹\r#\r$$ \\begin{gathered}\\hat{y}(\\mathrm{x})=w_0+\\sum_{i=1}^n w_i x_i+\\sum_{i=1}^n \\sum_{j=i+1}^n\\left\\langle\\mathrm{v}{i, f_j}, \\mathrm{v}{j, f_i}\\right\\rangle x_i x_j \\w_0 \\in \\mathbb{R}, \\quad w_i \\in \\mathbb{R}, \\quad \\mathrm{v}_{i, f} \\in \\mathbb{R}^k\\end{gathered}\n$$\nì°¸ê³  â€” FM Formula\r$$ \\begin{gathered}\\hat{y}(\\mathrm{x})=w_0+\\sum_{i=1}^n w_i x_i+\\sum_{i=1}^n \\sum_{j=i+1}^n\\left\\langle\\mathrm{v}_i, \\mathrm{v}_j\\right\\rangle x_i x_j \\w_0 \\in \\mathbb{R}, \\quad w_i \\in \\mathbb{R}, \\quad \\mathrm{v}_i \\in \\mathbb{R}^k\\end{gathered} $$\nFMì€ kì°¨ì›ì˜ íŒŒë¼ë¯¸í„°ë¥¼ $v_i$ì™€ $v_j$ê°€ ë‚´ì ì´ ëœ í˜•íƒœë¡œ ìƒí˜¸ì‘ìš©ì„ í‘œí˜„í•˜ëŠ” ë°˜ë©´,\nFFMì€ $x_i$ì— ëŒ€ì‘ë˜ëŠ” íŒŒë¼ë¯¸í„°ê°€ $v_i$ê°€ ì•„ë‹ˆë¼, $v_{i,f_j}$ê°€ ëœë‹¤.\nì¦‰, field $(f_i)$ë³„ë¡œ Factorization íŒŒë¼ë¯¸í„°ê°€ ì •ì˜ëœë‹¤.\nex) ê´‘ê³  í´ë¦­ ë°ì´í„°ê°€ ì¡´ì¬í•˜ê³  ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” featureê°€ ì´ ì„¸ ê°œ(Publisher, Advertiser, Gender)ì¼ ë•Œ,\nCliked Publisher (P) Advertiser (A) Gender (G) Yes ESPN Nike Male FM\ní•„ë“œê°€ ì¡´ì¬í•˜ì§€ ì•ŠëŠ”ë‹¤.\ní•˜ë‚˜ì˜ ë³€ìˆ˜ì— ëŒ€í•´ factorization ì°¨ì›$(k)$ ë§Œí¼ì˜ íŒŒë¼ë¯¸í„°ë¥¼ í•™ìŠµí•œë‹¤.\n$$ \\hat{y}(\\mathrm{x}) =w_0+w_{\\mathrm{ESPN}}+w_{\\mathrm{Nike}}+w_{\\text {Male }}+{\\mathrm{v}{\\mathrm{ESPN}} \\cdot \\mathrm{v}{\\mathrm{Nike}}\\ +\\mathrm{v}{\\mathrm{ESPN}} \\cdot \\mathrm{v}{\\text {Male }}+\\mathrm{v}{\\mathrm{Nike}} \\cdot \\mathrm{v}{\\text {Male }}} $$\nFFM\nê°ê°ì˜ featureë¥¼ í•„ë“œ P,A,Gë¡œ ì •ì˜\ní•˜ë‚˜ì˜ ë³€ìˆ˜ì— ëŒ€í•´ í•„ë“œ ê°œìˆ˜$(f)$ì™€ factorization ì°¨ì›$(k)$ì˜ ê³± $(=fk)$ë§Œí¼ì˜ íŒŒë¼ë¯¸í„°ë¥¼ í•™ìŠµí•œë‹¤.\n$$ \\hat{y}(\\mathrm{x}) =w_0+w_{\\mathrm{ESPN}}+w_{\\mathrm{Nike}}+w_{\\text {Male }}+{\\mathrm{v}{\\mathrm{ESPN}, \\mathrm{A}} \\cdot \\mathrm{v}{\\mathrm{Nike}, \\mathrm{P}}\\\n+\\mathrm{v}{\\mathrm{ESPN}, \\mathrm{G}} \\cdot \\mathrm{v}{\\text {Male,P }}+\\mathrm{v}{\\mathrm{Nike}, \\mathrm{G}} \\cdot \\mathrm{v}{\\mathrm{Male}, \\mathrm{A}}} $$\nFFMì˜ í•„ë“œ êµ¬ì„±\r#\rCategorical Feature\r#\rFM\nFFM\nNumerical Feature\r#\rì‹¤ìˆ˜ë„ ë°˜ë“œì‹œ íŠ¹ì • í•„ë“œì— ì†í•´ì•¼ í•œë‹¤.\ndummy field\nnumeric feature í•œ ê°œë‹¹ í•˜ë‚˜ì˜ í•„ë“œì— í• ë‹¹í•˜ê³  ì‹¤ìˆ˜ ê°’ì„ ì‚¬ìš©\nfieldê°€ í¬ê²Œ ì˜ë¯¸ë¥¼ ê°–ì§€ ì•ŠëŠ”ë‹¤.\ndiscretize\nnumeric featureë¥¼ nê°œì˜ êµ¬ê°„ìœ¼ë¡œ ë‚˜ëˆ„ì–´ ì´ì§„ ê°’ì„ ì‚¬ìš©í•˜ê³ , nê°œì˜ ë³€ìˆ˜ë¥¼ í•˜ë‚˜ì˜ í•„ë“œì— í• ë‹¹í•œë‹¤.\nFM / FFM ì„±ëŠ¥ ë¹„êµ\r#\rLRì— ë¹„í•´ FM, FFMì˜ ì„±ëŠ¥ì´ ë” ë‚«ë‹¤.\ní•„ë“œë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ì ì ˆí•˜ì§€ ì•Šì€ ë°ì´í„°ì…‹ì˜ ê²½ìš° FFMë³´ë‹¤ FMì´ ì„±ëŠ¥ì´ ë” ì˜ë‚˜ì˜¨ë‹¤.\n"},{"id":12,"href":"/posts/2023-05-21-FM-Factorization-Machine/","title":"FM: Factorization Machine","section":"Blog","content":" General Predictorì— Latent Factor Modelì„ ì¶”ê°€í•œ ëª¨ë¸.\nBackground\r#\rFactorization Machines\nSVMê³¼ Factorization Modelì˜ ì¥ì ì„ ê²°í•©í•œ FMì„ ì²˜ìŒ ì†Œê°œí•œ ë…¼ë¬¸\në“±ì¥ ë°°ê²½\në”¥ëŸ¬ë‹ì´ ë“±ì¥í•˜ê¸° ì´ì „ SVMì´ ê°€ì¥ ë§ì´ ì‚¬ìš©ëë‹¤.\në§¤ìš° í¬ì†Œí•œ ë°ì´í„°ê°€ ë§ì€ CF í™˜ê²½ì—ì„œëŠ” SVMë³´ë‹¤ MF ê³„ì—´ì˜ ëª¨ë¸ì´ ë” ë†’ì€ ì„±ëŠ¥ì„ ë‚´ì™”ë‹¤.\nSVMê³¼ MFì˜ ì¥ì ì„ ê²°í•©í•  ìˆ˜ ì—†ì„ê¹Œ? â‡’ FM íƒ„ìƒ.\nMF ê¸°ë°˜ ëª¨ë¸ì˜ í•œê³„ â‡’ User-Item í–‰ë ¬ ê¸°ë°˜\nì¦‰, íŠ¹ì • ë°ì´í„° í¬ë§·ì— íŠ¹í™”ë˜ì–´ ìˆë‹¤.\n$X:$ (ìœ ì €, ì•„ì´í…œ) â†’ $Y:$ (rating)ìœ¼ë¡œ ì´ë£¨ì–´ì§„ ë°ì´í„°ì— ëŒ€í•´ì„œë§Œ ì ìš©ì´ ê°€ëŠ¥í•˜ë‹¤.\nì¼ë°˜ì ì¸ ë°ì´í„°ì…‹ì— ë°”ë¡œ ì ìš© ë¶ˆê°€ëŠ¥ User-Item í–‰ë ¬ ì™¸ì˜ ì •ë³´ë¥¼ í™œìš©í•˜ê¸° ì–´ë µë‹¤. User-Item í–‰ë ¬ì„ ë²”ìš©ì ì¸ í˜•íƒœë¡œ ë³€ê²½í–ˆì„ ë•Œì˜ ë¬¸ì œì \nML ëª¨ë¸ì— ì‚¬ìš©ë˜ëŠ” ë°ì´í„° í˜•íƒœë¥¼ ë§Œë“¤ë©´ high sparsity ë¬¸ì œê°€ ë°œìƒí•œë‹¤. ì›í™œí•œ íŒŒë¼ë¯¸í„° í•™ìŠµì´ ì–´ë µë‹¤. Featureê°„ ìƒí˜¸ì‘ìš©ì„ ë°˜ì˜í•˜ê¸° ì–´ë µë‹¤. FM ê³µì‹\r#\r$$ \\hat{y}(\\mathrm{x})=w_0+\\sum_{i=1}^n w_i x_i\\blue{+\\sum_{i=1}^n \\sum_{j=i+1}^n\\left\\langle\\mathrm{v}_i, \\mathrm{v}_j\\right\\rangle x_i x_j} \\\nw_0 \\in \\mathbb{R}, \\quad w_i \\in \\mathbb{R}, \\quad \\mathrm{v}_i \\in \\mathbb{R}^k $$\n$\\langle \\cdot,\\cdot \\rangle:$ ë‘ ë²¡í„°ì˜ ìŠ¤ì¹¼ë¼ê³±(dot product)\n$$ \\left\\langle\\mathrm{v}i, \\mathrm{v}j\\right\\rangle:=\\sum{f=1}^k \\mathrm{v}{i, f} \\cdot \\mathrm{v}_{j, f} $$\nLogistic Regressionì— ë‘ Featureì˜ ìƒí˜¸ì‘ìš©ì„ ë‚˜íƒ€ë‚´ëŠ” Termì´ ì¶”ê°€ëœ í˜•íƒœ\nLogistic Regression\n$$ \\hat{y}(\\mathrm{x})=w_0+\\sum_{i=1}^n w_i x_i\\\nw_0 \\in \\mathbb{R}, \\quad w_i \\in \\mathbb{R} $$\nPolynomial Modelê³¼ ìƒí˜¸ì‘ìš©ì„ ëª¨ë¸ë§í•˜ëŠ” Termì´ ë‹¤ë¥´ë‹¤.\nPolynomial Regression\n$$ \\hat y(x)=\\left(w_0+\\sum_{i=1}^n w_i x_i{+\\sum_{i=1}^n \\sum_{j=i+1}^n w_{i j} x_i x_j}\\right), \\quad w_i, w_{i j} \\in \\mathbb{R} $$\n$x_ix_j$ì˜ ìƒí˜¸ì‘ìš©ì„ í•˜ë‚˜ì˜ íŒŒë¼ë¯¸í„°$(w_{ij})$ë¡œ ë‚˜íƒ€ë‚¸ Polynomialì— ë¹„í•´, $\\left\\langle\\mathrm{v}_i, \\mathrm{v}_j\\right\\rangle$ì˜ kì°¨ì›ì˜ Factorization íŒŒë¼ë¯¸í„°ë¡œ ë‚˜íƒ€ë‚´ ë”ìš± ì¼ë°˜í™”ì‹œì¼°ë‹¤.\nFMì˜ í™œìš©\r#\rSparseí•œ ë°ì´í„°ì…‹ì—ì„œ ì˜ˆì¸¡í•˜ê¸°\nìœ ì €ì˜ ì˜í™”ì— ëŒ€í•œ í‰ì  ë°ì´í„°ëŠ” ëŒ€í‘œì ì¸ High Sparsity ë°ì´í„°\nìœ ì € - ì•„ì´í…œ ë§¤íŠ¸ë¦­ìŠ¤ì—ì„œ ë‹¤ë£¨ë˜ Sparse Matrixì™€ëŠ” ë‹¤ë¥¸ ì˜ë¯¸\ní‰ì  ë°ì´í„° = { (ìœ ì €1, ì˜í™”2, 5), (ìœ ì €3, ì˜í™”1, 4), (ìœ ì €2, ì˜í™”3, 1), â€¦ }\nì¼ë°˜ì ì¸ CF ë¬¸ì œì˜ ì…ë ¥ ë°ì´í„°ì™€ ê°™ìŒ\nìœ„ì˜ í‰ì  ë°ì´í„°ë¥¼ ì¼ë°˜ì ì¸ ì…ë ¥ ë°ì´í„°ë¡œ ë°”ê¾¸ë©´, ì…ë ¥ ê°’ì˜ ì°¨ì›ì´ ì „ì²´ ìœ ì €ì™€ ì•„ì´í…œ ìˆ˜ë§Œí¼ ì¦ê°€\nex) ìœ ì € ìˆ˜ê°€ $U$ëª…, ì˜í™”ì˜ ìˆ˜ê°€ $M$ê°œì¼ ë•Œ\nSparseí•œ Featureë“¤ì˜ ìƒí˜¸ì‘ìš©ì´ í•™ìŠµë˜ëŠ” ë°©ë²•\nìœ ì € Aì˜ STì— ëŒ€í•œ í‰ì  ì˜ˆì¸¡ â†’ $V_A, V_{ST}$ê°€ FM ëª¨ë¸ì„ í†µí•´ í•™ìŠµë˜ê¸° ë•Œë¬¸ì— ìƒí˜¸ì‘ìš©ì´ ë°˜ì˜ëœë‹¤.\n$V_{ST}$ â€” ìœ ì € B,Cì˜ ì˜í™” STì— ëŒ€í•œ í‰ì  ë°ì´í„°ë¥¼ í†µí•´ í•™ìŠµëœë‹¤.\nìœ ì € B,CëŠ” ì˜í™” ST ì™¸ì— ë‹¤ë¥¸ ì˜í™”ë„ í‰ê°€í•œë‹¤.\n$V_A$ â€” ìœ ì € B,Cê°€ ìœ ì € Aì™€ ê³µìœ í•˜ëŠ” ì˜í™” SWì˜ í‰ì  ë°ì´í„°ë¥¼ í†µí•´ í•™ìŠµí•œë‹¤.\nFMì˜ ì¥ì \nvs. SVM\në§¤ìš° sparseí•œ ë°ì´í„°ì— ëŒ€í•´ì„œ ë†’ì€ ì˜ˆì¸¡ ì„±ëŠ¥ì„ ë³´ì¸ë‹¤.\nì„ í˜• ë³µì¡ë„$(O(kn))$ë¥¼ ê°€ì§€ë¯€ë¡œ ìˆ˜ì‹­ ì–µ ê°œì˜ í•™ìŠµ ë°ì´í„°ì— ëŒ€í•´ì„œë„ ë¹ ë¥´ê²Œ í•™ìŠµí•œë‹¤.\nëª¨ë¸ì˜ í•™ìŠµì— í•„ìš”í•œ íŒŒë¼ë¯¸í„°ì˜ ê°œìˆ˜ë„ ì„ í˜•ì ìœ¼ë¡œ ë¹„ë¡€í•œë‹¤.\nvs. Matrix Factorization\nì—¬ëŸ¬ ì˜ˆì¸¡ ë¬¸ì œ(íšŒê·€/ë¶„ë¥˜/ë­í‚¹)ì— ëª¨ë‘ í™œìš© ê°€ëŠ¥í•œ ë²”ìš©ì ì¸ ì§€ë„ í•™ìŠµ ëª¨ë¸\nì¼ë°˜ì ì¸ ì‹¤ìˆ˜ ë³€ìˆ˜(real-value feature)ë¥¼ ëª¨ë¸ì˜ ì…ë ¥(input)ìœ¼ë¡œ ì‚¬ìš©í•œë‹¤.\nMFì™€ ë¹„êµí–ˆì„ ë•Œ ìœ ì €, ì•„ì´í…œ ID ì™¸ì— ë‹¤ë¥¸ ë¶€ê°€ ì •ë³´ë“¤ì„ ëª¨ë¸ì˜ í”¼ì³ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤.\nFMì˜ ì‹œê°„ ë³µì¡ë„\r#\r$$ \\begin{aligned}\u0026amp; \\tt\\sum_{i=1}^n \\sum_{j=i+1}^n\\left\\langle\\mathbf{v}i, \\mathbf{v}j\\right\\rangle x_i x_j \\quad\\quad \\quad\\quad\\quad\\quad\\quad\\quad{\\Longrightarrow O(kn^2)} \\= \u0026amp; \\frac{1}{2} \\sum{i=1}^n \\sum{j=1}^n\\left\\langle\\mathbf{v}i, \\mathbf{v}j\\right\\rangle x_i x_j-\\frac{1}{2} \\sum{i=1}^n\\left\\langle\\mathbf{v}i, \\mathbf{v}i\\right\\rangle x_i x_i \\= \u0026amp; \\tt\\frac{1}{2}\\left(\\sum{i=1}^n \\sum{j=1}^n \\sum{f=1}^k v_{i, f} v_{j, f} x_i x_j-\\sum_{i=1}^n \\sum_{f=1}^k v_{i, f} v_{i, f} x_i x_i\\right) \\= \u0026amp; \\tt\\frac{1}{2} \\sum_{f=1}^k\\left(\\left(\\sum_{i=1}^n v_{i, f} x_i\\right)\\left(\\sum_{j=1}^n v_{j, f} x_j\\right)-\\sum_{i=1}^n v_{i, f}^2 x_i^2\\right) \\= \u0026amp; \\tt\\frac{1}{2} \\sum_{f=1}^k\\left(\\left(\\sum_{i=1}^n v_{i, f} x_i\\right)^2-\\sum_{i=1}^n v_{i, f}^2 x_i^2\\right)\\quad\\quad{\\Longrightarrow O(kn)}\\end{aligned} $$\nì–´ë–»ê²Œ ìˆ˜ì‹ì„ ì •ë¦¬í•˜ì—¬ ì‹œê°„ë³µì¡ë„ë¥¼ ì¤„ì¼ ìˆ˜ ìˆì—ˆë‚˜?\n2ì¤‘ ë°˜ë³µë¬¸ì„ (1ì¤‘ ë°˜ë³µë¬¸)^2ì˜ í˜•íƒœë¡œ ì¹˜í™˜í•˜ì—¬ ê³„ì‚°ì„ ì¤„ì˜€ë‹¤.\n"},{"id":13,"href":"/posts/2023-06-02-Git-Branch/","title":"Git BranchëŠ” ë¬´ì—‡ì´ê³ , ë¬´ì—‡ì„ ìœ„í•´ ì‚¬ìš©í•˜ëŠ”ê°€?","section":"Blog","content":"ë…ë¦½ì ìœ¼ë¡œ íŠ¹ì • ì‘ì—…ì„ ì§„í–‰í•  ë•Œ ì‚¬ìš©í•œë‹¤.\níŒ€ìœ¼ë¡œ ì—¬ëŸ¬ì‘ì—…ì„ ë™ì‹œì— ì‘ì—…í•  ìˆ˜ ìˆë‹¤.\nì—¬ëŸ¬ Branchë¥¼ í•©ì¹˜ëŠ” ë°©ë²• â€” Merge\nmerge\r#\rbranchì˜ ëª¨ë“  ê¸°ë¡ ë³´ì¡´, mergeì— ëŒ€í•œ ê¸°ë¡ ì¶”ê°€.\nì´ ë°©ë²•ì„ ì‚¬ìš©í•˜ë©´ mergeì— ëŒ€í•œ commitì´ í•˜ë‚˜ ìƒì„±ë˜ê³  ì–´ëŠ ì‹œì ì— mergeë¥¼ ì§„í–‰í–ˆëŠ”ì§€ ì‰½ê²Œ ì•Œ ìˆ˜ ìˆë‹¤. branchê°€ ëŠ˜ì–´ë‚˜ê³  ì—¬ëŸ¬ ë²ˆì˜ mergeê°€ ìƒê¸°ê²Œ ë˜ë©´, ê·¸ë˜í”„ê°€ ë³µì¡í•´ì ¸Â **ì»¤ë°‹ íˆìŠ¤í† ë¦¬(Commit History)**ë¥¼ íŒŒì•…í•˜ê¸° ë”ìš± ì–´ë ¤ì›Œì§ˆ ìˆ˜ ìˆë‹¤. squash and merge\r#\rì—¬ëŸ¬ commit ê¸°ë¡ í•˜ë‚˜ë¡œ í•©ì¹˜ê¸°, mergeê¸°ë¡ ë‚¨ê¸°ì§€ ì•Šê¸°.\nì§€ì €ë¶„í•œ ì»¤ë°‹ ì´ë ¥ë“¤ì„ ì‚­ì œí•˜ë©´ì„œ master branchë¡œ í•©ì¹  ë•Œ ì‚¬ìš©í•œë‹¤.\nmergeí•  ë•Œ ì—¬ëŸ¬ commitë“¤ì„ í•˜ë‚˜ë¡œ í•©ì¹œë‹¤. Squashë¥¼ í•˜ê²Œ ë˜ë©´ ëª¨ë“  ì»¤ë°‹ ì´ë ¥ì´ í•˜ë‚˜ì˜ ì»¤ë°‹ìœ¼ë¡œ í•©ì³ì§€ë©° ì‚¬ë¼ì§„ë‹¤. mergeì— ëŒ€í•œ ì´ë ¥ì€ ë‚¨ê¸°ì§€ ì•ŠëŠ”ë‹¤. rebase and merge\r#\rì—¬ëŸ¬ commit ê¸°ë¡ ë‚¨ê¸°ê¸°, merge ê¸°ë¡ ë‚¨ê¸°ì§€ ì•Šê¸°.\nê¸°ì¡´ì˜ ì»¤ë°‹ ì´ë ¥ì„ ìœ ì§€í•˜ë©´ì„œë„, ê¹”ë”í•˜ê²Œ í•˜ë‚˜ì˜ íë¦„ìœ¼ë¡œ ê´€ë¦¬í•˜ê³ ì í•  ë•Œ ì‚¬ìš©í•œë‹¤.\nì¼ë°˜ì ìœ¼ë¡œ ê°€ì¥ ë§ì´ ì‚¬ìš©ëœë‹¤.\nmergeì— ëŒ€í•œ ì´ë ¥ì´ ë‚¨ì§€ ì•ŠëŠ”ë‹¤.\nì¦‰, ì–¸ì œ mergeê°€ ëëŠ”ì§€ ì•Œ ìˆ˜ ì—†ë‹¤.\në§Œì•½ merge ì‹œì ì— ëŒ€í•œ ê¸°ë¡ì´ í•„ìš”í•˜ë‹¤ë©´ ì»¤ë°‹ìœ¼ë¡œ ì§ì ‘ ëª…ì‹œí•´ì£¼ë©´ ëœë‹¤.\nëª¨ë“  ì»¤ë°‹ì´ë ¥ë“¤ì´ rebase ë˜ì–´ì„œ ì¶”ê°€ëœë‹¤. ì¦‰, fast-forward ëœë‹¤.\nRebaseë¥¼ í•˜ë©´ ì»¤ë°‹ë“¤ì˜ Baseê°€ ë³€ê²½ë˜ë¯€ë¡œ Commit Hash ë˜í•œ ë³€ê²½ ë  ìˆ˜ ìˆë‹¤.\nì´ë¡œ ì¸í•´ Force Pushê°€ í•„ìš”í•œ ê²½ìš°ê°€ ë°œìƒí•œë‹¤.\në‚´ê°€ branchë¥¼ ë¶„ê¸°í•œ ì´í›„, ê¸°ì¡´ì˜ branchê°€ ìˆ˜ì •ëœ ê²½ìš° ë‚´ branchì— ê¸°ì¡´ branchë¥¼ pullí•´ì„œ conflict í•´ê²°í•˜ê¸°.\në‚´ê°€ ì‘ì„±í•œ ì»¤ë°‹ì„ ì ˆëŒ€ ë³€ê²½í•˜ê³  ì‹¶ì§€ ì•Šì€ ê²½ìš°\në‚´ê°€ ì‘ì„±í•œ ì»¤ë°‹ ê¸°ë¡ì€ ìœ ì§€í•œì±„, Mergeë¼ëŠ” ê¸°ëŠ¥ì„ í†µí•´ í•©ì³ì¡Œë‹¤ë¼ëŠ” ê¸°ë¡ì„ ê°™ì´ ë‚¨ê¸°ê²Œ ëœë‹¤. mergeí•  ë•Œ rebase and mergeí•˜ê¸°. â† ë” ë‚˜ì€ ë°©ë²•. Conflictê°€ ë°œìƒí•œë‹¤ë©´?\r#\rë§ˆì§€ë§‰ìœ¼ë¡œ push í•˜ëŠ” ì‚¬ëŒì´ í•´ê²°í•´ì•¼ í•œë‹¤.\në§ˆì§€ë§‰ì— pushí•˜ëŠ” ì‚¬ëŒì€ conflict ì—ëŸ¬ê°€ ë°œìƒí•œë‹¤.\ngit statusë¡œ ì¶©ëŒ ìœ„ì¹˜ í™•ì¸í•˜ê¸°\nAì™€ Bê°€ í˜‘ì˜ í›„ ìˆ˜ì •í•˜ê¸°\nCVNì˜ í•´ê²°ë°©ë²•ê³¼ ìœ ì‚¬í•˜ë‹¤.\nì›ê²© ë¸Œëœì¹˜ ê°€ì ¸ì˜¤ê¸°\r#\rgit remote update ì›ê²© ì €ì¥ì†Œ ê°±ì‹ \nê°€ë” ê°±ì‹ ì„ í•˜ì§€ ì•Šìœ¼ë©´ pushí•˜ê³ ì í•˜ëŠ” ì›ê²© branchë¥¼ ì°¾ì„ ìˆ˜ ì—†ë‹¤ëŠ” ì˜¤ë¥˜ë¥¼ ë³´ê²Œëœë‹¤.\ní•´ë‹¹ ì˜¤ë¥˜ë¥¼ ì²˜ìŒ ë§Œë‚˜ë©´ ë¶„ëª… branchìˆëŠ”ë° ì™œ ì—†ì–´! í•˜ê²Œ ëœë‹¤.\ngit branch -r ì›ê²© ì €ì¥ì†Œ branch ë¦¬ìŠ¤íŠ¸ í™•ì¸\ngit branch -d dev ë¡œì»¬ ì €ì¥ì†Œ branch ì œê±°í•˜ê¸°\ngit checkout -t origin/dev ì›ê²© ì €ì¥ì†Œ branch ê°€ì ¸ì˜¤ê¸°\n-t: ì›ê²© ì €ì¥ì†Œì˜ branch ì´ë¦„ê³¼ ë™ì¼í•œ ë¡œì»¬ branchë¥¼ ìƒì„±\n-f: ë¬¸ì œê°€ ë°œìƒí•˜ë”ë¼ë„ ê°•ì œë¡œ ì§„í–‰í•œë‹¤.\nbranch í•©ì¹˜ê¸°, ë³‘í•©í•˜ê¸°\r#\rgit checkout -b issue1 branch ìƒì„±í•˜ê³  ë³€ê²½í•˜ê¸°\nìœ„ ëª…ë ¹ì–´ëŠ” ì•„ë˜ì˜ ë‘ ê°€ì§€ ëª…ë ¹ì–´ë¥¼ í•œ ë²ˆì— ìˆ˜í–‰í•œ ê²ƒì´ë‹¤. git branch issue1 main main branchë¡œë¶€í„° issue1 branch ìƒì„±í•˜ê¸° git checkout issue1 main branchì—ì„œ issue1 branchë¡œ ë³€ê²½í•˜ê¸° â€” ì½”ë“œ ì¶”ê°€ ì‘ì—… ìˆ˜í–‰ â€”\ngit checkout master (ì‘ì—…ì´ ì™„ë£Œë˜ê³  ë‚œ í›„) masterÂ ë¸Œëœì¹˜ë¡œ ì´ë™í•˜ê¸°\ngit merge issue1 issue1ì„Â masterì— merge (ë³‘í•©)í•˜ê¸°\nBranch ì „ëµ â€” Flows\r#\rê°€ì¥ ìµœì‹  ì»¤ë°‹ì´ ì–´ë””ì— ì¡´ì¬í•˜ëŠëƒì— ë”°ë¼ ë‘ ê°€ì§€ ë°©ë²•ìœ¼ë¡œ ë‚˜ë‰œë‹¤.\nLocal ì¤‘ì‹¬ â€” Git Flow Remote ì¤‘ì‹¬ â€” Github Flow "},{"id":14,"href":"/posts/2023-06-24-KPT-%EC%9D%BC%EC%9D%BC%ED%9A%8C%EA%B3%A0%EB%A1%9D-230326/","title":"KPT20230326","section":"Blog","content":"ë¶€ìŠ¤íŠ¸ìº í¼ë“¤ì˜ í•™ìŠµì •ë¦¬ ê¸°ë¡ë“¤ì„ ì½ì—ˆë‹¤.\në•ë¶„ì— ë§ì€ ë™ê¸°ë¶€ì—¬ê°€ ë˜ì–´, ë‚˜ë„ íšŒê³ ë¥¼ í•˜ê³ ì ë§ˆìŒë¨¹ì—ˆë‹¤.\nëŠ¦ì—ˆì§€ë§Œ, ì§€ê¸ˆ ë‹¹ì¥ ì‹œì‘í•´ì•¼ì§€.\nTIL (Today I Learn)\r#\rë…¸ì…˜ì— íŠ¹ì • ì‹œê°„ì— ìë™ìœ¼ë¡œ í˜ì´ì§€ ìƒì„± ê¸°ëŠ¥ì´ ìˆë‹¤ëŠ” ê²ƒì„ ê¹¨ë‹¬ì•˜ë‹¤. Keep\r#\ríšŒê³ ë¡ì„ ì“°ê¸° ì‹œì‘í•œ ê²ƒ! ì—‰ë§ì§„ì°½ì´ë˜ ì—¬ëŸ¬ í´ë” ìë£Œ ì •ë¦¬ë¥¼ í•œ ê²ƒ. ë¶€ìŠ¤íŠ¸ìº í”„ ì‹œì‘í•˜ê³  ì²˜ìŒìœ¼ë¡œâ€¦ê³„íšì„ ì„¸ìš´ ê²ƒ!(ì´ëŸ° ë‚´ê°€â€¦MBTI J..?) ì•¼ì‹ì„ ì°¸ì€ ê²ƒ. ë°°ê°€ ê³ í”„ì§€ë§Œ ì‚´ì´ ì˜ ë¹ ì§€ê³  ìˆë‹¤. Problem\r#\rë‚´ì¼ì€â€¦ë°˜ë“œì‹œ ì¼ì° ìë„ë¡ í•˜ì. Try\r#\rë¯¸ë¦¬ë¯¸ë¦¬ í•˜ë£¨ë¥¼ ë¶€ì§€ëŸ°í•˜ê²Œ ì‚´ê³ , í•˜ë£¨ì˜ ëª©í‘œì¹˜ëŠ” ì ë‹¹í•œ ì–‘ìœ¼ë¡œ ì¡ì. "},{"id":15,"href":"/posts/2023-06-24-KPT-%EC%9D%BC%EC%9D%BC%ED%9A%8C%EA%B3%A0%EB%A1%9D-230327/","title":"KPT20230327","section":"Blog","content":"\rTIL (Today I Learn)\r#\rStackingì´ ë¬´ì—‡ì´ê³ , ì–´ë–¤ ìƒí™©ì— ì‚¬ìš©í•˜ëŠ”ì§€ ì¶”ì²œ ì‹œìŠ¤í…œì´ ë¬´ì—‡ì¸ì§€ì™€, ì¶”ì²œì‹œìŠ¤í…œì„ í†µí•´ í•´ê²°í•˜ê³ ì í•˜ëŠ” ë¬¸ì œì˜ ë³¸ì§ˆ(The Long tail í˜„ìƒ) ì¶”ì²œì‹œìŠ¤í…œì˜ ëª©ì  ì¶”ì²œì‹œìŠ¤í…œì˜ í‰ê°€ ì§€í‘œ AP, MAP CG, DCG, IDCG, NDCG ìŠ¤ì½”ì–´ ìƒì„± ë°©ë²• UMAP í™œìš© ë°©ë²• Keep\r#\rê°•ì˜ë¥¼ í†µí•´ ì¶”ì²œ ì‹œìŠ¤í…œì˜ ì „ë°˜ì ì¸ í‹€ì„ ì•Œê²Œ ë˜ì—ˆë‹¤. ì—¬ëŸ¬ ì¶”ì²œ ì‹œìŠ¤í…œ ì˜ˆì‹œë“¤ì„ ì ‘í•˜ë‹¤ë³´ë‹ˆ, ê³¼ê±°ì— ë‚´ê°€ ìˆ˜í–‰í–ˆì—ˆë˜ í”„ë¡œì íŠ¸ê°€ ê²°êµ­ ì¶”ì²œì‹œìŠ¤í…œì´ë¼ëŠ” ê²ƒì„ ê¹¨ë‹¬ì•˜ë‹¤. ì—°êµ¬ì‹¤ ê³¼ì œë¥¼ ìˆ˜í–‰í•˜ë©° ì°¨ì› ì¶•ì†Œë¥¼ ì§„í–‰í•´ì•¼ í–ˆëŠ”ë°, ì¼ì „ì— 1ë¶„ ë§í•˜ê¸°ë•Œ ì°¨ì›ì¶•ì†Œì— ëŒ€í•´ ì •ë¦¬í–ˆë˜ ê²ƒì´ ë„ì›€ì´ ëë‹¤. Problem\r#\rì „ë‚  ì ì„ ì ê²Œ ìì„œ ë‚®ì ì„ 2ì‹œê°„ì •ë„ ìë²„ë ¸ë‹¤. ë‚®ì ì„ ì” í›„, ì´ë¦¬ì €ë¦¬ ì¼ì •ì´ ìˆì–´ì„œ ì‹¤ì§ˆì ì¸ ê³µë¶€ëŸ‰ì´ ë„ˆë¬´ ë¶€ì¡±í–ˆë‹¤. ì°¨ì› ì¶•ì†Œë¥¼ ì§„í–‰í•  ë•Œ, Metricì„ ì„¤ì •í•´ì£¼ì–´ì•¼ í–ˆëŠ”ë°, ë‹¤ì–‘í•œ Metricì„ ë°°ì› ìŒì—ë„ ë¶ˆêµ¬í•˜ê³  ì–´ë–¤ Metricì„ ì ‘ëª©í•´ì•¼ í•´ë‹¹ ë°ì´í„°ë¥¼ ê°€ì¥ ì˜ ë‚˜íƒ€ë‚¼ì§€ íŒŒì•…í•˜ê¸°ê°€ ì–´ë ¤ì› ë‹¤. Try\r#\rë‚®ì ì„ ìëŠ” ê²ƒ ê¹Œì§„ ê´œì°®ë‹¤ê³  ìƒê°í•˜ì§€ë§Œ, 30ë¶„ ì´ìƒì˜ ë‚®ì ì„ ìì•¼í•˜ëŠ” ìƒí™©ì€ í”¼í•˜ë„ë¡ í•˜ì. Metricë³„ ì–´ë–¤ ìƒí™©ì— ì“°ì´ëŠ”ì§€ë¥¼ ì•Œì•„ë‘˜ í•„ìš”ê°€ ìˆë‹¤. "},{"id":16,"href":"/posts/2023-06-24-KPT-%EC%9D%BC%EC%9D%BC%ED%9A%8C%EA%B3%A0%EB%A1%9D-230328/","title":"KPT20230328","section":"Blog","content":"\rTIL (Today I Learn)\r#\rgit-flow ì„¤ëª…í•˜ê¸° TF-IDF UMAP í™œìš© HDBSCAN í™œìš© Data Clustering ì‹œê°í™” Pandas í™œìš© value_count í™œìš© Dataframe filter í™œìš© groupby, get_group í™œìš© Keep\r#\rì—°êµ¬ì‹¤ ê³¼ì œë¥¼ ì§„í–‰í•˜ë©° ì°¨ì› ì¶•ì†Œ ì•Œê³ ë¦¬ì¦˜ UMAPê³¼, í´ëŸ¬ìŠ¤í„°ë§ ê¸°ë²•ì¸ HDBSCANì„ í™œìš©í•´ë³´ì•˜ë‹¤. ì§ì ‘ ì¨ë³´ë‹ˆ í™•ì‹¤íˆ ê¸°ì–µì— ì˜¤ë˜ ë‚¨ì„ ê²ƒ ê°™ë‹¤. Visualizationë„ ì§ì ‘ í™œìš©í•´ë³´ê³ , Problem\r#\rgit ê°•ì˜ëŠ” ë“£ì§€ ì•Šì•˜ë‹¤.\nì†”ì§íˆ ê°•ì˜ ì§„í–‰ì´ ë§ì´ ëŠë¦¬ë‹¤ê³  ëŠë‚€ë‹¤.\nì´ë¯¸ ì•Œê³  ìˆëŠ” ë‚´ìš©ì´ë¼, ê°•ì˜ë¥¼ ì•ˆë“£ê³  ì‹¶ì§€ë§Œâ€¦ ë§ˆìŒ í¸íˆ ë¬´ì‹œí•˜ì§€ëŠ” ëª»í•´ì„œ, ì‹œê°„ì´ë‚˜ ì§‘ì¤‘ë ¥ì´ í—ˆë¹„ë˜ëŠ” ëŠë‚Œì´ë‹¤.\nê·¸ë˜ë„ ë§ì´ ì¤‘ìš”í•œ ë‚´ìš©ì´ë‹ˆ, ë³µìŠµí–ˆë‹¤ê³  ìƒê°í•˜ì.\nì§€ê¸ˆì´ ìƒˆë²½ 2ì‹œì´ë‹¤. í”¼ê³¤í•˜ê³ â€¦ì§€ì¹œë‹¤.\nTry\r#\rì–´ì„œ ìˆ™ë©´ì„ ì·¨í•˜ì. "},{"id":17,"href":"/posts/2023-06-24-KPT-%EC%9D%BC%EC%9D%BC%ED%9A%8C%EA%B3%A0%EB%A1%9D-230401/","title":"KPT20230401","section":"Blog","content":"ì£¼ë§ì´ì§€ë§Œ, ë°€ë¦° ì§„ë„ì™€ ê³µë¶€ë¥¼ ë”°ë¼ì¡ëŠ” ì‹œê°„ìœ¼ë¡œ ì—´ì‹¬íˆ ë³´ëƒˆë‹¤.\nTIL (Today I Learn)\r#\rMF, SVDë¥¼ ì™„ë²½í•˜ê²Œ ì´í•´í–ˆë‹¤.\nWord2Vec ìƒì„± ë°©ë²•ì„ ì™„ì „íˆ ì´í•´í–ˆë‹¤.\nCBOW â€” ì¤‘ê°„ ë‹¨ì–´ ì˜ˆì¸¡\nSG â€” CBOW ë’¤ì§‘ì€ ë²„ì „\nSGNS â€” SGë¥¼ ì´ì§„ ë¶„ë¥˜ë¡œ ë°”ê¾¼ ê²ƒ.\nMBCF\nNCF\nAE\nDAE\nKeep\r#\rì§‘ì¤‘ì„ ì˜ í•œ ê²ƒ\n4ì‹œ ì „ê¹Œì§€ëŠ” ë§Œì¡±ìŠ¤ëŸ½ê²Œ ì§‘ì¤‘í•˜ëŠ” ì‹œê°„ì„ ê°€ì¡Œë‹¤.\nê·¸ ì´í›„ëŠ” ì²´ë ¥ ë•Œë¬¸ì— ì§‘ì¤‘ë„ê°€ ë–¨ì–´ì¡Œë‹¤.\n4ì‹œ ì´í›„ëŠ” ì§‘ì¤‘ì´ ë–¨ì–´ì§ˆ ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì„ ê°ì•ˆí•˜ê³  ì²´ë ¥ê´€ë¦¬ë¥¼ í•´ì•¼ê² ë‹¤.\nProblem\r#\rëŠ˜ ê·¸ë¬ë“¯ì´, ì ì„ ëŠ¦ê²Œ ìëŠ” ê²ƒ.\nì €ë… 8ì‹œì¯¤ì— ì ì´ ì™€ì„œ ì—„ì²­ ë¶ˆí¸í•œ ìì„¸ë¡œ í•œì‹œê°„ ë°˜ì •ë„ ì ë“¤ì–´ë²„ë ¸ë‹¤. ì°¨ë¼ë¦¬ ì§‘ì•½ì ìœ¼ë¡œ ê³µë¶€í•œ í›„ ì¼ì° ìê±°ë‚˜, ì•„ë‹ˆë©´ í¸í•˜ê²Œ ì§§ê²Œ ìëŠ” ê²Œ ë” ë‚˜ì•˜ì„ í…ë°â€¦\nì—°ë½ì„ ëª»ë°›ì€ ê²ƒ\nì•Œê³ ë³´ë‹ˆ ì „í™”ê°€ ìŒì†Œê±° ìƒíƒœì˜€ë‹¤.\nTry\r#\rì ì„ ë” ìì. "},{"id":18,"href":"/posts/2023-06-24-KPT-%EC%9D%BC%EC%9D%BC%ED%9A%8C%EA%B3%A0%EB%A1%9D-230623/","title":"KPT20230623","section":"Blog","content":"\rTIL (Today I Learn)\r#\rTIL ì‘ì„±í•˜ëŠ” ë°©ë²•\nì´ë•Œê¹Œì§€ ë§¤ì¼ ë°°ìš°ëŠ” ê²ƒì´ ì—†ì–´ì„œ, ì–´ë–»ê²Œ TILì„ ì‘ì„±í•  ì§€ì— ëŒ€í•´ ê³ ë¯¼í–ˆì—ˆë‹¤.\nê¾¸ì¤€í•œ ì‚¬ëŒì—ê²ŒëŠ” ë³€ëª…ì²˜ëŸ¼ ë“¤ë¦´ ìˆ˜ ìˆê² ì§€ë§Œ, ë‚˜ëŠ” TILì„ ë§¤ì¼ ì ëŠ” ê²ƒì´ ì–´ë ¤ì› ë‹¤.\níŒŒì•…í•œ ê°€ì¥ í•µì‹¬ì ì¸ ì´ìœ ëŠ”, ë§¤ì¼ í•™ìŠµëŸ‰ì´ ì¼ì •í•˜ì§€ ì•Šë‹¤ëŠ” ê²ƒì´ë‹¤.\ní•™ìŠµëŸ‰ì´ ì ì€ ë‚ ì€ ì‘ì„±í•  ë‚´ìš©ì´ ì—†ê³ , ë”°ë¼ì„œ TILì„ ì‘ì„±í•˜ëŠ” ê²ƒì´ ì˜¤íˆë ¤ ì£„ì±…ê°ì„ ë¶ˆëŸ¬ì¼ìœ¼í‚¤ê±°ë‚˜, ë‚´ìš© ì—†ëŠ” ê¸€ì„ ì‘ì„±í•˜ê³  ìˆë‹¤ëŠ” ê¸°ë¶„ì´ ë“¤ì—ˆë‹¤.\nê·¸ëŸ¬ë‹¤ë³´ë©´, ìì—°ìŠ¤ë ˆ TILì„ ì“°ì§€ ì•Šê²Œ ëœë‹¤.\nì•ìœ¼ë¡œëŠ” ëª©í‘œ í•™ìŠµëŸ‰ì„ ì±„ìš°ì§€ ëª»í•˜ë©´, Digital Gardenì˜ í‚¤ì›Œë“œë¥¼ ë³µìŠµí•˜ê³  TILì— ì±„ì›Œë„£ê¸°ë¡œ í–ˆë‹¤.\nìœ„ìƒ ì •ë ¬ ì•Œê³ ë¦¬ì¦˜\nìœ„ìƒ ì •ë ¬ì˜ ì‹œê°„ë³µì¡ë„ê°€ O(V + K)ë¼ëŠ” ê²ƒ.\nìœ„ìƒ ì •ë ¬ì„ íë¡œ êµ¬í˜„í•˜ëŠ” ë°©ë²•\nì»¤ë¦¬ì–´ í”„ë ˆì„ì›Œí¬\nKeep\r#\rTILì„ ê¾¸ì¤€íˆ ì‘ì„±í•  ê²ƒ. ëª©í‘œí•˜ëŠ” ë°”ë¥¼ ì–´ë–»ê²Œ ë‹¬ì„±í•  ì§€ ê³ ë¯¼í•œ ê²ƒ ë¶€ì¡±í•œ ì ì„ ê³ ë¯¼í•œ ê²ƒ. ìš´ë™ì„ í•œ ê²ƒ Problem\r#\rì§€ê¸ˆ(ì˜¤ì „ 2:02)ê¹Œì§€ ì ì„ ìì§€ ì•Šì€ ê²ƒ. ë°±ì¤€ ë¬¸ì œë¥¼ í’€ì§€ ì•Šì€ ê²ƒ. ê¹ƒí—ˆë¸Œì— ì”ë””ë¥¼ ì‹¬ì§€ ëª»í•œ ê²ƒ. êµ¬ê¸€ ì¿ ë²„ë„¤í‹°ìŠ¤ ê°•ì˜ë¥¼ ì§€ê¸ˆê¹Œì§€ ë¯¸ë¤„ì„œ ê²°êµ­ ë§¤ìš° ê¸‰í•˜ê²Œ í•´ê²°í•´ì•¼ í•˜ëŠ” ìƒíƒœê°€ ëœ ê²ƒ. Try\r#\rì˜¤ëŠ˜ë³´ë‹¤ ì¼ì° ìê¸°.(ë‚´ì¼ì€ 1ì‹œ ë°˜ ì „ì— ìì) "},{"id":19,"href":"/posts/2023-06-25-KPT-%EC%9D%BC%EC%9D%BC%ED%9A%8C%EA%B3%A0%EB%A1%9D-230624/","title":"KPT20230624","section":"Blog","content":"\rTIL (Today I Learn)\r#\rí•˜ë£¨ì¢…ì¼ ì¿ ë²„ë„¤í‹°ìŠ¤ ê³¼ì œí•˜ë‹¤ê°€, ìœ íŠœë¸Œ í•œ í¸ ë´¤ë”ë‹ˆ ì•„ì´ëŸ¬ë‹ˆí•˜ê²Œë„ ë” ë§ì€ ê²ƒì„ ë°°ì› ë‹¤.\nê¸°ë¡í•˜ëŠ” ë°©ë²•\nê·¹ë‹¨ì ìœ¼ë¡œ ìš”ì•½í•˜ê¸°\ní‚¤ì›Œë“œ ì¤‘ì‹¬ìœ¼ë¡œ ë©”ëª¨í•˜ê¸°\në‚´ ê²ƒìœ¼ë¡œ ì†Œí™”í•œ í›„ ë©”ëª¨í•˜ê¸°\në‚´ ë¨¸ë¦¿ì†ì— ê°ì¸ëœ ê²ƒë“¤ì„ ì¤‘ì‹¬ìœ¼ë¡œ ë©”ëª¨í•˜ê¸°.\ní•˜ë£¨ ì¼ê¸°ì“°ëŠ” ë°©ë²•\ní•˜ë£¨ 1~2ì‹œê°„ë§ˆë‹¤ í•œ ì¤„ì •ë„ì”© ë©”ëª¨í•˜ê¸°\ní”Œë˜ë„ˆ ì‘ì„±í•˜ê¸°\nì˜¤ì „ 2ê°œ, ì˜¤í›„ 1ê°œ, ì €ë… 2ê°œ ëª©í‘œë¡œ ì¡ê¸°.\nì‹œê°„ ë‹¨ìœ„ë¡œ ë‚˜ëˆ„ê²Œ ë˜ë©´ ì•ˆí•˜ê³  ìì±…í•˜ê²Œ ëœë‹¤.\nì—…ë¬´ ì—­ëŸ‰ ì˜¬ë¦¬ê¸°\nì¼ì„ ì‹œì‘í•˜ê¸° 5ë¶„ ì „, ë¬´ì—‡ì— ì§‘ì¤‘í•´ì•¼ í•˜ëŠ”ì§€ ìƒê°í•˜ê¸°.\n1,2,3ë‹¨ê³„ ë‚˜ëˆ„ì–´ì„œ ê²°ê³¼ë¥¼ ì—®ê¸°.\nì´ê²ƒì„ ë‚´ê°€ ì™œ í•˜ëŠ”ì§€, ì™œ í•˜ê³  ì‹¶ì€ì§€ ìƒê°í•´ë³´ê¸°.\në‚´ ê²ƒìœ¼ë¡œ ì´í•´í•˜ê³ , ê°ì¸í•˜ê³ , ì£¼ê´€ì ìœ¼ë¡œ í–‰ë™í•˜ê¸°.\nKeep\r#\rì–»ì€ ì •ë³´ì— ëŒ€í•´ ë©”ëª¨í•œ ê²ƒ. Problem\r#\rë¶€ì •ì ì¸ ìš”ì†Œë¡œ ì‘ìš©í–ˆê±°ë‚˜ ì•„ì‰¬ì› ë˜ ì \nê³µë¶€í•˜ê¸° ì‹«ì–´ì„œ ë¹ˆë‘¥ëŒ„ ê²ƒ.\nì–´ì°¨í”¼ í•´ì•¼í•˜ëŠ” ê³µë¶€ì˜€ìŒì—ë„, í•˜ê¸° ì‹«ì–´ ë¹ˆë‘¥ëŒ€ë©° ë†€ì§€ë„ ì•Šê³ , ê³µë¶€í•˜ì§€ë„ ì•ŠëŠ” ì‹œê°„ì´ ë§ì•˜ë‹¤.\nTry\r#\rProblemì— ëŒ€í•œ í•´ê²° ë°©ì‹ìœ¼ë¡œ ë‹¤ìŒì— ì‹œë„í•´ë³¼ ì \nê³µë¶€í•˜ê¸° ì‹«ì„ ë•, ëª©í‘œí•œ ê³µë¶€ë¥¼ í•˜ë‚˜ë„ ì•ˆí•œ ì±„ ë°¤ 12ì‹œê°€ ë˜ëŠ” ìƒìƒì„ í•˜ì. ê³µë¶€í•˜ê¸° ì‹«ì„ ë•Œ ì‚°ì±… ê²¸ ë‹¬ë¦¬ê¸°ë¥¼ í•˜ì. ìˆ™ë©´ì´ ì¶©ë¶„í–ˆëŠ”ì§€ë¥¼ ìƒê°í•´ë³´ì. ì§€ë‚˜ê³  ìƒê°í•´ë³´ë‹ˆ, ì ì´ ë¶€ì¡±í•´ì„œ ì§‘ì¤‘ë ¥ì´ ë–¨ì–´ì¡Œë˜ ê²ƒ ê°™ë‹¤. "},{"id":20,"href":"/posts/2023-06-28-KPT-%EC%9D%BC%EC%9D%BC%ED%9A%8C%EA%B3%A0%EB%A1%9D-20230628/","title":"KPT20230628","section":"Blog","content":"\rTIL (Today I Learn)\r#\rCFì˜ ë‹¨ì \nCFì˜ ë‹¨ì ì€ ëª¨ë‘ Inputì´ ìœ ì €-ì•„ì´í…œ ìƒí˜¸ì‘ìš© í˜•íƒœì— êµ­í•œëœë‹¤ëŠ” ì ì—ì„œ ë¹„ë¡¯ëœë‹¤.\nCBFì™€ CFëŠ” ì„œë¡œ ìƒí˜¸ ë³´ì™„ì ì¸ ì„±ê²©ì„ ê°€ì§€ë©°, CBFë¥¼ ë‹¨ë…ìœ¼ë¡œ ì˜ ì‚¬ìš©í•˜ì§€ ì•ŠëŠ”ë‹¤.\në§ˆì§€ë§‰ ëª°ì… ì±…ì„ ë‹¤ì‹œ ì½ìœ¼ë©°, ì–´ë– í•œ ì •ë³´ë¥¼ ì•”ê¸°í•˜ê¸° ìœ„í•´ì„œëŠ” í•´ë‹¹ ì •ë³´ê°€ ë‚˜ì¤‘ì— ì–´ë–¤ ìƒí™©ì—ì„œ ë‹¤ì‹œ ì‚¬ìš©ë  ê²ƒì´ë¼ëŠ” ìƒê°ì„ ê°€ì ¸ì•¼ í•œë‹¤ëŠ” ê²ƒì„ ê¹¨ë‹¬ì•˜ë‹¤.\nì´ì™¸ì˜ CDL, Group Aware ì¶”ì²œ, SDAE, FME, FPMC, PRMEì— ëŒ€í•œ ê°œë…ì„ í•™ìŠµí–ˆë‹¤.\nê°•ì˜ìë£Œë¥¼ ì •ë…í•˜ë©° ë…¸ì…˜ì— ê°œë…ì„ ì •ë¦¬í–ˆì§€ë§Œ, ë¨¸ë¦¿ì†ì— ê¸°ì–µë˜ì§€ëŠ” ì•Šì•˜ë‹¤.\nìœ„ì˜ ê°œë…ë“¤ì´ ì–´ë””ì— ì“°ì¼ì§€ë¥¼ ê³ ë¯¼í•´ë³´ì•„ì•¼ ë¨¸ë¦¿ì†ì— ë‚¨ê¸¸ ìˆ˜ ìˆì„ ê²ƒ ê°™ë‹¤.\nKeep\r#\rë§Œì¡±í–ˆê³ , ì•ìœ¼ë¡œ ì§€ì†í•˜ê³  ì‹¶ì€ ë¶€ë¶„ 4ì‹œê°„, 5ì‹œê°„ì •ë„ ëª°ì…í•˜ì—¬ ê³µë¶€í•œ ê²ƒ. ê°€ë”ì”© ì¥ì†Œë¥¼ ë°”ê¿”ì£¼ë©´ ì§‘ì¤‘ì´ ì˜ ë  ê²ƒ ê°™ë‹¤. Problem\r#\rë¶€ì •ì ì¸ ìš”ì†Œë¡œ ì‘ìš©í–ˆê±°ë‚˜ ì•„ì‰¬ì› ë˜ ì \në‚˜ë¦„ ë…¸íŠ¸ ì •ë¦¬ë¥¼ í–ˆì§€ë§Œ, ë¨¸ë¦¿ì†ì— ë‚¨ì§€ ì•Šì•˜ë‹¤.\nìƒˆë¡­ê²Œ ë“¤ì–´ì˜¨ ê°œë…ë“¤ì´ ë„ˆë¬´ ë§ê¸°ë„ í–ˆê³ , ì‚¬ì‹¤ ì–¸ì œ ì–´ë–»ê²Œ ì“°ì´ëŠ”ì§€ì— ëŒ€í•œ ê°ì´ ì¡íˆì§€ ì•Šì•˜ë‹¤.\nTry\r#\rProblemì— ëŒ€í•œ í•´ê²° ë°©ì‹ìœ¼ë¡œ ë‹¤ìŒì— ì‹œë„í•´ë³¼ ì  CDL, SDAE, FPMC, PRME, FMEê°€ ì–´ë”” ì“°ì´ëŠ”ì§€ ì•Œì•„ë³´ê¸°. "},{"id":21,"href":"/posts/2023-06-30-KPT-%EC%9D%BC%EC%9D%BC%ED%9A%8C%EA%B3%A0%EB%A1%9D-20230629/","title":"KPT20230629","section":"Blog","content":"\rTIL (Today I Learn)\r#\rAutoregression\nRegressionì„ ìê¸° ìì‹ ì—ê²Œ ì ìš©í•˜ëŠ” ê²ƒ\níšŒê·€ ë¶„ì„ì˜ ê´€ì ì—ì„œ ê³¼ê±°ì˜ ë°ì´í„°ë¥¼ ë³´ê³  í˜„ì¬ ë˜ëŠ” ë¯¸ë˜ì˜ ê²°ê³¼ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ëª¨ë¸\nì´ì „ë¶€í„° Autoregressionì— ëŒ€í•œ ê°œë… ì„¤ëª…ì„ ëª‡ ë²ˆ ë“¤ì—ˆìœ¼ë‚˜, ì§ê´€ì ìœ¼ë¡œ ì™€ë‹¿ì§€ ì•Šì•˜ëŠ”ë°, ì´ë²ˆì— í•™ìŠµí•˜ë©° ì´í•´ëë‹¤.\nê²½ì‚¬í•˜ê°•ë²• ê¸°ë°˜ ì„ í˜•íšŒê·€ ì•Œê³ ë¦¬ì¦˜\nInput: X, y, lr, T, Output: beta â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” # norm: L2-ë…¸ë¦„ì„ ê³„ì‚°í•˜ëŠ” í•¨ìˆ˜ # lr: í•™ìŠµë¥ , T: í•™ìŠµíšŸìˆ˜ for t in range(T): error = y - X @ beta grad = - transpose(X) @ error beta = beta - lr * grad ì˜¤ëœë§Œì— AI Mathì˜ ë‚´ìš©ì„ ë³µìŠµí–ˆë”ë‹ˆ ë˜ ë§¤ìš° ìƒˆë¡­ë‹¤.\në•ë¶„ì— ë‚´ê°€ ë§ì€ ê²ƒì„ ìŠì—ˆë‹¤ëŠ” ì‚¬ì‹¤ì„ ì•Œê²Œ ë˜ì—ˆë‹¤.\níŒŒì´ì¬ì—ì„œì˜ @ â€” í–‰ë ¬ê³± ì—°ì‚°ì\nì´ì „ì— ì •ë¦¬í•œ ë‚´ìš©ì„ ë³µìŠµí•˜ëŠ”ë°, @ê°€ ë­ì§€??í•˜ê³  ë‹¹í™©í–ˆë‹¤.\nìƒê¹€ìƒˆë¥¼ ë³´ì•„í•˜ë‹ˆ ë°ì½”ë ˆì´í„°ëŠ” ì•„ë‹Œ ë“¯ ë³´ì˜€ë‹¤.\nì°¾ì•„ë³´ë‹ˆ @ëŠ” í–‰ë ¬ê³±ì„ ê³„ì‚°í•˜ëŠ” ì—°ì‚°ìì´ë‹¤.\nì¦‰, numpyì—ì„œì˜ matmulê³¼ ê°™ì€ ì—­í• ì„ í•œë‹¤.\nasteriskë¡œ í‘œí˜„ë˜ëŠ” element-wise ê³± ì—°ì‚°ì´ ì•„ë‹Œ í–‰ë ¬-í–‰ë ¬ê°„ ê³±ì„ ê³„ì‚°í• ë•Œ ì‚¬ìš©í•œë‹¤.\nKeep\r#\rë§Œì¡±í–ˆê³ , ì•ìœ¼ë¡œ ì§€ì†í•˜ê³  ì‹¶ì€ ë¶€ë¶„\nì¼ì „ì— ë§í–ˆë˜ ëŒ€ë¡œ, ì˜¤ëŠ˜ ì¶”ê°€ë¡œ í•™ìŠµí•œ ë‚´ìš©ì´ ì—†ì–´ Digital Gardenì˜ ê°•ì˜ë…¸íŠ¸ë¥¼ ë³µìŠµí–ˆë‹¤.\në³µìŠµí–ˆë”ë‹ˆ, ê¸°ë¡í•  ê²ƒì´ ìƒê²¼ë‹¤.\nì´ ë°©ë²• ê´œì°®ì€ ê²ƒ ê°™ë‹¤.\nProblem\r#\rë¶€ì •ì ì¸ ìš”ì†Œë¡œ ì‘ìš©í–ˆê±°ë‚˜ ì•„ì‰¬ì› ë˜ ì  í˜ì´ì§€ ìƒë‹¨ì— ìš”ì•½ì„ ì‘ì„±í•˜ê³ ì ë§ˆìŒë¨¹ì—ˆì—ˆëŠ”ë°, ë˜ ìŠì–´ë²„ë¦¬ê³  ê²Œì‹œí–ˆë‹¤. Try\r#\rProblemì— ëŒ€í•œ í•´ê²° ë°©ì‹ìœ¼ë¡œ ë‹¤ìŒì— ì‹œë„í•´ë³¼ ì  ë‹¤ìŒ ê²Œì‹œê¸€ì„ ì˜¬ë¦´ ë•Œ í•¨ê»˜ ìˆ˜ì •í•´ì„œ ì—…ë¡œë“œí•˜ì. "},{"id":22,"href":"/posts/2023-11-28-LSTM/","title":"LSTM: Long Short Term Memory","section":"Blog","content":"RNNì˜ ì¥ê¸°ë¬¸ë§¥ ì˜ì¡´ì„±ì„ í•´ê²°í•˜ê¸° ìœ„í•´ íƒ„ìƒí•œ ëª¨ë¸\nì„ ë³„ì  ê²Œì´íŠ¸ë¼ëŠ” ê°œë…ìœ¼ë¡œ ì„ ë³„ ê¸°ì–µ ëŠ¥ë ¥ì„ í™•ë³´í•œë‹¤.\nê·¸ë¦¼ì€ ì´í•´ë¥¼ ë•ê¸°ìœ„í•´ O,Xë¡œ í‘œí˜„í–ˆì§€ë§Œ, ì‹¤ì œë¡œëŠ” ê²Œì´íŠ¸ëŠ” 0~1 ì‚¬ì´ì˜ ì‹¤ìˆ˜ê°’ìœ¼ë¡œ ì—´ë¦° ì •ë„ë¥¼ ì¡°ì ˆí•œë‹¤.\nê²Œì´íŠ¸ì˜ ì—¬ë‹«ëŠ” ì •ë„ëŠ” ê°€ì¤‘ì¹˜ë¡œ í‘œí˜„ë˜ë©° ê°€ì¤‘ì¹˜ëŠ” í•™ìŠµìœ¼ë¡œ ì•Œì•„ë‚¸ë‹¤.\nê°€ì¤‘ì¹˜\nìˆœí™˜ ì‹ ê²½ë§ì˜ ${U, V, W}$ì— 4ê°œë¥¼ ì¶”ê°€í•˜ì—¬ ${U, U_i , U_o , W, W_i , W_o , V}$\n$i$ : ì…ë ¥ ê²Œì´íŠ¸\n$o$ : ì¶œë ¥ ê²Œì´íŠ¸\në‹¤ì–‘í•œ êµ¬ì¡° ì„¤ê³„ê°€ ê°€ëŠ¥í•˜ë‹¤.\nModel Concept\r#\rCell State\r#\rLSTMì˜ í•µì‹¬\nëª¨ë“ˆ ê·¸ë¦¼ì—ì„œ ìˆ˜í‰ìœ¼ë¡œ ê·¸ì–´ì§„ ìœ— ì„ ì— í•´ë‹¹\nì¼ì¢…ì˜ ì»¨ë² ì´ì–´ ë²¨íŠ¸\nì‘ì€ linear interactionë§Œì„ ì ìš©ì‹œí‚¤ë©´ì„œ ë°ì´í„°ì˜ íë¦„ì€ ê·¸ëŒ€ë¡œ ìœ ì§€í•œë‹¤.\nì•„ë¬´ëŸ° ë™ì‘ì„ ì¶”ê°€í•˜ì§€ ì•ŠëŠ”ë‹¤ë©´, ì •ë³´ëŠ” ì „í˜€ ë°”ë€Œì§€ ì•Šê³  ê·¸ëŒ€ë¡œ íë¥¸ë‹¤.\nCell Stateì—ì„œ gateì— ì˜í•´ ì •ë³´ê°€ ì¶”ê°€ë˜ê±°ë‚˜ ì‚­ì œëœë‹¤.\nGate\r#\rForget Gate\nInput Gate\nCell State ì—…ë°ì´íŠ¸\nOutput Gate\nìˆ˜ì‹ ìš”ì•½\r#\r$$ \\begin{aligned}f_t \u0026amp; =\\sigma_g\\left(W_f x_t+U_f h_{t-1}+b_f\\right) \\i_t \u0026amp; =\\sigma_g\\left(W_i x_t+U_i h_{t-1}+b_i\\right) \\o_t \u0026amp; =\\sigma_g\\left(W_o x_t+U_o h_{t-1}+b_o\\right) \\\\tilde{c}t \u0026amp; =\\sigma_c\\left(W_c x_t+U_c h{t-1}+b_c\\right) \\c_t \u0026amp; =f_t \\odot c_{t-1}+i_t \\odot \\tilde{c}_t \\h_t \u0026amp; =o_t \\odot \\sigma_h\\left(c_t\\right)\\end{aligned} $$\nft = sigmoid(np.dot(xt, Wf) + np.dot(ht_1, Uf) + bf) # forget gate it = sigmoid(np.dot(xt, Wi) + np.dot(ht_1, Ui) + bi) # input gate ot = sigmoid(np.dot(xt, Wo) + np.dot(ht_1, Uo) + bo) # output gate Ct = ft * Ct_1 + it * np.tanh(np.dot(xt, Wc) + np.dot(ht_1, Uc) + bc) ht = ot * np.tanh(Ct) ëª¨ë¸ ìš”ì•½\r#\rInput / Output Shape\r#\rimport torch import torch.nn as nn # Size: [batch_size, seq_len, input_size/num_of_features] input = torch.randn(3, 5, 4) lstm = nn.LSTM(input_size=4, hidden_size=2, batch_first=True) output, h = lstm(input) output.size() # =\u0026gt; torch.Size([3, 5, 2]), batch_size, seq_len, hidden_size LSTMì„ í™œìš©í•˜ì—¬ ì£¼ì‹ ê°€ê²©ì„ ì˜ˆì¸¡ â€” ê³¼ê±° 5ì¼ì˜ ì¢…ê°€ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ê²½ìš° Seq_len = 5 Input_size = 1(ì¢…ê°€) Batch_size = N LSTMì„ í™œìš©í•˜ì—¬ ì£¼ì‹ ê°€ê²©ì„ ì˜ˆì¸¡ â€” ê³¼ê±° 5ì¼ì˜ ì‹œê°€, ì¢…ê°€, ê±°ë˜ëŸ‰ì„ ì˜ˆì¸¡í•˜ëŠ” ê²½ìš° Seq_len = 5 Input_size = 3(ì‹œê°€, ì¢…ê°€, ê±°ë˜ëŸ‰) Batch_size = N LSTMì„ í™œìš©í•˜ì—¬ ì£¼ì‹ ê°€ê²©ì„ ì˜ˆì¸¡ â€” ì—¬ëŸ¬ ì—°ì†í˜• ë³€ìˆ˜ì™€ ë²”ì£¼í˜• ë³€ìˆ˜ê°€ í¬í•¨ëœ ê²½ìš° Seq_len = 5 Input_size = embedding size(ì‹œê°€, ì¢…ê°€, ê±°ë˜ëŸ‰) Batch_size = N Model\r#\rê¸°ë³¸ìœ¼ë¡œ ì œê³µë˜ëŠ” Featureë“¤ì„ ë³‘í•©í•˜ì—¬ LSTMì— ì£¼ì…í•œë‹¤.\ndef forward(self, input): test, question, tag, _, mask, interaction, _ = input batch_size = interaction.size(0) #Embedding embed_interaction = self.embedding_interaction(interaction) embed_test = self.embedding_test(test) embed_question = self.embedding_question(question) embed_tag = self.embedding_tag(tag) embed = torch.cat([embed_interaction, embed_test, embed_question, embed_tag,], 2) X = self.conb_proj(embed) hidden = self.init_hidden(batch_size) out, hidden = self.lstm(X, hidden) out = out.contiguous().view(batch_size, -1, self.hidden_dim) out = self.fc(out) preds = self.activation(out).view(batch_size, -1) return preds LSTM + Attention\r#\rê¸°ì¡´ì˜ LSTM ëª¨ë¸ì— Attention Layerì„ ì¶”ê°€í•œë‹¤.\ndef forward(self, input): test, question, tag, _, mask, interaction, _ = input batch_size = interaction.size(0) #Embedding embed_interaction = self.embedding_interaction(interaction) embed_test = self.embedding_test(test) embed_question = self.embedding_question(question) embed_tag = self.embedding_tag(tag) embed = torch.cat([embed_interaction, embed_test, embed_question, embed_tag,], 2) X = self.conb_proj(embed) hidden = self.init_hidden(batch_size) out, hidden = self.lstm(X, hidden) out = out.contiguous().view(batch_size, -1, self.hidden_dim) **extended_attention_mask = mask.unsqueeze(1).unsqueeze(2) extended_attention_mask = extended_attention_mask.to(dtype=torch.float32) extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0 head_mask = [None] * self.n_layers encoded_layers = self.attn(out, extended_attention_mask, head_mask=head_mask) sequence_output = encoded_layer[-1]** out = self.fc(sequence_output) preds = self.activation(out).view(batch_size, -1) return preds BERT\r#\rdef forward(self, input): test, question, tag, _, mask, interaction, _ = input batch_size = interaction.size(0) #Embedding embed_interaction = self.embedding_interaction(interaction) embed_test = self.embedding_test(test) embed_question = self.embedding_question(question) embed_tag = self.embedding_tag(tag) embed = torch.cat([embed_interaction, embed_test, embed_question, embed_tag,], 2) X = self.conb_proj(embed) **# Bert encoded_layers = self.encoder(inputs_embeds=X, attention_mask=mask) out=encoded_layers[0]** out = out.contiguous().view(batch_size, -1, self.hidden_dim) **** out = self.fc(sequence_output) preds = self.activation(out).view(batch_size, -1) return preds ì°¸ê³  ìë£Œ\nLong Short-Term Memory (LSTM) ì´í•´í•˜ê¸°\n"},{"id":23,"href":"/posts/2023-06-21-MICE/","title":"MICE : Multiple Imputation Chained Equation","section":"Blog","content":"Multiple Imputation Chained Equation(ë‹¤ì¤‘ ì‚°ì… ì—°ë¦½ ë°©ì •ì‹)\nMICE ì ‘ê·¼ ë°©ì‹ì—ëŠ” MIì—ì„œ ì–¸ê¸‰ëœ ë™ì¼í•œ ê°œë…ì´ ì ìš©ëœë‹¤.\nê°’ë“¤ì€ ê° ë°©ì‹ì— ë”°ë¼ ì‚°ì…ëœ í›„ ì™„ì „í•œ ë°ì´í„°ì…‹ì— ëŒ€í•œ ë¶„ì„ì´ ì§„í–‰ë˜ê³  ê²°ê³¼ê°€ í•©ì³ì§„ë‹¤. ë‹¤ë§Œ ì°¨ì´ì ìœ¼ë¡œ, MIì—ì„œëŠ” ëª¨ë“  ë³€ìˆ˜ì— ëŒ€í•´ ë™ì‹œì— ì‚°ì…ë˜ì§€ë§Œ, MICEì—ì„œëŠ” ê° ë³€ìˆ˜ì˜ ê°’ì´ ìˆœì°¨ì ìœ¼ë¡œ ì‚°ì…ëœë‹¤.\nProcess\r#\rëˆ„ë½ëœ ë°ì´í„°ì˜ ì–‘ì´ ê°€ì¥ ì ì€ ë³€ìˆ˜ê°€ ê°€ì¥ ë¨¼ì € ì‚°ì…ëœë‹¤.\nê°€ì¥ ì²« ë³€ìˆ˜ëŠ” mean replacement(í‰ê·  ëŒ€ì²´) ë°©ë²•ìœ¼ë¡œ ì±„ì›Œì§„ë‹¤.\nì´í›„, ì±„ì›Œì§„ ë³€ìˆ˜ëŠ” ë‹¤ë¥¸ ë³€ìˆ˜ë¥¼ ì±„ìš¸ ë•Œ í•¨ê»˜ ì˜ˆì¸¡ ë³€ìˆ˜ë¡œ ì‚¬ìš©ëœë‹¤.\níšŒê·€ë¡œ ê²°ì¸¡ê°’ì´ ëª¨ë‘ ì±„ì›Œì§„ ê·¸ëŸ´ì‹¸í•œ ë¶„í¬ì˜ ë§¤ê°œë³€ìˆ˜ëŠ” ê²°ì¸¡ê°’ì„ ì¬ì¶”ì •í•˜ëŠ” ë° ì‚¬ìš©ëœë‹¤.\nì´í›„ ì—¬ëŸ¬ ì‚¬ì´í´ ë™ì•ˆ ì—°ë¦½ ë°©ì •ì‹ì„ ë°˜ë³µí•œë‹¤.\nì‚¬ì´í´ì´ ì™„ë£Œë˜ë©´ ë°ì´í„°ê°€ \u0026ldquo;ì™„ì „í•œ\u0026rdquo; ë°ì´í„°ì…‹ìœ¼ë¡œ ì €ì¥ëœë‹¤.\nê° ë³€ìˆ˜ëŠ” ê°œë³„ì ìœ¼ë¡œ ì‚°ì…ë˜ê¸° ë•Œë¬¸ì—, ê° ë³€ìˆ˜ ìœ í˜•ì— ì í•©í•œ ëª¨ë¸ë“¤ì´ ì‚¬ìš©ëœë‹¤.\nBinary Variable â†’ Logistic regression Model Categorical Variable â†’ Multinomial Logit Model Ordered categorical Variable â†’ Ordered Logit Model R íŒ¨í‚¤ì§€ì— MICEê°€ í¬í•¨ë˜ì–´ ìˆë‹¤.\nMICE algorithmì—ëŠ” FCS(Fully Conditional Specification)ì„ êµ¬í˜„í–ˆë‹¤.\nMICEì— ë‚´ì¥ëœ Imputation ë°©ë²•\nPmm (any): Predictive mean matching Sample (any): Random sample from observed values Mean (numeric): Unconditional mean imputation norm.nob (numeric): Linear regression ignoring model error Logreg (binary): Logistic regression Polr (ordered): Proportional odds model Polyreg (unordered): Polytomous logistic regression methods(mice)ë¥¼ ì‚¬ìš©í•˜ë©´ imputation í•  ìˆ˜ ìˆëŠ” ë°©ë²•ë¡ ì„ ì­‰ ì•Œë ¤ì¤€ë‹¤.\ní•˜ì§€ë§Œ, ê°€ë…ì„±ì´ ì¢‹ì§€ ì•Šê¸° ë•Œë¬¸ì— ì¶”ì²œí•˜ì§„ ì•ŠëŠ”ë‹¤.\nì´ë³´ë‹¨ ê·¸ëƒ¥ ?miceë¡œ ê²€ìƒ‰í•˜ì—¬ í™•ì¸í•˜ë©´ ë” ìì„¸í•˜ê²Œ ì˜ ì í˜€ìˆë‹¤.\nmd.pattern()\r#\rí–‰ë ¬ì´ë‚˜ ë°ì´í„°í”„ë ˆì„ì˜ í˜•íƒœë¡œ ê²°ì¸¡ë°ì´í„°ì˜ ìœ í˜•ë“¤ì„ í‘œë¡œ ë‚˜íƒ€ë‚¸ë‹¤.\nmd.pattern(x, plot = TRUE, rotate.names = FALSE)\ní‘œ ë³´ëŠ” ë°©ë²•\nê°€ì¥ ì™¼ìª½ ì—´ì˜ ìˆ«ì: í•´ë‹¹ patternìœ¼ë¡œ êµ¬ì„±ëœ ë°ì´í„°ì˜ ê°œìˆ˜\nì²« ë²ˆì§¸ í–‰ì„ ë³´ë©´, ê²°ì¸¡ì¹˜ê°€ ì—†ëŠ” ë°ì´í„°ê°€ 4133ê°œë¼ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤.\në‘ ë²ˆì§¸ í–‰ì„ ë³´ë©´, R.DExpemsesë§Œ ê²°ì¸¡ì¹˜ê°€ ë°œìƒí•œ ë°ì´í„°ê°€ 11ê°œë¼ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤.\nê°€ì¥ ì˜¤ë¥¸ìª½ ì—´ì˜ ìˆ«ì: ê²°ì¸¡ì¹˜ê°€ ë°œìƒí•œ ë³€ìˆ˜(column)ì˜ ê°œìˆ˜\nìµœí•˜ë‹¨ì˜ ìˆ«ì: ê° ë³€ìˆ˜ì—ì„œ ê²°ì¸¡ê°’ì˜ ì´ ê°œìˆ˜\nmd.pairs()\r#\rì¶œë ¥ ê°’ì—ëŠ” rr, rm, mr, mm ë„¤ ê°€ì§€ êµ¬ì„± ìš”ì†Œê°€ ì¡´ì¬í•œë‹¤.\nmì€ Missingë¨ì„ ì˜ë¯¸í•˜ê³ , rì€ response, ì¡´ì¬í•¨ì„ ì˜ë¯¸í•œë‹¤.\n"},{"id":24,"href":"/posts/2023-03-29-NBCF-Neighborhood-based-CF/","title":"NBCF: Neighborhood-based CF(ì´ì›ƒ ê¸°ë°˜ í˜‘ì—… í•„í„°ë§)","section":"Blog","content":"Neighborhood-based CF í˜¹ì€ Memory Based CFë¼ê³  ë¶€ë¥´ê¸°ë„ í•œë‹¤.\nì‚¬ìš©ì ë˜ëŠ” ì•„ì´í…œ ê°„ì˜ similarity ê°’ì„ ê³„ì‚°í•˜ê³  ì´ë¥¼ rating prediction ë˜ëŠ” top-K rankingì— í™œìš©í•˜ëŠ” ë°©ë²•\nSimilarityë¥¼ ê³„ì‚°í•˜ê¸° ìœ„í•œ Metricìœ¼ë¡œ Jaccard, Cosine, Pearson ë“±ì„ í™œìš©í•œë‹¤.\nì¥ì \nêµ¬í˜„ì´ ê°„ë‹¨í•˜ê³  ì´í•´í•˜ê¸° ì‰½ë‹¤. Similarityë¥¼ í™œìš©í•˜ê¸° ë•Œë¬¸ì— ì¶”ì²œì˜ ì´ìœ ì— ëŒ€í•œ ì§ê´€ì ì¸ ì„¤ëª…ì„ ì œê³µí•œë‹¤. ìµœì í™”ë‚˜ í›ˆë ¨ ê³¼ì •ì´ í•„ìš” ì—†ë‹¤. ë‹¨ì \nSparsity(í¬ì†Œì„±) ë¬¸ì œ\nNBCFë¥¼ ì ìš©í•˜ë ¤ë©´ ì ì–´ë„ sparsity ratioê°€ 99.5%ë¥¼ ë„˜ì§€ ì•ŠëŠ” ê²ƒì´ ì¢‹ë‹¤.\në°ì´í„°ê°€ ì¶©ë¶„í•˜ì§€ ì•Šë‹¤ë©´ ìœ ì‚¬ë„ ê³„ì‚°ì´ ë¶€ì •í™•í•´ì§„ë‹¤. â‡’ ì„±ëŠ¥ ì €í•˜\nCold Start\në°ì´í„°ê°€ ë¶€ì¡±í•˜ê±°ë‚˜ í˜¹ì€ ì•„ì˜ˆ ì—†ëŠ” ìœ ì €, ì•„ì´í…œì˜ ê²½ìš° ì¶”ì²œì´ ë¶ˆê°€ëŠ¥í•˜ë‹¤.\nScalability(í™•ì¥ì„±) ë¬¸ì œ\nìœ ì €ì™€ ì•„ì´í…œì´ ëŠ˜ì–´ë‚ ìˆ˜ë¡ ìœ ì‚¬ë„ ê³„ì‚°ì´ ëŠ˜ì–´ë‚œë‹¤.\nì¶”ë¡ ì„ ìœ„í•œ ë§ì€ ì–‘ì˜ Offline ê³„ì‚°ì´ ìš”êµ¬ëœë‹¤.\nìœ ì €, ì•„ì´í…œì´ ë§ì•„ì•¼ ì •í™•í•œ ì˜ˆì¸¡ì„ í•˜ì§€ë§Œ ë°˜ëŒ€ë¡œ ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦°ë‹¤.\nì´ì›ƒ ë°ì´í„°ë¥¼ í•™ìŠµí•œë‹¤.\níŠ¹ì • ì£¼ë³€ ì´ì›ƒì— ì˜í•´ í¬ê²Œ ì˜í–¥ì„ ë°›ì„ ìˆ˜ ìˆë‹¤.\nê³µí†µì˜ ìœ ì € / ì•„ì´í…œì„ ë§ì´ ê³µìœ í•´ì•¼ë§Œ ìœ ì‚¬ë„ ê°’ì´ ì •í™•í•´ì§„ë‹¤.\nìœ ì‚¬ë„ ê°’ì´ ì •í™•í•˜ì§€ ì•Šì€ ê²½ìš° ì´ì›ƒì˜ íš¨ê³¼ë¥¼ ë³´ê¸° ì–´ë µë‹¤.\nUBCF: User-based CF(ìœ ì € ê¸°ë°˜ í˜‘ì—… í•„í„°ë§)\r#\rëŒ€ìƒ ìœ ì €ì™€ ìœ ì‚¬ë„ê°€ ë†’ì€ ìœ ì €ë“¤ì´ ì„ í˜¸í•˜ëŠ” ì•„ì´í…œì„ ì¶”ì²œí•˜ëŠ” ë°©ì‹\nex) ì˜í™” í‰ì \nUser Aê°€ User Bì™€ ì„ í˜¸ë„ê°€ ë¹„ìŠ·í•˜ë¯€ë¡œ, User Bì˜ ìŠ¤íƒ€ì›Œì¦ˆ í‰ì ì€ ë†’ì„ ê²ƒì´ë¼ê³  ì˜ˆìƒí•œë‹¤. í‰ì  ì‚°ì¶œ ë°©ì‹\nAbsolute Rating\nAverage â€” í‰ê· ë‚´ê¸°\nëª¨ë“  ìœ ì €ì˜ í‰ì ì„ í‰ê· ë‚´ì„œ í‰ì ì„ ì˜ˆì¸¡í•œë‹¤.\nWeighted Average â€” ìœ ì €ì˜ ìœ ì‚¬ë„ë¥¼ ë°˜ì˜í•˜ì—¬ í‰ì  ì˜ˆì¸¡\nìœ ì € ê°„ì˜ ìœ ì‚¬ë„ë¥¼ ë°˜ì˜í•˜ì—¬ í‰ì ì„ ì˜ˆì¸¡í•œë‹¤.\ní•œê³„\nìœ ì €ê°€ í‰ì ì„ ì£¼ëŠ” ê¸°ì¤€ì´ ì œê°ê¸° ë‹¤ë¥´ë‹¤.\në”°ë¼ì„œ, ê° ìœ ì €ë³„ ë°œìƒí•˜ëŠ” í¸ì°¨ë¥¼ ì œëŒ€ë¡œ ë°˜ì˜í•˜ì§€ ëª»í•˜ê²Œ ëœë‹¤.\nRelative Rating\nìœ ì €ì˜ í‰ì ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ì§€ ì•Šê³ , ìœ ì €ì˜ í‰ê·  í‰ì ì—ì„œì˜ í¸ì°¨ë¥¼ ì‚¬ìš©í•œë‹¤.\n$$ \\begin{gathered}\\operatorname{dev}(u, i)=r(u, i)-\\overline{r_u} \\quad \\text { for known rating } \\\\widehat{\\operatorname{dev}}(u, i)=\\frac{\\sum_{u \\Omega^{\\prime} \\in \\Omega_i} \\operatorname{dev}\\left(u^{\\prime}, i\\right)}{\\left|\\Omega_i\\right|}=\\frac{\\sum_{u^{\\prime} \\in \\Omega_i} r\\left(u^{\\prime}, i\\right)-\\overline{r_{u^{\\prime}}}}{\\left|\\Omega_i\\right|} \\\\hat{r}(u, i)=\\overline{r_u}+\\frac{\\sum_{u \\prime \\in \\Omega_i} r\\left(u^{\\prime}, i\\right)-\\overline{r_{u^{\\prime}}}}{\\left|\\Omega_i\\right|}=\\overline{r_u}+\\widehat{\\operatorname{dev}}(u, i)\\end{gathered} $$\nex) ìœ ì € Bì˜ ìŠ¤íƒ€ì›Œì¦ˆì— ëŒ€í•œ ì˜ˆì¸¡ í‰ì \nì˜ˆì¸¡ Deviation = 0.23\n$$ {1.6 \\times 0.95 + (-1.6) \\times (0.6) + 0 \\times 0.85\\over0.95 + 0.6 + 0.85} = 0.23 $$\nìœ ì € Bì˜ í‰ì  í‰ê·  = 3\nìœ ì € Bì˜ ì˜ˆì¸¡ í‰ì  = 3.23\nIBCF: Item-based CF(ì•„ì´í…œ ê¸°ë°˜ í˜‘ì—… í•„í„°ë§)\r#\ríƒ€ê²Ÿ ì•„ì´í…œê³¼ ìœ ì‚¬ë„ê°€ ë†’ì€ ì•„ì´í…œ ì¤‘ ì„ í˜¸ë„ê°€ í° ì•„ì´í…œì„ ì¶”ì²œí•˜ëŠ” ë°©ì‹\nì¥ì \nì‹œê°„ì— ë”°ë¼ ìœ ì‚¬ë„ ë³€í™”ê°€ ì ë‹¤.\nUser based CFì— ë¹„í•´ ê³„ì‚°ëŸ‰ì´ ì ë‹¤.\nex) ì˜í™” í‰ì \ní—í¬ì™€ ìŠ¤íƒ€ì›Œì¦ˆì˜ ìœ ì €ë³„ í‰ì  ë¶„í¬ê°€ ë¹„ìŠ·í•˜ë¯€ë¡œ, ìœ ì € Bì˜ ìŠ¤íƒ€ì›Œì¦ˆ í‰ì ì´ ë†’ì„ ê²ƒì´ë¼ ì˜ˆì¸¡í•œë‹¤. í‰ì  ì‚°ì¶œ ë°©ì‹\nAbsolute Rating\nAverage â€” í‰ê· ë‚´ê¸°\nëª¨ë“  ì•„ì´í…œì˜ í‰ì ì„ í‰ê· ë‚´ì„œ í‰ì ì„ ì˜ˆì¸¡í•œë‹¤.\nWeighted Average â€” ì•„ì´í…œì˜ ìœ ì‚¬ë„ë¥¼ ë°˜ì˜í•˜ì—¬ í‰ì  ì˜ˆì¸¡\nì•„ì´í…œ ê°„ì˜ ìœ ì‚¬ë„ë¥¼ ë°˜ì˜í•˜ì—¬ í‰ì ì„ ì˜ˆì¸¡í•œë‹¤.\nì´ ë•Œ, ì¼ë¶€ ì•„ì´í…œë§Œ í™œìš©í•˜ì—¬ í‰ì ì„ ì˜ˆì¸¡í•˜ê¸°ë„ í•œë‹¤.\nRelative Rating\nì•„ì´í…œì˜ í‰ì ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ì§€ ì•Šê³ , ì•„ì´í…œì˜ í‰ê·  í‰ì ì—ì„œì˜ í¸ì°¨ë¥¼ ì´ìš©í•œë‹¤.\nex) ìœ ì € Bì˜ ìŠ¤íƒ€ì›Œì¦ˆì— ëŒ€í•œ ì˜ˆì¸¡ í‰ì \nì½”ì‚¬ì¸ ìœ ì‚¬ë„ ì‚¬ìš©. 2-NN ê¸°ì¤€ ì•„ì´ì–¸ë§¨ 0.9, í—í¬ 0.95, ìŠ¤íƒ€ì›Œì¦ˆ í‰ê·  3.0\nì˜ˆì¸¡ Deviation = 1.15\n$$ {0.9 \\times 0.25 + 0.95 \\times2\\over0.9 + 0.95} = 1.15 $$\nì•„ì´í…œ Bì˜ í‰ì  í‰ê·  = 3\nìœ ì € Bì˜ ì˜ˆì¸¡ í‰ì  = 4.15\nTop-N Recommendation\nëŒ€ìƒ ìœ ì €ì— ëŒ€í•œ ì•„ì´í…œ ì˜ˆì¸¡ í‰ì  ê³„ì‚°ì´ ì™„ë£Œë˜ë©´, í‰ì ìˆœìœ¼ë¡œ ì •ë ¬í•˜ì—¬ ìƒìœ„ Nê°œë§Œ ì¶”ì²œí•˜ê¸°\n"},{"id":25,"href":"/posts/2023-09-23-Optimizer/","title":"Optimizerë€?","section":"Blog","content":" ê°„ë‹¨ ìš”ì•½\nLossì˜ ë¯¸ë¶„ê°’ì„ íŒŒë¼ë¯¸í„°ì— ì–´ë–»ê²Œ ë°˜ì˜í•  ì§€ì— ëŒ€í•œ ë°©ë²•\në°˜ì˜ ë°©ë²• : lossì˜ ë¯¸ë¶„ ê°’ì„ íŒŒë¼ë¯¸í„°ì— ì–´ë–»ê²Œ ë°˜ì˜í•  ê²ƒì¸ê°€? Learning rate : í•œ ë²ˆì— ì–¼ë§ˆë‚˜ ë°˜ì˜í•  ê²ƒì¸ê°€?\n{: .prompt-info } Background: Gradient Descent(GD)ì—ì„œì˜ Issue\r#\r1. Local minima, Saddle point\r#\r{: w=\u0026ldquo;700\u0026rdquo; h=\u0026ldquo;400\u0026rdquo; }\nì‹¤ì œë¡œëŠ” Local Minimaë³´ë‹¨ ì•ˆì¥ì (saddle point)ì´ ë¬¸ì œì¸ ê²½ìš°ê°€ ë” ë§ë‹¤.\nlocal minimaê°€ ë˜ê¸° ìœ„í•´ì„  ëª¨ë“  ë³€ìˆ˜ ë°©í–¥ì—ì„œ lossê°€ ì¦ê°€í•´ì•¼ í•˜ëŠ”ë°, ì´ëŠ” í”ì¹˜ ì•Šë‹¤.\n{: w=\u0026ldquo;400\u0026rdquo; h=\u0026ldquo;250\u0026rdquo; }\n{: w=\u0026ldquo;400\u0026rdquo; h=\u0026ldquo;250\u0026rdquo; }\nëŒ€ì‹  ìœ„ì˜ ìƒí™©ì—ì„œ gradient descent ì•Œê³ ë¦¬ì¦˜ì´ í‰í‰í•œ ê³³ì— ë¨¸ë¬¼ëŸ¬ë²„ë¦¬ëŠ” ë¬¸ì œê°€ ë°œìƒí•  ìˆ˜ ìˆë‹¤.\nê¼­ ë¨¸ë¬´ë¥´ì§€ ì•Šë”ë¼ë„ ì£¼ë³€ì´ í‰í‰í•˜ê¸° ë•Œë¬¸ì— ë§¤ìš° ë”ë””ê²Œ í•™ìŠµì´ ì§„í–‰ë˜ëŠ” ë¬¸ì œì ë„ ìˆë‹¤.\n2. ê¸¸ í—¤ë§¤ê¸°\r#\rSGD, Mini-batch GDê°€ êµ‰ì¥íˆ í—¤ë§¤ë©´ì„œ ê¸¸ì„ ì°¾ëŠ”ë‹¤.\ní—¤ë§¤ëŠ” ì •ë„ë¥¼ ì¤„ì¼ í•„ìš”ê°€ ìˆë‹¤.\nì´ë¥¼ ìœ„í•´ Optimizerê°€ ë“±ì¥í–ˆë‹¤.\nOptimizerì—ì„œëŠ” SGDì—ì„œ í¬ê²Œ ë‘ ê°€ì§€ë¥¼ ê°œì„ í•œë‹¤.\n1. ë°©í–¥\r#\rSGDì—ì„œ Optimumì„ í–¥í•´ ë‚˜ì•„ê°ˆ ë•Œ, ìœ„ì˜ ì˜ˆì‹œì²˜ëŸ¼ ë°©í–¥ì„ ëŠì„ì—†ì´ ë°”ê¾¸ë©° ë‚˜ì•„ê°„ë‹¤.\nì´ë³´ë‹¨, ì˜¬ë°”ë¥¸ ë°©í–¥ìœ¼ë¡œ ë” ë§ì´ ê°€ëŠ” ê²ƒì„ ì›í•œë‹¤.\nì´ë¥¼ ìœ„í•´ **ê´€ì„±(Momentum)**ì„ ì ìš©í•œë‹¤.\nMomentum\r#\rê°€ì¤‘ì¹˜ë¥¼ ê°±ì‹ í•  ë•Œ, ì´ì „ì— ë‚˜ì•„ê°”ë˜ ë°©í–¥ë„ ë°˜ì˜ì„ í•´ì¤€ë‹¤.\në‹¨ì \ní•™ìŠµë¥ ì— ë”°ë¼ minimum pointì— ìˆ˜ë ´í•˜ì§€ ëª»í•˜ëŠ” ê²½ìš°ê°€ ë°œìƒí•œë‹¤.\nNAG : Nesterov Accelerated Gradient\r#\rMomentumì˜ ë‹¨ì ì„ ê°œì„ í•œ ë°©ë²•.\nê´€ì„± ë°©í–¥ìœ¼ë¡œ ë¨¼ì € ì´ë™í•œ í›„, gradientë¥¼ ê³„ì‚°í•œë‹¤.\nMomentumë³´ë‹¤ ìˆ˜ë ´ì´ ë” ë¹ ë¥´ë‹¤.\n2. ê±°ë¦¬(í•™ìŠµë¥ )\r#\rAdagard\r#\rí˜„ì¬ê¹Œì§€ ê°’ì´ ë§ì´ ë³€í•œ íŒŒë¼ë¯¸í„°ì— ëŒ€í•´ì„œëŠ” ì ê²Œ ë³€í™”ì‹œí‚¤ê³ , ì ê²Œ ë³€í•œ íŒŒë¼ë¯¸í„°ëŠ” ë§ì´ ë³€í™”ì‹œí‚¨ë‹¤.\në‹¨ì \n$G_t$ì˜ ê°’ì€ ê³„ì† ì»¤ì§€ëŠ”ë°, ì´ ê°’ì´ ë¬´í•œëŒ€ì— ê°€ê¹ê²Œ ì»¤ì§€ê²Œ ë˜ë©´ ê°’ì´ 0ì´ ë˜ì–´ë²„ë¦°ë‹¤.\nì¦‰, ì´ë™ì„ ë©ˆì¶°ë²„ë¦¬ê²Œ ëœë‹¤.\nAdadelta\r#\rAdagradì˜ ë‹¨ì ì„ ê°œì„ í•œ ë°©ë²•.\n$G_t$ê°€ ë„ˆë¬´ ì»¤ì§€ëŠ” ê²ƒì„ ë°©ì§€í•œë‹¤.\nLearning rateê°€ ì—†ì–´ ë³€í˜•ì´ ë¶ˆê°€ëŠ¥í•˜ê¸° ë•Œë¬¸ì—, ì˜ ì“°ì§€ ì•ŠëŠ”ë‹¤.\nRMSProp\r#\rAdadelta + stepsize.\nAdam\r#\rMomentum + RMSProp\nê°€ì¥ ë§ì´ ì“°ì¸ë‹¤.\nê·¸ë¦¼ìœ¼ë¡œ ë³´ëŠ” ë‹¤ì–‘í•œ Optimization ê¸°ë²•ë“¤\në”¥ëŸ¬ë‹(Deep learning) ì‚´í´ë³´ê¸° 2íƒ„\n"},{"id":26,"href":"/posts/2023-09-12-Polynomial-Interpolation/","title":"Polynomial Interpolation(ë³´ê°„ ë‹¤í•­ì‹)","section":"Blog","content":"\rLinear Interpolation\r#\rê°€ì¥ ê°„ë‹¨í•œ ë³´ê°„ë²•\në‘ ì ì„ ì´ì€ ì§ì„ ì˜ ë°©ì •ì‹ì„ ê·¼ì‚¬ í•¨ìˆ˜ë¡œ ì‚¬ìš©í•œë‹¤.\në°ì´í„° ì ë“¤ ì‚¬ì´ì˜ ê°„ê²©ì´ ì‘ì„ìˆ˜ë¡ ë” ì¢‹ì€ ê·¼ì‚¿ê°’ì„ ì–»ëŠ”ë‹¤.\n$$ \\mathrm{g}(x)=\\frac{f\\left(x_{i+1}\\right)-f\\left(x_i\\right)}{x_{i+1}-x_i}\\left(x-x_i\\right)+f\\left(x_i\\right) $$\nPolynomial interpolation\r#\r(n+1)ê°œì˜ ì ì´ ì£¼ì–´ì§„ ê²½ìš°Â nì°¨ ì´í•˜ì˜ ìœ ì¼í•œ ë‹¤í•­ì‹ì„ êµ¬í•  ìˆ˜ ìˆë‹¤.\nQ. n+1ê°œì˜ ì ìœ¼ë¡œ ì°¾ì„ ìˆ˜ ìˆëŠ” nì°¨ ë‹¤í•­ì‹ì€ ì™œ ìœ ì¼í•œê°€?\në°©ë°ë¥´ëª½ë“œ í–‰ë ¬\nê° í–‰ì˜ ì´ˆí•­ì´ 1ì¸ ë“±ë¹„ìˆ˜ì—´ë¡œ ì´ë£¨ì–´ì§„ í–‰ë ¬\n$$ V=\\left(\\begin{array}{ccccc}1 \u0026amp; \\alpha_1 \u0026amp; \\alpha_1^2 \u0026amp; \\cdots \u0026amp; \\alpha_1^{n-1} \\1 \u0026amp; \\alpha_2 \u0026amp; \\alpha_2^2 \u0026amp; \\cdots \u0026amp; \\alpha_2^{n-1} \\1 \u0026amp; \\alpha_3 \u0026amp; \\alpha_3^2 \u0026amp; \\cdots \u0026amp; \\alpha_3^{n-1} \\\\vdots \u0026amp; \\vdots \u0026amp; \\vdots \u0026amp; \u0026amp; \\vdots \\1 \u0026amp; \\alpha_m \u0026amp; \\alpha_m^2 \u0026amp; \\cdots \u0026amp; \\alpha_m^{n-1}\\end{array}\\right) $$\në°©ë°ë¥´ëª½ë“œ í–‰ë ¬ $V$ì— ëŒ€í•´ ë‹¤ìŒê³¼ ê°™ì´ ì¼ë°˜í™”í•  ìˆ˜ ìˆë‹¤.\n$$ \\operatorname{det} V=\\prod_{i\u0026lt;j}\\left(\\alpha_i-\\alpha_j\\right) $$\në”°ë¼ì„œ, $a_0,a_1,\u0026hellip;,a_n$ì´ ì„œë¡œ ë‹¤ë¥¸ ê°’ì„ ê°€ì§„ë‹¤ë©´ $V$ëŠ” ì—­í–‰ë ¬ì´ ì¡´ì¬í•œë‹¤.\nê°€ì—­ í–‰ë ¬ì˜ ê¸°ë³¸ ì •ë¦¬ì— ì˜í•´ $\\tt Vx = b$ì˜ í•´ëŠ” ìœ ì¼í•˜ë‹¤.\nì´ì œ ë‹¤í•­ì‹ì„ ì°¾ì•„ë‚´ëŠ” ì„¸ ê°€ì§€ ë°©ë²•ì„ ì•Œì•„ë³´ì.\në¯¸ì • ê³„ìˆ˜ë²•\r#\rë‹¤í•­ì‹ì„ ì°¾ëŠ” ê°€ì¥ ë³´í¸ì ì¸ ë°©ë²•\në³´ê°„ ë‹¤í•­ì‹ $p(x) = a_0 + a_1x + a_2x^2 + \u0026hellip; + a_nx^n$ì— ëŒ€í•´\nì£¼ì–´ì§„ $n+1$ê°œì˜ ì ì„ $p(x)$ì— ëŒ€ì…í•˜ì—¬ ì—°ë¦½ ë°©ì •ì‹ì„ ìƒì„±í•œë‹¤.\nëª¨ë‘ ë‹¤ ëŒ€ì…í•˜ë©´ ì•„ë˜ì™€ ê°™ì´ ë°©ë°ë¥´ëª½ë“œ í–‰ë ¬ì‹ í˜•íƒœë¥¼ ì–»ì„ ìˆ˜ ìˆë‹¤.\n$$ \\begin{array}{cc}p\\left(x_0\\right)=f\\left(x_0\\right) \u0026amp; a_0+a_1 x_0+a_2 x_0^2+\\cdots+a_n x_0^n=f\\left(x_0\\right) \\p\\left(x_1\\right)=f\\left(x_1\\right) \u0026amp; a_0+a_1 x_1+a_2 x_1^2+\\cdots+a_n x_1^n=f\\left(x_1\\right) \\\\vdots \u0026amp; \\vdots \\p\\left(x_n\\right)=f\\left(x_n\\right) \u0026amp; a_0+a_1 x_n+a_2 x_n^2+\\cdots+a_n x_n^n+\\left(x_n\\right)\\end{array} $$\n$$ \\left[\\begin{array}{ccccc}1 \u0026amp; x_0 \u0026amp; x_0^2 \u0026amp; \\cdots \u0026amp; x_0^n \\1 \u0026amp; x 1 \u0026amp; x_1^2 \u0026amp; \\cdots \u0026amp; x_1^n \\\u0026amp; \u0026amp; \u0026amp; \\cdots \u0026amp; \\1 \u0026amp; x_n \u0026amp; x_n^2 \u0026amp; \\cdots \u0026amp; x_n^n\\end{array}\\right]\\left[\\begin{array}{l}a 0 \\a 1 \\\\cdots \\a n\\end{array}\\right]=\\left[\\begin{array}{c}f\\left(x_0\\right) \\f\\left(x_1\\right) \\\\cdots \\f\\left(x_n\\right)\\end{array}\\right] $$\nê°€ìš°ìŠ¤ ì†Œê±°ë²• ë“±ìœ¼ë¡œ ì—°ë¦½ ë°©ì •ì‹ì˜ í•´ë¥¼ êµ¬í•œë‹¤.\në‹¨ì \nëŠë¦° ê³„ì‚° ì‹œê°„ ì˜¤ì°¨ ë°œìƒ LagrangeÂ Interpolation(ë¼ê·¸ë‘ì£¼Â ë³´ê°„ë²•)\r#\rì—°ë¦½ ë°©ì •ì‹ì„ í’€ì§€ ì•Šê³  ë‹¤í•­ì‹ì„ ê²°ì •í•˜ëŠ” ë°©ë²•\níŠ¹ì • ìˆ«ìë¥¼ ëŒ€ì…í•˜ë©´ 0ì´ë‚˜ 1ì´ ë˜ëŠ” í•­ì„ ë§Œë“ ë‹¤.\n$(x-a)$ë¥¼ ê³±í•´ì£¼ë©´ $x$ì— aê°’ì„ ëŒ€ì…í•  ë•Œ 0ì´ ëœë‹¤. $(x-a)$ë¥¼ $(b-a)$ë¡œ ë‚˜ëˆ„ë©´, $x$ì— $b$ë¥¼ ëŒ€ì…í–ˆì„ ë•Œ 1ì´ ëœë‹¤. 1ì°¨ í•¨ìˆ˜\në‘ ì $(x_0, y_0), (x_1, y_1)$ì´ ì£¼ì–´ì§„ ê²½ìš°\n$$ y=\\left(\\frac{x-x_1}{x_0-x_1}\\right) y_0+\\left(\\frac{x-x_0}{x_1-x_0}\\right) y_1 $$\nìœ„ ì‹ì— $x_0$ì„ ëŒ€ì…í•˜ë©´ $y_0$ì´ ë‚˜ì˜¤ê³ , $x_1$ì„ ëŒ€ì…í•˜ë©´ $y_1$ì´ ë‚˜ì˜¨ë‹¤.\ní•´ë‹¹ ì‹ì€ ì§ê´€ì ìœ¼ë¡œ $(x_0,0),(x_1,y_1)$ì„ ì§€ë‚˜ëŠ” ì§ì„ ì˜ ê¸°ìš¸ê¸°ì™€ $(x_0,y_0),(x_1,0)$ì„ ì§€ë‚˜ëŠ” ì§ì„ ì˜ ê¸°ìš¸ê¸°ì˜ í•©ìœ¼ë¡œ ì´í•´í•  ìˆ˜ ìˆë‹¤.\n2ì°¨ í•¨ìˆ˜\nì„¸ ì $(x_0, y_0), (x_1, y_1), (x_2, y_2)$ì´ ì£¼ì–´ì§„ ê²½ìš°\n$$ y=\\left(\\frac{\\left(x-x_1\\right)\\left(x-x_2\\right)}{\\left(x_0-x_1\\right)\\left(x_0-x_2\\right)}\\right) y_0+\\left(\\frac{\\left(x-x_2\\right)\\left(x-x_0\\right)}{\\left(x_1-x_2\\right)\\left(x_1-x_0\\right)}\\right) y_1\\ +\\left(\\frac{\\left(x-x_0\\right)\\left(x-x_1\\right)}{\\left(x_2-x_0\\right)\\left(x_2-x_1\\right)}\\right) y_2 $$\në§ˆì°¬ê°€ì§€ë¡œ, $(x_i,y_i)$ë¥¼ ì§€ë‚˜ë©´ì„œ $(x_j,0)$ì„ ì§€ë‚˜ëŠ” ì´ì°¨ í•¨ìˆ˜ì˜ ê¸°ìš¸ê¸°ì˜ í•©ìœ¼ë¡œ ì´í•´í•  ìˆ˜ ìˆë‹¤.\n3ì°¨ í•¨ìˆ˜\në„¤ ì ì´ ì£¼ì–´ì§„ ê²½ìš°\n$$ y=\\left(\\frac{\\left(x-x_1\\right)\\left(x-x_2\\right)\\left(x-x_3\\right)}{\\left(x_0-x_1\\right)\\left(x_0-x_2\\right)\\left(x_0-x_3\\right)}\\right) y_0\\ +\\left(\\frac{\\left(x-x_2\\right)\\left(x-x_3\\right)\\left(x-x_0\\right)}{\\left(x_1-x_2\\right)\\left(x_1-x_3\\right)\\left(x_1-x_0\\right)}\\right) y_1\\ +\\left(\\frac{\\left(x-x_0\\right)\\left(x-x_1\\right)\\left(x-x_3\\right)}{\\left(x_2-x_0\\right)\\left(x_2-x_1\\right)\\left(x_2-x_3\\right)}\\right) y_2\\ +\\left(\\frac{\\left(x-x_0\\right)\\left(x-x_1\\right)\\left(x-x_2\\right)}{\\left(x_3-x_0\\right)\\left(x_3-x_1\\right)\\left(x_3-x_2\\right)}\\right) y_3 $$\nì¼ë°˜í™”\n$$ L_i(x)=\\prod_{\\substack{j=0 \\ j \\neq i}}^n \\frac{x-x_j}{x_i-x_j} $$\nìœ„ì˜ ì‹ì€ ê³§ $x_i$ë¥¼ ë„£ì—ˆì„ ë•Œ $y_i$ê°€ ë‚˜ì˜¨ë‹¤ëŠ” ê²ƒì„ ì˜ë¯¸í•œë‹¤.\n$$ \\begin{matrix}P_n(x)\u0026amp;=\u0026amp;L_0(x) f\\left(x_0\\right)+L_1(x) f\\left(x_1\\right)+\\cdots L_n(x) f\\left(x_n\\right)\\\\ \u0026amp;=\u0026amp;\\sum_{i=0}^n L_{i(x)} f\\left(x_i\\right) \\end{matrix} $$\nQ. ë‹¨ì¼ í•¨ìˆ˜ë¥¼ êµ¬í•  ìˆ˜ ìˆëŠ”ê°€?\n$P_n\\left(x_i\\right)=y_i$ë¥¼ ë§Œì¡±í•˜ë¯€ë¡œ $\\mathrm{n}+1$ê°œì˜ ì  $\\left(x_i, y_i\\right)$ì„ ì§€ë‚˜ëŠ” ìœ ì¼í•œ $\\mathrm{n}$ì°¨ ë‹¤í•­ì‹ì´ë‹¤.\nQ. ì—°ì‚°ëŸ‰ì´ ê¸°ì¡´ ë°©ë²•ê³¼ ë¹„êµí–ˆì„ ë•Œ ëŠ˜ì–´ë‚˜ëŠ”ê°€, ì¤„ì–´ë“œëŠ”ê°€?\në‹¨ì \nì°¨ìˆ˜ê°€ ì»¤ì§€ë©´ ì°¸ ê°’ì„ ê¸°ëŒ€í•  ìˆ˜ ì—†ì„ ì •ë„ì˜ ì˜¤ì°¨ê°€ ë°œìƒí•œë‹¤.\në°ì´í„°ì˜ ìˆ˜ê°€ ì¦ê°€í•  ë•Œ,Â ë°”ë¡œ ì§ì „ì˜ ê²°ê³¼ë¥¼ ì‚¬ìš©í•˜ì§€ ëª»í•œë‹¤.\nì ì´ ì¶”ê°€ë˜ë©´ ì‹ì„ ì²˜ìŒë¶€í„° ë‹¤ì‹œ ê³„ì‚°í•´ì•¼ í•œë‹¤.\ní•˜ë‚˜ì˜ ë³´ê°„ì„ ìœ„í•´ í•„ìš”í•œ ê³„ì‚°ëŸ‰ì´ ë§ë‹¤.\nê·¸ë ‡ë‹¤ë©´, ì‹ì„ ì²˜ìŒë¶€í„° ë‹¤ì‹œ ê³„ì‚°í•˜ì§€ ì•ŠëŠ” ë°©ë²•ì€ ì—†ì„ê¹Œ?\nê²°êµ­ ë¼ê·¸ë‘ì£¼ ë³´ê°„ë²•ì€ ê° ì ì„ ì§€ë‚˜ëŠ” í•¨ìˆ˜ì˜ ê¸°ìš¸ê¸°ë¥¼ í•©ì‚°í•˜ëŠ” ë°©ì‹ì´ê¸° ë•Œë¬¸ì—, ê¸°ìš¸ê¸°ê°€ ì¤‘ë³µìœ¼ë¡œ ê³„ì‚°ë˜ëŠ” ì§€ì ì„ ì œê±°í•˜ì—¬ ì—°ì‚°ëŸ‰ì„ ì¤„ì¼ ìˆ˜ ìˆë‹¤. ì´ë¥¼ ìœ„í•´ Divided Differenceì´ ì“°ì¸ë‹¤.\nNewtonâ€™s divided differences interpolation\r#\rë¼ê·¸ë‘ì£¼ ë³´ê°„ë²•ì˜ ëª¨ë“  ë‹¨ì ì„ í•´ê²°í•˜ëŠ” ë°©ë²•\nì  ë‘ ê°œë¡œ ì¼ì°¨ í•¨ìˆ˜ë¥¼ ë¨¼ì € ê·¸ë¦° í›„, ì ì„ ì¶”ê°€í•´ë‚˜ê°€ë©° ë‹¤í•­ì‹ì˜ ì°¨ìˆ˜ë¥¼ ì ì  í™•ì¥í•´ë‚˜ê°€ëŠ” ë°©ì‹ ì—°ë¦½ ì¼ì°¨ ë°©ì •ì‹ì„ ì‚¬ìš©í•˜ì§€ ì•ŠëŠ”ë‹¤. ë‰´í„´ í˜•ì„ í™œìš©í•œë‹¤. Divided Differences(ë¶„í•  ì°¨ë¶„ë²•)\r#\rê°„ë‹¨ ìš”ì•½\në¶„í•  êµ¬ê°„ì—ì„œ í•¨ìˆ˜ê°’ë“¤ì˜ ì°¨ì´\nê¸°ìš¸ê¸°ì— ëŒ€í•œ ì´ì‚°ì ì¸ ì¶”ì •ì¹˜ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤.\në‰´í„´ í˜•ì´ ì£¼ì–´ì¡Œì„ ë•Œ, ìƒìˆ˜ í•­ë“¤ì„ ì–´ë–»ê²Œ ê³„ì‚°í•´ì•¼ í• ê¹Œ?\në‰´í„´ í˜•\nì„œë¡œ ë‹¤ë¥¸ ì  $x_0, \u0026hellip;, x_n$ì— ëŒ€í•´ xê°’ì— ë”°ë¼ ìƒìˆ˜ í•­ë“¤ì„ ìˆœì„œëŒ€ë¡œ êµ¬í•  ìˆ˜ ìˆëŠ” í˜•íƒœ\n$$ \\begin{matrix}P_n(x)=a_0+a_1\\left(x-x_0\\right) +a_2\\left(x-x_0\\right)\\left(x-x_1\\right) +\\\\ldots +a_n\\left(x-x_0\\right)\\left(x-x_1\\right) \\ldots\\left(x-x_{n-1}\\right)\\end{matrix} $$\nì‹ì´ ë³µì¡í•˜ê²Œ ìƒê²¼ë‹¤. $P_n(x) = f(x)$ë¼ ì •ì˜í•˜ê³ , ì‹ì„ ìƒìˆ˜ í•­ ê¸°ì¤€ìœ¼ë¡œ ì •ë¦¬í•´ë³´ì.\n$a_0, a_1$ ë„ì¶œ ê³¼ì •\r#\r$$ a_0 = f\\left(x_0\\right)\\ f(x_1) = a_0 + a_1(x_1-x_0), \\ \\therefore a_1 = {f\\left(x_1\\right) - f(x_0)\\over x_1-x_0} $$\ní•´ë‹¹ ì‹ì˜ í˜•íƒœë¥¼ $f[x_0,x_1]$ë¡œ ì¹˜í™˜í•˜ì.\nì´ëŠ” first order Divided Differenceë¼ê³  ë¶€ë¥¸ë‹¤.\nfirst order Divided Difference\r#\r$$ f\\left[x_0, x_1\\right]=\\frac{f\\left(x_1\\right)-f\\left(x_0\\right)}{x_1-x_0} $$\ní•œê¸€ë¡œ ë²ˆì—­í•˜ë©´ 1ì°¨ ë¶„í•  ì°¨ë¶„ì¸ë°, ì´ëŠ” ìœ„ì˜ ìˆ˜ì‹ì´ 1ì°¨ ë¯¸ë¶„ì— ëŒ€í•œ ì´ì‚°ì ì¸ ì¶”ì •ì¹˜ë¡œ ì“°ì¼ ìˆ˜ ìˆê¸° ë•Œë¬¸ì´ë‹¤.\në§Œì•½ $f(x)$ê°€ êµ¬ê°„ $[x_0,x_1]$ì—ì„œ ë¯¸ë¶„ ê°€ëŠ¥í•˜ë‹¤ë©´, í‰ê· ê°’ ì •ë¦¬ì— ì˜í•´$f\\left[x_0, x_1\\right]=f^{\\prime}(c)$ì„ì„ ë³´ì¥í•œë‹¤.\n$a_2$ ë„ì¶œ ê³¼ì •\r#\r$$ \\begin{matrix} f(x_2) \u0026amp;=\u0026amp; a_0 + a_1(x_2 - x_0) + a_2(x_2-x_0)(x_2-x_1)\\ \u0026amp;=\u0026amp; f(x_0) + (x_2 - x_0)(a_1 + a_2(x_2 - x_1)),\\ f(x_2) - f(x_0) \u0026amp;=\u0026amp; (x_2-x_0)(a_1 + a_2(x_2 - x_1)) \\end{matrix} $$\nì´ê³ , ì´ë¥¼ ì¢€ ë” ì •ë¦¬í•˜ë©´\n$$ \\frac{f(x_2)-f(x_0)}{x_2-x_0} = a_1 + a_2(x_2 - x_1) $$\nì´ ëœë‹¤.\nì—¬ê¸°ì„œ, $a_1$ê³¼ $a_2$ì—ì„œ ë°˜ë³µë˜ëŠ” í˜•íƒœë¥¼ $f[x_a,x_b]$ë¡œ ì¹˜í™˜í•˜ì.\n$$ f[x_0,x_2] = f[x_0,x_1] + a_2(x_2 - x_1)\\ \\frac{f[x_0,x_2] - f[x_0,x_1]}{x_2 - x_1} = a_2\\ $$\nìœ„ ì‹ì€ $f[x_0,x_1,x_2]$ë¡œ ì¹˜í™˜í•˜ë©°, Second order Divided Differenceë¼ê³  ë¶€ë¥¸ë‹¤.\nHigh order Divided Difference\r#\r$$ f\\left[x_0, x_1, x_2\\right]=\\frac{f\\left[x_1, x_2\\right]-f\\left[x_0, x_1\\right]}{x_2-x_0} $$\n$$ f\\left[x_0, x_1, x_2, x_3\\right]=\\frac{f\\left[x_1, x_2, x_3\\right]-f\\left[x_0, x_1, x_2\\right]}{x_3-x_0} $$\n$$ f\\left[x_0, \\ldots, x_n\\right]=\\frac{f\\left[x_1, \\ldots, x_n\\right]-f\\left[x_0, \\ldots, x_{n-1}\\right]}{x_n-x_0} $$\nì´ì™€ ê°™ì€ í˜•íƒœë¡œ ë‚˜ë¨¸ì§€ $a_n$ì— ëŒ€í•´ì„œë„ ì •ë¦¬í•  ìˆ˜ ìˆê³ , ìµœì¢…ì ìœ¼ë¡œ ê¸°ì¡´ì˜ ë‰´í„´ í˜•ì€ ë‹¤ìŒê³¼ ê°™ì€ í˜•íƒœê°€ ëœë‹¤.\n$$ \\begin{aligned}\u0026amp; P_1(x)=f\\left(x_0\\right)+\\left(x-x_0\\right) f\\left[x_0, x_1\\right] \\\u0026amp; \\begin{aligned}P_2(x)=f\\left(x_0\\right) \u0026amp; +\\left(x-x_0\\right) f\\left[x_0, x_1\\right] \\\u0026amp; +\\left(x-x_0\\right)\\left(x-x_1\\right) f\\left[x_0, x_1, x_2\\right]\\end{aligned}\\ \u0026amp;\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\vdots\\\u0026amp; \\begin{aligned}P_n(x)=f\\left(x_0\\right) \u0026amp; +\\left(x-x_0\\right) f\\left[x_0, x_1\\right]+\\cdots \\\u0026amp; +\\left(x-x_0\\right)\\left(x-x_1\\right) \\cdots\\left(x-x_{n-1}\\right) f\\left[x_0, x_1, \\ldots, x_n\\right]\\end{aligned}\\end{aligned} $$\nì—¬ê¸°ì—ì„œ ì¤‘ìš”í•œ ì ì€, ê¸°í˜¸í™”ë¥¼ í•¨ìœ¼ë¡œì¨ ê°’ì„ ì¬í™œìš©í•  ìˆ˜ ìˆê²Œ ë˜ì—ˆë‹¤ëŠ” ê²ƒì´ë‹¤.\në˜í•œ, ê¸°ì¡´ì˜ ê°’ì„ í™œìš©í•˜ì—¬ ë‹¤ìŒ ê°’ì„ êµ¬í•  ìˆ˜ ìˆê²Œ ë˜ì—ˆë‹¤.\nìµœì¢…ì ìœ¼ë¡œ ë‰´í„´ ê³µì‹ì„ ì¼ë°˜í™”í•˜ì—¬ ì •ë¦¬í•˜ë©´ ë‹¤ìŒê³¼ ê°™ì€ í˜•íƒœê°€ ëœë‹¤.\n$$ \\begin{aligned}P_n(x) \u0026amp; =f\\left[x_0\\right]+f\\left[x_0, x_1\\right]\\left(x-x_0\\right)+\\cdots\\ \u0026amp;\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ +f\\left[x_0, \\cdots, x_n\\right]\\left(x-x_0\\right) \\cdots\\left(x-x_{n-1}\\right) \\\u0026amp; =f\\left[x_0\\right]+\\sum_{k=1}^n f\\left[x_0, \\cdots, x_k\\right]\\left(x-x_0\\right) \\cdots\\left(x-x_{k-1}\\right) \\\u0026amp; =f\\left[x_0\\right]+\\sum_{k=1}^n f\\left[x_0, \\cdots, x_k\\right] \\prod_{i=0}^{k-1}\\left(x-x_i\\right)\\end{aligned} $$\nì´ë¥¼ ì í™”ì‹ì˜ í˜•íƒœë¡œ ì •ë¦¬í•˜ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.\n$$ p_{n+1}(x)=p_n(x)+f[x_0, x_1, \\cdots, x_n, x_{{n+1}}] \\prod_{j=0}^n(x-x_j) $$\n2. Error in polynomial interpolation\r#\rë³´ê°„ ë‹¤í•­ì‹ì€ ê²°êµ­ ì‹¤ì œ í•¨ìˆ˜ì— ëŒ€í•œ ì¶”ì •ì´ê¸° ë•Œë¬¸ì—, ì˜¤ì°¨ê°€ ì¡´ì¬í•œë‹¤.\në¼ê·¸ë‘ì£¼ ë³´ê°„ë²•ì˜ ì˜¤ì°¨ë¥¼ ê³„ì‚°í•´ë³´ì.\nì•ì—ì„œ $P_n(x)$ë¥¼ ë‹¤ìŒê³¼ ê°™ì´ ì •ë¦¬í–ˆë‹¤.\n$$ \\begin{matrix}P_n(x)\u0026amp;=\u0026amp;L_0(x) f\\left(x_0\\right)+L_1(x) f\\left(x_1\\right)+\\cdots L_n(x) f\\left(x_n\\right)\\ \u0026amp;=\u0026amp;\\sum_{i=0}^n L_{i(x)} f\\left(x_i\\right) \\end{matrix} $$\n$f(x):$ êµ¬ê°„ $[a,b]$ì—ì„œ ì •ì˜ëœ í•¨ìˆ˜(ì‹¤ì œ í•¨ìˆ˜) $p_n(x): n+1$ $n+1$$f(x)$ì˜ ë³´ê°„ ë‹¤í•­ì‹ ì´ë¼ í–ˆì„ ë•Œ, ë‹¤ìŒì´ ì„±ë¦½í•œë‹¤.\n$$ f(x)=P_n(x) + \\frac{\\left(x-x_0\\right)\\left(x-x_1\\right) \\cdots\\left(x-x_n\\right)}{(n+1) !} f^{(n+1)}\\left(c_x\\right) $$\n$c_x : [a,b]$ êµ¬ê°„ ë‚´ ì„ì˜ì˜ ì \nì¦ëª… ê³¼ì •\nì‹¤ì œ í•¨ìˆ˜$f(x)$ì™€ ë³´ê°„ ë‹¤í•­ì‹ $P_n(x)$ì˜ ì°¨ì´ì— ëŒ€í•œ í•¨ìˆ˜ë¥¼ $R_n(x)$ë¼ í•˜ì. $(x \\neq x_k)$\nì¦‰, $f(x) = P_n(x) + R_n(x)$ê°€ ì„±ë¦½í•˜ëŠ” ìƒí™©ì—ì„œ,\n$R_n(x)$ëŠ” $x_k$ë§ˆë‹¤ 0ì´ ë˜ê¸° ë•Œë¬¸ì— ë‹¤ìŒê³¼ ê°™ì´ ì •ì˜í•  ìˆ˜ ìˆë‹¤.\n$$ R_n(x)=C \\prod_{k=0}^n\\left(x-x_k\\right) $$\n$C$ëŠ” ìƒìˆ˜ë¥¼ ì˜ë¯¸í•œë‹¤. ìƒˆë¡œìš´ í•¨ìˆ˜ $F(x)$ë¥¼\n$$ F(x) = f(x) - P_n(x) - R_n(x) $$\në¼ê³  í•  ë•Œ,\n**ë¡¤ì˜ ì •ë¦¬(Rolle\u0026rsquo;s Theorem)**ì— ì˜í•´ $\\mathrm{n}$ê°œ ì ì—ì„œ í•¨ìˆ˜ê°€ 0ì´ë©´, $\\mathrm{n}-1$ì°¨ ë¯¸ë¶„ì˜ ê°’ì´ 0ì¸ ì ì´ ì¡´ì¬í•œë‹¤.\n$\\mathrm{g}(\\mathrm{t})$ ëŠ” $x, x_0, x_1, \\ldots, x_n$ ì˜ êµ¬ê°„ìœ¼ë¡œ $\\mathrm{n}+2$ê°œì˜ í•¨ìˆ˜ê°€ 0 ì¸ ì ì´ ì¡´ì¬í•˜ë¯€ë¡œ, $\\mathrm{n}+1$ ì°¨ ë¯¸ë¶„ì´ 0 ì¸ ì  $c_x$ê°€ ì¡´ì¬í•œë‹¤.\n$$ f^{n+1}(c_x)-P^{n+1}(c_x)-[f(x)-P(x)] \\frac{d^{n+1}}{d t^{n+1}}\\left[\\Pi_{i=0}^n \\frac{t-x_i}{x-x_i}\\right]_{t=c_x} $$\n$\\mathrm{P}$ ëŠ” ìµœëŒ€ $\\mathrm{n}$ì°¨ì‹ì´ë¯€ë¡œ $P^{n+1}=0$ $g^{n+1}(c_x)=0=f^{n+1}(c_x)-0-f(x)-P(x) ! \\Pi_{i=0}^n \\frac{1}{x-x_i}$\n$\\left(t-x_i\\right)$ ëŠ” $\\mathrm{n}+1$ ì°¨í•­ì´ë¯€ë¡œ $\\mathrm{n}+1$ë²ˆ ë¯¸ë¶„í•˜ë©´ $(\\mathrm{n}+1)!$ ìœ„ ì‹ì„ $\\mathrm{f}(\\mathrm{x})$ ì— ëŒ€í•´ ì •ë¦¬í•˜ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.\n$$ f(x)=P(x)+\\frac{f^{n+1}(c_x)}{(n+1) !}\\left(x-x_0\\right)\\left(x-x_1\\right) \\ldots\\left(x-x_n\\right) $$\nê²°ë¡ ì ìœ¼ë¡œ, ì˜¤ì°¨(ì‹¤ì œ í•¨ìˆ˜ - ë³´ê°„ ë‹¤í•­ì‹)ëŠ” ë‹¤ìŒê³¼ ê°™ì´ ì •ì˜ëœë‹¤.\n$$ e_n(x)=f(x)-P_n(x) $$\n$$ f(x)-P(x)=\\frac{f^{n+1}(c_x)}{(n+1) !}\\left(x-x_0\\right)\\left(x-x_1\\right) \\ldots\\left(x-x_n\\right) $$\nìµœëŒ€ ì˜¤ì°¨ëŠ” $\\max |\\frac{f^{n+1}(c_x)}{(n+1) !}| \\cdot \\max |\\left(x-x_0\\right)\\left(x-x_1\\right) \\ldots\\left(x-x_n\\right)|$ $f(x):$ êµ¬ê°„ $[a,b]$ì—ì„œ ì •ì˜ëœ í•¨ìˆ˜(ì‹¤ì œ í•¨ìˆ˜)\n$p_n(x): n+1$ $n+1$$f(x)$ì˜ ë³´ê°„ ë‹¤í•­ì‹\nì´ë¼ í–ˆì„ ë•Œ, ì˜¤ì°¨(ì‹¤ì œ í•¨ìˆ˜ - ë³´ê°„ ë‹¤í•­ì‹)ëŠ” ë‹¤ìŒê³¼ ê°™ì´ ì •ì˜ëœë‹¤.\n$$ e_n(x)=f(x)-p_n(x) $$\në”°ë¼ì„œ ë‹¤ìŒì´ ì„±ë¦½í•œë‹¤.\n$$ \\begin{aligned} \u0026amp; p_{n+1}\\left(x_i\\right)=f\\left(x_i\\right), \\quad i=0,1,2, \\cdots, n \\ \u0026amp; p_{n+1}(\\bar{x})=f(\\bar{x}) \\end{aligned} $$\në‰´í„´ ê³µì‹ìœ¼ë¡œ ë‹¤ì‹œ í‘œí˜„í•˜ë©´\n$$ p_{n+1}(x)=p_n(x)+f\\left[x_0, x_1, \\cdots, x_n, \\bar{x}\\right] \\prod_{j=0}^n(x-x_j) $$\nê³¼ ê°™ê³ , ì´ ë•Œì˜ $f(x)$ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.\n$$ f(\\bar{x})=p_{n+1}+f\\left[x_0, x_1, \\cdots, x_n, \\bar{x}\\right) \\prod_{j=0}^n\\left(\\bar{x}-x_j\\right) $$\nì•„ë˜ì—ì„œ í‘œí˜„ëœ ì‹ë“¤ë¡œ ì˜¤ì°¨ì— ëŒ€í•œ ì‹ì„ ë‹¤ì‹œ ì •ë¦¬í•´ë³´ë©´\n$$ e_n(\\bar{x})=f\\left[x_0, x_1, \\cdots, x_n, \\bar{x}\\right] \\prod_{j=0}^n\\left(\\bar{x}-x_j\\right) $$\nìœ„ì²˜ëŸ¼ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆë‹¤.\nì°¸ê³  ìë£Œ\n[ìˆ˜ì¹˜í•´ì„] 6. Lagrange Interpolation\në‹¤í•­ í•¨ìˆ˜ ë³´ê°„(Polynomial Interpolation)\nìˆ˜ì¹˜í•´ì„ ë° ì‹¤ìŠµ - 6 ë¶„í•  ì°¨ë¶„í‘œì™€ ë³´ê°„í‘œ\n6ì°¨ì‹œ - ë¶„í• ì°¨ë¶„í‘œì™€ ë³´ê°„ë²•(1)\nê·¸ë ‡ë‹¤ë©´ ê³¼ì—° ë‰´í„´ ë³´ê°„ë²•ì€ ë‹¨ì ì´ ì—†ì„ê¹Œ? ê·¸ë ‡ì§€ ì•Šë‹¤.\n3. Spline Interpolation\r#\rë‰´í„´ ë³´ê°„ë²•ê³¼ ë¼ê·¸ë‘ì£¼ ë³´ê°„ë²•ì€ ê³„ë‹¨ í•¨ìˆ˜ì™€ ê°™ì€ ê¸‰ê²©í•œ ë¶ˆì—°ì†ì„ ì˜ í‘œí˜„í•˜ì§€ ëª»í•œë‹¤.\nRunge í˜„ìƒ\nRunge í•¨ìˆ˜ëŠ” Polynomialë¡œ ì í•©ì´ ì˜ ë˜ì§€ ì•ŠëŠ” í•¨ìˆ˜ë¡œ ì•Œë ¤ì ¸ ìˆë‹¤.\n$$ f(x)=\\frac{1}{1+25 x^2} $$\nGibbs í˜„ìƒ ë¶ˆì—°ì† í•¨ìˆ˜ë¥¼ ê·¼ì‚¬í•  ë•Œ ë¶ˆì—°ì† ê°’ ê·¼ì²˜ì—ì„œ ë‚˜íƒ€ë‚˜ëŠ” ë¶ˆì¼ì¹˜ í˜„ìƒ Piecewise Polynomials Interpolation\r#\rì—¬ëŸ¬ ê°œì˜ ë°ì´í„°ë¥¼ í•˜ë‚˜ì˜ ì¶”ì • í•¨ìˆ˜ë¡œ í‘œí˜„í•˜ì§€ ì•Šê³ , êµ¬ê°„ ë³„ë¡œ ì¶”ì • í•¨ìˆ˜ë¥¼ êµ¬í•˜ëŠ” ê²ƒ\nì‚¬ì§„ì€ Interpolationì´ ì•„ë‹ˆë¼ Regressionì— í•´ë‹¹í•˜ì§€ë§Œ, Piecewise Polynomialì— ëŒ€í•œ ì´í•´ë¥¼ ë•ê¸° ìœ„í•´ ê°€ì ¸ì™”ë‹¤.\në‹¤ë§Œ, ì‚¬ì§„ì²˜ëŸ¼ knotì—ì„œ ë¶ˆì—°ì†ì´ê¸° ë•Œë¬¸ì—, í•©ë¦¬ì ì´ì§€ ì•Šì€ ì¶”ì • í•¨ìˆ˜ê°€ ë‚˜ì˜¬ ìˆ˜ ìˆë‹¤.\nContinuous Piecewise Polynomials Interpolation\r#\rPiecewiseì— ì—°ì†ì´ë¼ëŠ” ì œì•½ ì¡°ê±´ì„ ì¶”ê°€í–ˆë‹¤.\ní•˜ì§€ë§Œ ì—¬ì „íˆ ë§Œì¡±ìŠ¤ëŸ½ì§€ ì•Šë‹¤.\nSpline\r#\rê° ì§€ì (knots)ì—ì„œ ìê¸° ìì‹ ê³¼ 1ì°¨ ë¯¸ë¶„ í•¨ìˆ˜ë¶€í„° $d-1$ì°¨ ë¯¸ë¶„ í•¨ìˆ˜ê¹Œì§€ ëª¨ë‘ ì—°ì†ì´ë‹¤. knotì—ì„œ í•¨ìˆ˜ì˜ ê³„ìˆ˜ê°€ ë³€í•˜ë¯€ë¡œ knotê°€ ë§ì„ ìˆ˜ë¡ ë” ìœ ì—°í•˜ê²Œ ëœë‹¤. ì ê³¼ ì  ì‚¬ì´ë¥¼ ê·¸ì € ì—°ê²°í•˜ë©´ Linear Splineì´ ë˜ê¸° ë•Œë¬¸ì—, ì„ í˜• ìŠ¤í”Œë¼ì¸ì€ ì˜ í™œìš©í•˜ì§€ ì•ŠëŠ”ë‹¤.\n2ì°¨ Splineë¶€í„° ì•Œì•„ë³´ì.\nQuadratic Spline Interpolation\r#\rn+1ê°œì˜ ì ì„ ì—°ê²°í•˜ëŠ” nê°œì˜ 2ì°¨ ë‹¤í•­ì‹ì„ ì¶”ì •í•˜ê³ ì í•œë‹¤.\nê° 2ì°¨ ë‹¤í•­ì‹ë§ˆë‹¤ $ax^2 + bx + c$ì™€ ê°™ì´ 3ê°œì˜ ë¯¸ì§€ìˆ˜ê°€ ì¡´ì¬í•˜ê¸° ë•Œë¬¸ì—, ëª¨ë“  ë‹¤í•­ì‹ì„ ì¶”ì •í•˜ê¸° ìœ„í•´ì„œ $3n$ê°œì˜ ì¡°ê±´ì´ í•„ìš”í•˜ë‹¤.\nì´ëŸ¬í•œ $3n$ê°œì˜ ì¡°ê±´ì€ ì ì ˆí•œ ì œì•½ì„ ì¶”ê°€í•˜ì—¬ ì–»ì„ ìˆ˜ ìˆë‹¤.\nì²« ë²ˆì§¸ í•¨ìˆ˜ì™€ ë§¨ ë§ˆì§€ë§‰ í•¨ìˆ˜ëŠ” ê°ê° ì²« ë²ˆì§¸ ì ê³¼ ë§ˆì§€ë§‰ ì ì„ ì§€ë‚˜ì•¼ í•œë‹¤.\nì´ë¡œë¶€í„° 2ê°œì˜ ì¡°ê±´ì„ ì–»ì„ ìˆ˜ ìˆë‹¤.\n$\\begin{aligned}\u0026amp; f\\left(x_0\\right)=a_1 x_0^2+b_1 x_0+c_1 \\\u0026amp; f\\left(x_n\\right)=a_n x_n^2+b_n x_n+c_n\\end{aligned}$\nì´ì œ ë‚˜ë¨¸ì§€ nê°œì˜ ì¡°ê±´ì„ ì–»ìœ¼ë©´ ëœë‹¤.\nì–‘ ëì„ ì œì™¸í•œ n-1ê°œì˜ ì ì—ì„œ í•¨ìˆ˜ê°€ ì—°ì†í•´ì•¼ í•œë‹¤.\nì¦‰, ê° ë‚´ë¶€ì˜ ì ì—ì„œ nê°œì˜ í•¨ìˆ˜ëŠ” ì–‘ ë ì ì„ ì§€ë‚˜ì•¼ í•œë‹¤.\nì´ë¡œë¶€í„° $2n-2$ê°œì˜ ì¡°ê±´ì„ ì–»ì„ ìˆ˜ ìˆë‹¤.\n$i=2\\dots n$ì¼ ë•Œ, $f\\left(x_{i-1}\\right)=a_{i-1} x_{i-1}^2+b_{i-1} x_{i-1}+c_{i-1}:$ ì£¼ì–´ì§„ $i-1$ë²ˆì§¸ ë°ì´í„°ì˜ ì™¼ìª½ í•¨ìˆ˜ $f\\left(x_{i-1}\\right)=a_i x_{i-1}^2+b_i x_{i-1}+c_i:$ ì£¼ì–´ì§„ $i-1$ ë²ˆì§¸ ë°ì´í„°ì˜ ì˜¤ë¥¸ìª½ í•¨ìˆ˜\nëª¨ë“  ì ì—ì„œ í•¨ìˆ˜ê°€ ë§¤ë„ëŸ¬ì›Œì•¼ í•œë‹¤. ì¦‰, ëª¨ë“  knotsì—ì„œ ë¯¸ë¶„ ê°€ëŠ¥í•´ì•¼ í•œë‹¤.\n$i=2\\dots n$ì¼ ë•Œ, $f^{\\prime}\\left(x_{i-1}\\right)=2 a_{i-1} x_{i-1}+b_{i-1}$ : ì£¼ì–´ì§„ $i-1$ ë²ˆì§¸ ë°ì´í„°ì˜ ì™¼ìª½ 1ì°¨ ë„í•¨ìˆ˜ $f^{\\prime}\\left(x_{i-1}\\right)=2 a_i x_{i-1}+b_i$ : ì£¼ì–´ì§„ $i-1$ ë²ˆì§¸ ë°ì´í„°ì˜ ì™¼ìª½ 1ì°¨ ë„í•¨ìˆ˜\nì´ë¥¼ í†µí•´ $n-1$ê°œì˜ ì¡°ê±´ì„ ì–»ì„ ìˆ˜ ìˆë‹¤.\nì´ì œ ë‹¨ í•˜ë‚˜ì˜ ì¡°ê±´ë§Œ ìˆìœ¼ë©´ ëœë‹¤.\nì²« ë²ˆì§¸ í•¨ìˆ˜ì˜ ì´ê³„ ë„í•¨ìˆ˜ëŠ” 0ì´ë‹¤. ì¦‰, ì²« ë²ˆì§¸ í•¨ìˆ˜ëŠ” ì§ì„ ì´ë‹¤.\n$f_1\u0026rsquo;\u0026rsquo;(x_0) = a_1 = 0$\nì´ë ‡ê²Œ ì´ $3n$ê°œì˜ ì¡°ê±´ì„ ì–»ì—ˆìœ¼ë¯€ë¡œ, $n$ê°œì˜ 2ì°¨ ë‹¤í•­ì‹ì„ ì¶”ì •í•  ìˆ˜ ìˆë‹¤.\nCubic Spline Interpolation\r#\r2ì°¨ Spline ë³´ê°„ë²•ê³¼ ë§ˆì°¬ê°€ì§€ ì´ìœ ë¡œ ì´ë²ˆì—ëŠ” $4n$ê°œì˜ ì¡°ê±´ì´ í•„ìš”í•˜ë‹¤.\nì²« ë²ˆì§¸ì™€ ë§ˆì§€ë§‰ í•¨ìˆ˜ëŠ” ê° ì–‘ ë ì ì„ ì§€ë‚œë‹¤. â†’ $2$ ì—°ì† â†’ $2n - 2$ ë¯¸ë¶„ ê°€ëŠ¥(1ê³„ ë„í•¨ìˆ˜ ì—°ì†) â†’ $n-1$ 2ê³„ ë„í•¨ìˆ˜ ì—°ì† â†’ $n-1$ ì—¬ê¸°ê¹Œì§€ ê³„ì‚°í•˜ë©´ ì´ $4n - 2$ë¡œ 2ê°œì˜ ì¡°ê±´ì´ ë¶€ì¡±í•´ ìœ ì¼ í•´ë¥¼ êµ¬í•  ìˆ˜ ì—†ë‹¤.\në”°ë¼ì„œ ë‹¤ìŒê³¼ ê°™ì€ ì„ì˜ì˜ ì¡°ê±´ì„ ì¶”ê°€í•˜ì—¬ ìœ ì¼ í•´ë¥¼ ì±„ìš¸ ìˆ˜ ìˆë‹¤.\nì²« ë²ˆì§¸ í•¨ìˆ˜ì™€ ë§ˆì§€ë§‰ í•¨ìˆ˜ì˜ 2ê³„ ë„í•¨ìˆ˜ëŠ” 0ì´ì–´ì•¼ í•œë‹¤. â†’ $2$ ì´ëŸ¬í•œ ë‹¤ì„¯ ê°œì˜ ì¡°ê±´ìœ¼ë¡œ ë³´ê°„ëœ ê³¡ì„ ì„ Natural Cubic Splineì´ë¼ê³  í•œë‹¤.\n4ì°¨ ì´ìƒì˜ ê³ ì°¨ ìŠ¤í”Œë¼ì¸ì€ ë‚´ì¬ëœ ë¶ˆì•ˆì •ì„± ë•Œë¬¸ì— ì˜ ì‚¬ìš©í•˜ì§€ ì•Šê¸° ë•Œë¬¸ì—, Cubic Splineì„ ê°€ì¥ ë§ì´ í™œìš©í•œë‹¤.\nì°¸ê³  ìë£Œ\r#\ríšŒê·€ ìŠ¤í”Œë¼ì¸ (Regression Spline)ì— ëŒ€í•œ ì´í•´\n[interpolation] - Spline method\nSplines\nìŠ¤í”Œë¼ì¸ ë³´ê°„ë²• - ì ì„ ë¶€ë“œëŸ½ê²Œ ì‡ê¸°\n[ISL] 7ì¥ -ë¹„ì„ í˜•ëª¨ë¸(Local regression, Smoothing splines, GAM) ì´í•´í•˜ê¸° Â· Go\u0026rsquo;s BLOG\n"},{"id":27,"href":"/posts/2023-06-29-Pytorch-overview/","title":"PyTorch ê°œìš”","section":"Blog","content":" Naver BoostCamp AI Techì—ì„œ í•™ìŠµí•œ ë‚´ìš©ì„ ì¬êµ¬ì„±í–ˆìŠµë‹ˆë‹¤.\ní•´ë‹¹ ê²Œì‹œê¸€ì€ ì§€ì†ì ìœ¼ë¡œ ì—…ë°ì´íŠ¸í•  ì˜ˆì •ì…ë‹ˆë‹¤.\n{: .prompt-info }\nêµ¬í˜„ ê°œìš”(PyTorch)\r#\r1. ë°ì´í„° ì¤€ë¹„\r#\rTensor PyTorch Datasets \u0026amp; DataLoaders 2. ëª¨ë¸ ì •ì˜ (\rtorch.nn.Module)\r#\rPyTorch ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°\nInput size, Output size ì •ì˜ nn.Parameter Forward ì—°ì‚° ì •ì˜ Backward ì—°ì‚° ì •ì˜ 3. í•˜ì´í¼ íŒŒë¼ë¯¸í„° ì§€ì • Hyperparameter Tuning\r#\r4. ëª¨ë¸ í‰ê°€ ê¸°ì¤€ ë° Optimizer ì„¤ì •\r#\rëª¨ë¸ í‰ê°€ ê¸°ì¤€ : lossë¥¼ ì–´ë–»ê²Œ ê³„ì‚°í•  ê²ƒì¸ê°€? ì†ì‹¤ í•¨ìˆ˜(Loss Function) Optimizer ì„¤ì • 5. ëª¨ë¸ í•™ìŠµ\r#\r1 epochì— ì¼ì–´ë‚˜ëŠ” ì¼ PyTorch Multi-GPU í•™ìŠµ 6. ëª¨ë¸ ì„±ëŠ¥ í‰ê°€\r#\rMonitoring tools for PyTorch\n7. ì¶”ë¡ \r#\rAutoGrad íŠœí† ë¦¬ì–¼ Tensorì™€ AutoGrad íŠœí† ë¦¬ì–¼ Pytorchë¡œ Linear Regressioní•˜ê¸° Pytorchë¡œ Logistic Regressioní•˜ê¸° í•œ epochì—ì„œ ì´ë¤„ì§€ëŠ” ëª¨ë¸ í•™ìŠµ ê³¼ì •ì„ ì •ë¦¬í•´ë³´ê³  ì„±ëŠ¥ì„ ì˜¬ë¦¬ê¸° ìœ„í•´ì„œ ì–´ë–¤ ë¶€ë¶„ì„ ë¨¼ì € ê³ ë ¤í•˜ë©´ ì¢‹ì„ì§€ ë…¼ì˜í•´ë³´ê¸° ë°ì´í„° ê°œì„  ëª¨ë¸ ê°œì„  loss ê°œì„  optimizer ê°œì„  DL ëª¨ë¸ êµ¬í˜„ ì˜ˆì œ\nPyTorch Troubleshooting\n"},{"id":28,"href":"/posts/2023-09-21-PyTorch-%EB%AA%A8%EB%8D%B8-%EB%B6%88%EB%9F%AC%EC%98%A4%EA%B8%B0/","title":"PyTorch ëª¨ë¸ ì €ì¥í•˜ê³  ë¶ˆëŸ¬ì˜¤ê¸°","section":"Blog","content":"\rmodel.save()\r#\rí•™ìŠµì˜ ê²°ê³¼ë¥¼ ì €ì¥í•˜ê¸° ìœ„í•œ í•¨ìˆ˜\nëª¨ë¸ í˜•íƒœ(architecture)ì™€ íŒŒë¼ë¯¸í„°ë¥¼ ì €ì¥\nëª¨ë¸ í•™ìŠµ ì¤‘ê°„ ê³¼ì •ì˜ ì €ì¥ì„ í†µí•´ ìµœì„ ì˜ ê²°ê³¼ ëª¨ë¸ì„ ì„ íƒ\në§Œë“¤ì–´ì§„ ëª¨ë¸ì„ ì™¸ë¶€ ì—°êµ¬ìì™€ ê³µìœ í•˜ì—¬ í•™ìŠµ ì¬ì—°ì„± í–¥ìƒ\n# Print model\u0026#39;s state_dict print(\u0026#34;Model\u0026#39;s state_dict:\u0026#34;) # state dict: ëª¨ë¸ì˜ íŒŒë¼ë¯¸í„°ë¥¼ í‘œì‹œ for param_tensor in model.state_dict(): print(param_tensor, \u0026#34;\\t\u0026#34;, model.state_dict()[param_tensor].size()) ## ë°©ë²• 1. # ëª¨ë¸ì˜ íŒŒë¼ë¯¸í„°ë§Œ ì €ì¥í•˜ê¸° **torch.save**(model.**state_dict()**, os.path.join(MODEL_PATH, \u0026#34;model.pt\u0026#34;)) # ëª¨ë¸ì€.pt íŒŒì¼ë¡œ ì €ì¥í•œë‹¤. # dict typeìœ¼ë¡œ ì €ì¥ëœë‹¤. new_model = TheModelClass() # ëª¨ë¸ì˜ Architectureê°€ ë™ì¼í•œ ê²½ìš° íŒŒë¼ë¯¸í„°ë§Œ ì €ì¥í•˜ê³  ë¶ˆëŸ¬ì˜¨ë‹¤. new_model.**load_state_dict**(torch.load(os.path.join(MODEL_PATH, \u0026#34;model.pt\u0026#34;))) ## ë°©ë²• 2. # ëª¨ë¸ì˜ architectureì™€ í•¨ê»˜ ì €ì¥í•œë‹¤. torch.**save**(model, os.path.join(MODEL_PATH, \u0026#34;model.pt\u0026#34;)) model = torch.**load**(os.path.join(MODEL_PATH, \u0026#34;model.pt\u0026#34;)) # ëª¨ë¸ì˜ architectureì™€ í•¨ê»˜ loadí•œë‹¤. # ì‚¬ì‹¤ ëª¨ë¸ ìì²´ë¥¼ ê³µìœ í•˜ëŠ” ê²½ìš° ì½”ë“œë¡œ ê³µìœ í•˜ëŠ” ë°©ë²•ì´ ìˆê¸° ë•Œë¬¸ì—, # ì¼ë°˜ì ìœ¼ë¡œ ìœ„ì˜ ë°©ì‹ì´ ë” ë§ì´ ì“°ì¸ë‹¤. checkpoints\r#\rí•™ìŠµì˜ ì¤‘ê°„ ê²°ê³¼ë¥¼ ì €ì¥í•˜ì—¬ ìµœì„ ì˜ ê²°ê³¼ë¥¼ ì„ íƒ\nearlystopping ê¸°ë²• ì‚¬ìš©ì‹œ ì´ì „ í•™ìŠµì˜ ê²°ê³¼ë¬¼ì„ ì €ì¥í•œë‹¤.\nlossì™€ metric ê°’ì„ ì§€ì†ì ìœ¼ë¡œ í™•ì¸ ì €ì¥\nì¼ë°˜ì ìœ¼ë¡œ, epoch, loss, metricì„ í•¨ê»˜ ì €ì¥í•˜ì—¬ í™•ì¸í•œë‹¤.\ncolabì—ì„œ ì§€ì†ì ì¸ í•™ìŠµì„ ìœ„í•´ì„œëŠ” ë°˜ë“œì‹œ í•„ìš”í•˜ë‹¤.\ntorch.save({ # ëª¨ë¸ì˜ ì •ë³´ëŠ” epochì™€ í•¨ê»˜ ì €ì¥ \u0026#39;epoch\u0026#39;: e, \u0026#39;model_state_dict\u0026#39;: model.state_dict(), \u0026#39;optimizer_state_dict\u0026#39;: optimizer.state_dict(), \u0026#39;loss\u0026#39;: epoch_loss, }, f\u0026#34;saved/checkpoint_model_{e}*{epoch_loss/len(dataloader)}*{epoch_acc/len(dataloader)}.pt\u0026#34;) checkpoint = torch.load(PATH) model.load_state_dict(checkpoint[\u0026#39;model_state_dict\u0026#39;]) optimizer.load_state_dict(checkpoint[\u0026#39;optimizer_state_dict\u0026#39;]) epoch = checkpoint[\u0026#39;epoch\u0026#39;] loss = checkpoint[\u0026#39;loss\u0026#39;] Pretrained Learning\r#\rTransfer Learning\nComputer Vision ëª¨ë¸ ë ˆí¬ì§€í† ë¦¬ Segmentation ëª¨ë¸ ë ˆí¬ì§€í† ë¦¬ Transfer Learning vs Fine-tuning ê´€ë ¨ ë‚´ìš© Andrew Ng êµìˆ˜ë‹˜ì˜ Transfer Learning ì˜ë¬¸ê°•ì˜ (YouTube) "},{"id":29,"href":"/posts/2023-09-21-PyTorch-%EB%AA%A8%EB%8D%B8-%EC%A0%95%EC%9D%98%ED%95%98%EA%B8%B0/","title":"PyTorch ëª¨ë¸ ì •ì˜í•˜ê¸° - nn.Module","section":"Blog","content":" ê°„ë‹¨ ìš”ì•½\në°˜ë³µë˜ëŠ” Layerì„ ë§Œë“¤ê¸° ìœ„í•œ Torchì˜ ê°€ì¥ ê¸°ë³¸ì ì¸ ì‹ ê²½ë§ ëª¨ë“ˆ\në§¤ê°œë³€ìˆ˜ë¥¼ ìº¡ìŠí™”í•˜ëŠ” ê°„í¸í•œ ë°©ë²•\nGPUë¡œ ì´ë™, ë‚´ë³´ë‚´ê¸°(exporting), ë¶ˆëŸ¬ì˜¤ê¸°(loading) ë“±ì˜ ì‘ì—…ì„ ìœ„í•œ í—¬í¼(helper)ë¥¼ ì œê³µí•œë‹¤.\n{: .prompt-info }\nDL ëª¨ë¸ì€ ëª¨ë‘ Layerì˜ ë°˜ë³µì´ë©°, ë¸”ë¡ ë°˜ë³µì˜ ì—°ì†ì´ë‹¤.\nModuleì—ì„œ ì •ì˜í•˜ëŠ” ê²ƒ\nInput\nOutput\nForward\n(Backward)\nì´ ë•Œ, BackwardëŠ” ìë™ ë¯¸ë¶„ì´ ë˜ê¸° ë•Œë¬¸ì—, í•´ë‹¹ë˜ëŠ” weightì˜ ê°’ë“¤ì„ ë‚´ë³´ë‚´ì¤€ë‹¤.\nì¦‰, weightê°€ í•™ìŠµì˜ ëŒ€ìƒì´ ë˜ê³ , ì´ë¥¼ parameter(tensor)ë¡œ ì •ì˜í•œë‹¤.\nì¼ë°˜ì ìœ¼ë¡œëŠ” ì§ì ‘ ì§€ì •í•´ì¤„ í•„ìš”ê°€ ì—†ë‹¤.\nexample\r#\rimport torch from torch.autograd import Variable class LinearRegression(torch.nn.Module): def __init__(self, inputSize, outputSize): super(LinearRegression, self).__init__() # pytorchì—ì„œ ì œê³µí•˜ëŠ” xw + b ëª¨ë“ˆ self.linear = torch.nn.Linear(inputSize, outputSize) def forward(self, x): out = self.linear(x) return out "},{"id":30,"href":"/posts/2023-09-21-PyTorch-nn-Parameter/","title":"PyTorchì—ì„œ weightë¥¼ ì €ì¥í•˜ëŠ” ê°ì²´ - nn.Parameter","section":"Blog","content":" ê°„ë‹¨ ìš”ì•½\ní•™ìŠµì˜ ëŒ€ìƒì´ ë˜ëŠ” Weightë¥¼ ì •ì˜í•œë‹¤.\nTensor ê°ì²´ì˜ ìƒì† ê°ì²´ {: .prompt-info }\nTensor ê°ì²´ì™€ ë§¤ìš° ë¹„ìŠ·í•˜ë‹¤.\nnn.Moduleì˜ attributeê°€ ë  ë•ŒëŠ” required_grad = Trueë¡œ ìë™ìœ¼ë¡œ ì§€ì •ë˜ì–´ AutoGradì˜ ëŒ€ìƒì´ ëœë‹¤.\nëŒ€ë¶€ë¶„ì˜ Layerì—ëŠ” weights ê°’ë“¤ì´ ì§€ì •ë˜ì–´ ìˆê¸° ë•Œë¬¸ì—, ì§ì ‘ ì§€ì •í•  ì¼ì€ ë“œë¬¼ë‹¤. ê·¸ë˜ë„ ì§ì ‘ ì§€ì •í•˜ëŠ” ë²•ì„ ì•Œì•„ë³´ì.\nnn.Moduleë¡œ ë§Œë“  $\\tt xw +b$ë¼ëŠ” ì„ í˜• ëª¨ë¸ì„ ì‚´í´ë³¸ë‹¤.\n$\\tt xw +b$\r#\rclass MyLinear(nn.Module): def __init__ (self, in_features, out_features, bias=True): super(). init () **self.in_features = in_features self.out_features = out_features** **self.weights = nn.Parameter(torch.randn(in_features, out_features)) self.bias = nn.Parameter(torch.randn(out_features))** def forward(self, x : Tensor): return x @ self.weights + self.bias # xw + bì˜ í˜•íƒœë¡œ outputì´ ë‚˜ì˜¨ë‹¤. ex â€” Featureê°€ 7ê°œ ìˆê³ , ë°°ì¹˜ê°€ 3ì¸ ê²½ìš°\në°ì´í„°ëŠ” $3 \\times 7$ì˜ í˜•íƒœê°€ ëœë‹¤.\n7ê°œì˜ featureë¥¼ ë„£ì–´ì„œ\nin_features = 7\n5ê°œì˜ í´ë˜ìŠ¤ ì¤‘ í•˜ë‚˜ë¡œ ì˜ˆì¸¡í•˜ê³ ì í•œë‹¤ë©´\nout_features = 5\n$7 \\times 5$ í˜•íƒœì˜ weight ê°’ì´ í•„ìš”í•˜ë‹¤.\nnn.Parameter(torch.randn(7, 5)\nì´í›„, bias ê°’ë„ ì„ ì–¸í•´ì¤€ë‹¤.\nnn.Parameter(torch.randn(5))\nì´í›„, forwardì—ì„œ $\\tt xw +b$ì„ returní•´ì¤€ë‹¤.\ndef forward(self, x : Tensor):\rreturn x @ self.weights + self.bias\nì¦‰, ëª¨ë¸ì˜ ì˜ˆì¸¡ ê°’($\\hat y$)ì„ ë±‰ì–´ë‚¸ë‹¤.\nì´í›„, backward()ì—ì„œ ì‹¤ì œ ê°’ê³¼ ì˜ˆì¸¡ ê°’ì˜ ì°¨ì´(loss)ì— ëŒ€í•´ ë¯¸ë¶„ì„ ìˆ˜í–‰í•œë‹¤.\n"},{"id":31,"href":"/posts/2023-06-13-Backward/","title":"PyTorchì˜ Backwardì— ëŒ€í•´ ì•Œì•„ë³´ì.","section":"Blog","content":" âœ”ï¸ ê°„ë‹¨ ìš”ì•½\nforwardí•¨ìˆ˜ë¥¼ ì •ì˜í•˜ë©´ ìë™ìœ¼ë¡œ ì •ì˜ëœë‹¤.\në™ì‘ ê³¼ì •\ntensor(lossì— í•´ë‹¹)ê°€ í¬í•¨ëœ ì‹ì„ ë¯¸ë¶„í•œë‹¤. ë¯¸ë¶„ ê°’ì„ tensorì— ì €ì¥í•œë‹¤.\n{: .prompt-info } Autograd, loss, optimizer, nn.Module tensor(lossì— í•´ë‹¹)ê°€ í¬í•¨ëœ ì‹ì„ ë¯¸ë¶„í•œë‹¤.\nTensor ê°ì²´ì˜ backward í•¨ìˆ˜ì—ëŠ” defaultë¡œ Autograd ì„¤ì •ì´ ë˜ì–´ ìˆê¸° ë•Œë¬¸ì—, ë¯¸ë¶„ ìˆ˜ì‹ì„ ë”°ë¡œ ì‘ì„±í•˜ì§€ ì•Šì•„ë„ ìë™ìœ¼ë¡œ ë¯¸ë¶„ì´ ê°€ëŠ¥í•˜ë‹¤.\nê¸°ë³¸ ì˜ˆì œ\nw = torch.tensor(2.0, requires_grad=True) y = w**2 z = 10*y + 25 z.backward() w.grad() # output : tensor(40.) $$ w = 2\\ y = w^2\\ z = 10\\times y + 25\\ z = 10 \\times w^2 + 25\\ {dz\\over dw} = 20 \\times w = 40 $$\në¯¸ë¶„ ê°’ì´ ì—¬ëŸ¬ ê°œì¸ ê²½ìš°\na = torch.tensor([2.,3.], requires_grad=True) b = torch.tensor([6.,4.], requires_grad=True) Q = 3 * a ** 3 - b ** 2 # ë¯¸ë¶„ê°’ì´ ë‘ ê°œê°€ ë‚˜ì™€ì•¼í•˜ê¸° ë•Œë¬¸ì—, # gradient ê°’ì˜ í¬ê¸°ë¥¼ ì¡ì•„ì¤€ë‹¤. external_grad = torch.tensor([1., 1.]) Q.backward(gradient=external_grad) a.grad b.grad # output: tensor([36.,81.]) # output: tensor([-12.,-8.]) $$ \\begin{aligned} \u0026amp;Q = 3a^3 - b^2\\ \u0026amp; \\frac{\\partial Q}{\\partial a}=9 a^2 \\ \u0026amp; \\frac{\\partial Q}{\\partial b}=-2 b \\end{aligned} $$\në¯¸ë¶„ ê°’ì„ tensorì— ì €ì¥í•œë‹¤.\ntensorë¥¼ ì„ ì–¸í•  ë•Œ require_grad=Trueë¡œ ì„¤ì •í•˜ë©´ tensorì— grad_fn ì •ë³´ê°€ ì €ì¥ëœë‹¤.\ne.g. tensor(6.2564e-05, grad_fn=\u0026lt;MseLossBackward0\u0026gt;)\në˜í•œ, tensor.grad()ë¥¼ í†µí•´ ë¯¸ë¶„ ê°’ì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤.\ngrad_fnì—ëŠ” í…ì„œê°€ ì–´ë–¤ ì—°ì‚°ì„ í–ˆëŠ” ì—°ì‚° ì •ë³´ë¥¼ ë‹´ê³  ìˆê³ , ì´ ì •ë³´ëŠ” ì—­ì „íŒŒì— ì‚¬ìš©ëœë‹¤.\nPyTorch Gradient ê´€ë ¨ ì„¤ëª… (Autograd)\nì‹¤ì œ backwardëŠ” Module ë‹¨ê³„ì—ì„œ ì§ì ‘ ì§€ì •ì´ ê°€ëŠ¥í•˜ë‹¤.\nModuleì—ì„œ backwardì™€ optimizerë¥¼ ì˜¤ë²„ë¼ì´ë”© í•´ì£¼ë©´ ëœë‹¤.\nì‚¬ìš©ìê°€ ì§ì ‘ ë¯¸ë¶„ ìˆ˜ì‹ì„ ì¨ì•¼ í•˜ëŠ” ë¶€ë‹´ì´ ìˆë‹¤.\nì“¸ ì¼ì€ ì—†ì§€ë§Œ, ìˆœì„œë¥¼ ì´í•´í•  í•„ìš”ëŠ” ìˆë‹¤.\nì˜ˆì œ â€” Logistic Regression\r#\rclass LR(nn.Module): def init (self, dim, lr=torch.scalar_tensor(0.01)): super(LR, self). init () # intialize parameters self.w = torch.zeros(dim, 1, dtype=torch.float).to(device) self.b = torch.scalar_tensor(0).to(device) self.grads = {\u0026#34;dw\u0026#34;: torch.zeros(dim, 1, dtype=torch.float).to(device), \u0026#34;db\u0026#34;: torch.scalar_tensor(0).to(device)} self.lr = lr.to(device) $$ h_\\theta(x)=\\frac{1}{1+e^{-\\theta^T \\mathbf{x}}} $$\ndef forward(self, x): ## compute forward z = torch.mm(self.w.T, x) a = self.sigmoid(z) return a def sigmoid(self, z): return 1/ (1 + torch.exp(-z)) def backward(self, x, yhat, y): ## compute backward self.grads[\u0026#34;dw\u0026#34;] = (1/x.shape[1]) * torch.mm(x, (yhat - y).T) self.grads[\u0026#34;db\u0026#34;] = (1/x.shape[1]) * torch.sum(yhat - y) $$ \\begin{aligned}\u0026amp;\\frac{\\partial}{\\partial \\theta_j} J(\\theta)=\\frac{1}{m} \\sum_{i=1}^m\\left(h_\\theta\\left(x^i\\right)-y^i\\right) x_j^i\\end{aligned} $$\ndef optimize(self): ## optimization step self.w = self.w - self.lr * self.grads[\u0026#34;dw\u0026#34;] self.b = self.b - self.lr * self.grads[\u0026#34;db\u0026#34;] ê¸°ì¡´ì˜ $\\theta$, ì¦‰, $\\tt w$ ê°’ì— ë¯¸ë¶„ê°’ ë§Œí¼ì˜ ì—…ë°ì´íŠ¸ë¥¼ ìˆ˜í–‰í•´ì£¼ëŠ” í•¨ìˆ˜.\n$$ \\begin{aligned}\\theta_j \u0026amp; :=\\theta_j-\\alpha \\frac{\\partial}{\\partial \\theta_j} J(\\theta) \\\u0026amp; :=\\theta_j-\\alpha \\sum_{i=1}^m\\left(h_\\theta\\left(x^i\\right)-y^i\\right) x_j^i\\end{aligned} $$\nì „ì²´ ì½”ë“œ\nclass LR(nn.Module): def init (self, dim, lr=torch.scalar_tensor(0.01)): super(LR, self). init () # intialize parameters self.w = torch.zeros(dim, 1, dtype=torch.float).to(device) self.b = torch.scalar_tensor(0).to(device) self.grads = {\u0026#34;dw\u0026#34;: torch.zeros(dim, 1, dtype=torch.float).to(device), \u0026#34;db\u0026#34;: torch.scalar_tensor(0).to(device)} self.lr = lr.to(device) def forward(self, x): ## compute forward z = torch.mm(self.w.T, x) a = self.sigmoid(z) return a def sigmoid(self, z): return 1/ (1 + torch.exp(-z)) def backward(self, x, yhat, y): ## compute backward self.grads[\u0026#34;dw\u0026#34;] = (1/x.shape[1]) * torch.mm(x, (yhat - y).T) self.grads[\u0026#34;db\u0026#34;] = (1/x.shape[1]) * torch.sum(yhat - y) def optimize(self): ## optimization step self.w = self.w - self.lr * self.grads[\u0026#34;dw\u0026#34;] self.b = self.b - self.lr * self.grads[\u0026#34;db\u0026#34;] "},{"id":32,"href":"/posts/2023-11-16-TF-IDF-Implement/","title":"Ranked Retreival ëª¨ë¸ êµ¬í˜„(TF-IDF)","section":"Blog","content":"ìš°ì„ , ì‹œì‘í•˜ê¸°ì— ì•ì„œ corpus êµ¬ì„±ì„ í™•ì¸í–ˆë‹¤.\ní•œê¸€ë¡œ ì‘ì„±ë˜ì—ˆìœ¼ë©°, ì œëª©ê³¼ ë¬¸ì„œ ë‚´ìš©ìœ¼ë¡œ êµ¬ì„±ë˜ì–´ìˆëŠ” ê²ƒì„ í™•ì¸í–ˆë‹¤.\nì¼ë‹¨ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•˜ê¸° ìœ„í•´ ì½”ë“œì—ì„œ íŒŒì¼ì„ ì—´ì–´ì•¼ í•˜ëŠ”ë°, ë¬¸ì„œê°€ í•œê¸€íŒŒì¼ë¡œ ì œê³µë˜ì—ˆê¸° ë•Œë¬¸ì— í•œê¸€ ë¬¸ì„œë¥¼ txt ë¬¸ì„œë¡œ ë³€í™˜í•´ì£¼ì—ˆë‹¤.\n{: w=\u0026ldquo;200\u0026rdquo; h=\u0026ldquo;100\u0026rdquo; }\nì–¸ì–´ëŠ” íŒŒì´ì¬ì„ ì„ íƒí–ˆë‹¤.\nì²˜ìŒ í”„ë¡œì íŠ¸ë¥¼ ì‹œì‘í–ˆì„ ë•Œ, ë‚˜ëŠ” colab í™˜ê²½ì—ì„œ íŒŒì´ì¬ ì½”ë“œë¥¼ ì‹¤í–‰ì‹œí‚¤ê³ ì í–ˆìœ¼ë¯€ë¡œ Google Driveì— corpus íŒŒì¼ì„ ì—…ë¡œë“œí•˜ê³ , ì½”ë“œ ì‘ì„±ì„ ì‹œì‘í–ˆë‹¤.\nìˆ˜ì—… ë‚´ìš©ì—ì„œëŠ” ì˜ì–´ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë‹¤ë¤„ì™”ì—ˆëŠ”ë° í•œê¸€ì„ í† í°í™”í•˜ëŠ” ë°©ë²•ì´ ë– ì˜¤ë¥´ì§€ ì•Šì•˜ë‹¤.\në‹¤í–‰ìŠ¤ëŸ½ê²Œë„ í•œê¸€ í˜•íƒœì†Œ ë¶„ì„ ë¼ì´ë¸ŒëŸ¬ë¦¬ Konlpyê°€ ìˆì–´ì„œ ì´ë¥¼ í™œìš©í•˜ê³ ì í–ˆë‹¤.\ní•˜ì§€ë§Œ í•´ë‹¹ Konlpy ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ìë°”ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì§œì—¬ì¡Œê³ , íŒŒì´ì¬ìœ¼ë¡œ wrappingí•œ ì±„ë¡œ ì‚¬ìš©í•˜ëŠ” ë°©ì‹ì´ì—ˆë‹¤.\ní•´ë‹¹ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•˜ê¸° ìœ„í•´ì„œëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬ì—ì„œ ì§€ì›í•˜ëŠ” íŒŒì´ì¬ ë²„ì „ê³¼ ìë°” ë²„ì „ì´ ì¼ì¹˜í•´ì•¼ í–ˆë‹¤.\nì½”ë© í™˜ê²½ì—ì„œ í•´ë‹¹ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•˜ê¸° ìœ„í•´ ìˆ˜ë§ì€ ë¸”ë¡œê·¸ë¥¼ íƒìƒ‰í•˜ê³ , ê¹ƒí—ˆë¸Œ ë¼ì´ë¸ŒëŸ¬ë¦¬ í˜ì´ì§€ë¥¼ ë°©ë¬¸í•˜ì—¬ issueë¥¼ ì‚´í´ë´¤ì—ˆì§€ë§Œ í•´ë‹¹ ë¬¸ì œëŠ” ì‰½ì‚¬ë¦¬ í•´ê²°ë˜ì§€ ì•Šì•˜ë‹¤.\nê·¸ë˜ì„œ ê±°ì˜ í¬ê¸°í•˜ê³  ì§ì ‘ ë¬¸ì„œì˜ ì¡°ì‚¬ë¥¼ ì œê±°í•˜ì—¬ í™œìš©í•˜ê³ ì ë§ˆìŒ ë¨¹ê³  ì‘ì—…ì„ ìˆ˜í–‰í–ˆë‹¤.\ní•˜ì§€ë§Œ ê²°êµ­ ì´ ê³¼ì •ì€ í•™ìŠµì— ë„ì›€ì´ ë˜ì§€ ì•ŠëŠ”ë‹¤ê³  íŒë‹¨í•´ colab í™˜ê²½ì„ pycharm í™˜ê²½ìœ¼ë¡œ ë°”ê¿”ì„œ ë‹¤ì‹œ í•œë²ˆ í•´ë³´ìê³  ë§ˆìŒë¨¹ì—ˆê³ , ì‹œí–‰ì°©ì˜¤ ëì— ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆê²Œ ë˜ì—ˆë‹¤.\në¼ì´ë¸ŒëŸ¬ë¦¬ í™œìš©ì„ í¬ê¸°í•˜ê³  ì§ì ‘ ë¬¸ì„œì˜ ë‹¨ì–´ë¥¼ ì¶”ì¶œí•˜ê³  ìˆì—ˆë‹¤.\nGoogle Colaboratory\ní•œê¸€ ë‹¨ì–´ë¥¼ ì§ì ‘ ì¶”ì¶œí•˜ë˜ ì½”ë“œë‹¤. ë‚˜ì˜ ê³ í†µì˜ í”ì ì´ ë³´ì¸ë‹¤.\në¼ì´ë¸ŒëŸ¬ë¦¬ ì‹¤í–‰ì„ ì„±ê³µì‹œí‚¨ ë§ˆì§€ë§‰ ëª…ë ¹ì–´.\nJPype1.1.2ì™€ JPype3-1, Python 3.6, Python 3.7, Python 3.10ì˜ ì¡°í•©ì„ í™œìš©í•´ë³´ì•˜ê³ , ì‹¤í–‰ë˜ì§€ ì•Šì•˜ë‹¤.\ní•˜ì§€ë§Œ ê²°êµ­ ìœ„ ì‚¬ì§„ì—ì„œ ë³´ì´ë“¯ì´, JPype1ì˜ 1.4.0 ë²„ì „ê³¼ python 3.9.5 ë²„ì „ì„ í™œìš©í•˜ì—¬ ì„±ê³µí–ˆë‹¤.\nì•„ë˜ëŠ” ë‚´ê°€ ë¬¸ì œë¥¼ í•´ê²°í•˜ëŠ” ë° ë„ì›€ì„ ë°›ì€ ê¸€ì´ë‹¤.\nhttps://blog.naver.com/myincizor/221624979283\nhttps://ingu627.github.io/tips/install_konlpy/\nê²€ìƒ‰ì—”ì§„ ì½”ë“œ ì‘ì„±\nì´í›„ ê²€ìƒ‰ì—”ì§„ì˜ ì½”ë“œë¥¼ ì‘ì„±í•˜ëŠ” ê²ƒì€ ì–´ë µì§€ ì•Šì•˜ëŠ”ë°, ë‹¤ë§Œ ê°•ì˜ ë‚´ìš©ì„ ì™„ë²½íˆ ì´í•´í•˜ê³  ìˆì–´ì•¼ ì½”ë“œë¥¼ ìˆ˜ì›”í•˜ê²Œ ì§¤ ìˆ˜ ìˆëŠ” ê²ƒ ê°™ë‹¤.\nìˆ˜ì—…ì—ì„œ ë°°ìš´ ëŒ€ë¡œ tf-idf Weightingì„ ê¸°ì¤€ìœ¼ë¡œ ë¬¸ì„œì˜ ìˆœì„œë¥¼ ë§¤ê¸°ê³ , ì¿¼ë¦¬ë¥¼ ì…ë ¥ë°›ì•˜ì„ ë•Œ Scoreê°€ ë†’ì€ ìˆœì„œëŒ€ë¡œ ê²°ê³¼ë¥¼ ë³´ì—¬ì£¼ê³ ì í–ˆë‹¤.\nì´ë¥¼ êµ¬í˜„ ìˆœì„œëŒ€ë¡œ ë‚˜ì—´í•˜ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.\n1. corpus íŒŒì¼ ì½ê¸°: open()í•¨ìˆ˜ í™œìš©\r#\rfile = open(\u0026#34;corpus.txt\u0026#34;, \u0026#39;r\u0026#39;, encoding=\u0026#39;UTF8\u0026#39;) corpus = file.readlines() corpus = [line[:-1] for line in corpus if line != \u0026#34;\\n\u0026#34;] corpus 2. corpusë¥¼ dictionaryì— {key: ì œëª©, value: ë¬¸ì„œë‚´ìš© }ìœ¼ë¡œ ì €ì¥í•˜ê¸°.\r#\rí•´ë‹¹ ë¶€ë¶„ì€ ë¬¸ìì—´ íŒ¨í„´ì„ ì¶”ì¶œí•˜ê³  ê²€ì‚¬í•˜ëŠ” re ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ í™œìš©í•˜ì—¬ ë¹„êµì  ì‰½ê²Œ ìˆ˜í–‰í•  ìˆ˜ ìˆë‹¤.\ní•´ë‹¹ ì§€ì ì„ ìˆ˜í–‰í•˜ë©° ê¸°íƒ€ ì˜¤íƒˆìì˜ ì¡´ì¬ë¥¼ íŒŒì•…í–ˆìœ¼ë©° (ex: \u0026lt;title\u0026gt;~~~\u0026lt;title\u0026gt;)\níŠ¹ì´ ì‚¬í•­ìœ¼ë¡œ í•˜ë‚˜ì˜ ì œëª©ìœ¼ë¡œ ë‘ ê°œì˜ ë¬¸ì„œ ë‚´ìš©ì„ ê°€ì§„ ê²½ìš°ê°€ ìˆì—ˆë‹¤.\ní•´ë‹¹ ë¶€ë¶„ì„ ì²˜ë¦¬í•´ì£¼ê¸° ìœ„í•´ dictionaryì—ì„œ valueë¥¼ ë°”ë¡œ ë¬¸ì„œ ë‚´ìš©ìœ¼ë¡œ í•˜ì§€ ì•Šê³ , ë¦¬ìŠ¤íŠ¸ë¡œ ë§Œë“¤ì–´ì„œ extend()í•¨ìˆ˜ë¥¼ í†µí•´ ë‘ ë¬¸ì„œ ë‚´ìš©ì„ ì´ì–´ ë¶™ì˜€ë‹¤.\nfor line in corpus: if re.match(\u0026#34;\u0026lt;title\u0026gt;\u0026#34;, line): line = re.sub(\u0026#39;\u0026lt;title\u0026gt;\\d+. |\u0026lt;/title\u0026gt;|\u0026lt;title\u0026gt;|\\d+.\u0026#39;, \u0026#39;\u0026#39;, line) # line = line.replace(\u0026#39;\u0026lt;title\u0026gt;\u0026#39;, \u0026#39;\u0026#39;).replace(\u0026#39;\u0026lt;/title\u0026gt;\u0026#39;,\u0026#39;\u0026#39;) key = line # print(key) dic[key] = [] # valueì— í•´ë‹¹í•˜ëŠ” ë¦¬ìŠ¤íŠ¸ë¥¼ ë§Œë“¤ì–´ì£¼ê¸°. continue else: line = re.sub(\u0026#39;\\xa0\u0026#39;, \u0026#39; \u0026#39;, line) # print(line) dic[key].append(line) # value ë¦¬ìŠ¤íŠ¸ì— ë¬¸ì„œ ë‚´ìš© ë‹´ê¸°. for doc in dic.values(): if len(doc) != 1: doc[0] += doc[1] doc.pop() 3. ë¬¸ì„œë³„ term frequency êµ¬í•˜ê¸°\r#\rë¬¸ì„œì— í¬í•¨ë˜ëŠ” ë‹¨ì–´ë“¤ì˜ term frequencyë¥¼ êµ¬í•´ì•¼ í–ˆë‹¤.\nìš°ì„  ë‹¨ì–´ì˜ ë¹ˆë„ëŠ” okt ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ í™œìš©í•´ì„œ ì‰½ê²Œ êµ¬í•  ìˆ˜ ìˆì—ˆë‹¤.\nfor doc in dic.values(): doc_term = dict(Counter(okt.nouns(doc[0]))) ì´í›„, í•´ë‹¹ raw term frequencyë“¤ì„ log frequency weightingìœ¼ë¡œ ë°”ê¿”ì£¼ê¸° ìœ„í•´ í•¨ìˆ˜ë¥¼ ì„ ì–¸í–ˆë‹¤.\n$$ w_{t, d}=\\left{\\begin{array}{cc}1+\\log {10} \\mathrm{tf}{t, d}, \u0026amp; \\text { if } \\mathrm{tf}_{t, d}\u0026gt;0 \\0, \u0026amp; \\text { otherwise }\\end{array}\\right. $$\në‹¨ì–´ì˜ ë¹ˆë„ê°€ í¬ì§€ ì•Šê¸° ë•Œë¬¸ì—, ë‚˜ëŠ” logì˜ baseë¥¼ 2ë¡œ ì„¤ì •í–ˆë‹¤.\ndef lf_weighting(x): if x: return round(1 + math.log(x, **2**), 4) # ì†Œìˆ˜ì  ë„·ì§¸ì—ì„œ ë°˜ì˜¬ë¦¼ì„ ìˆ˜í–‰í–ˆë‹¤. else: return 0 ì´í›„ map í•¨ìˆ˜ë¥¼ í™œìš©í•˜ì—¬ ëª¨ë“  tf ê°’ë“¤ì„ lf_weighting ê°’ìœ¼ë¡œ ë³€ê²½í•´ì£¼ì—ˆë‹¤.\nfor doc in dic.values(): doc_term = dict(Counter(okt.nouns(doc[0]))) tf_raw = list(doc_term.values()) tf = list(map(lf_weighting, tf_raw)) 4. ë‹¨ì–´ì˜ idf êµ¬í•˜ê¸°\r#\rë¬¸ì„œì— í¬í•¨ë˜ëŠ” ë‹¨ì–´ë“¤ì´ ë¬¸ì„œë¥¼ êµ¬ë¶„í•˜ëŠ” ë° ì–¼ë§ˆë‚˜ ì˜í–¥ë ¥ì´ ìˆëŠ”ì§€ íŒë³„í•˜ê¸° ìœ„í•´ idfë¥¼ ê³„ì‚°í•´ì•¼ í•œë‹¤.\nì•„ë˜ ë‚´ìš©ì€ ë‚´ ê°•ì˜ ë…¸íŠ¸ì˜ ì¼ë¶€ë¶„ì„ ê°€ì ¸ì™”ë‹¤.\nğŸ“Œ idf weight\n$df_{term}$ëŠ” termë¥¼ í¬í•¨í•˜ëŠ”Â ë¬¸ì„œì˜ ë¹ˆë„ì´ë‹¤.\nìš°ë¦¬ëŠ” dfê°€ ì‘ì€ termì˜ ì ìˆ˜ë¥¼ ë” ë†’ê²Œ ì£¼ê³  ì‹¶ê¸° ë•Œë¬¸ì—, dfì„ ë’¤ì§‘ì–´ì„œ ë¶„ëª¨ë¡œ ì‚¬ìš©í•˜ì.\nidf(inverse document frequency)\n$idf_t=log_{10}(N/df_t)$\nN = ì „ì²´ document ìˆ˜\nidfê°’ì„ ì™„í™” ì‹œí‚¤ê¸° ìœ„í•´ logë¥¼ ì·¨í•´ì¤€ë‹¤.\nlogì˜ baseê°€ ê¼­ 10ì¼ í•„ìš”ëŠ” ì—†ë‹¤.\nìš°ì„  lf_weightì™€ ë§ˆì°¬ê°€ì§€ë¡œ idfë¥¼ ê³„ì‚°í•˜ê¸° ìœ„í•œ í•¨ìˆ˜ë¥¼ ì„ ì–¸í•´ì£¼ì—ˆë‹¤.\ndef idf_cal(x): return round(math.log(N / x, 2), 4) ë‹¨ì–´ì˜ idfë¥¼ êµ¬í•˜ê¸° ìœ„í•´, ë‹¨ì–´ê°€ ì „ì²´ ë¬¸ì„œ ì¤‘ ëª‡ ê°œì˜ ë¬¸ì„œì— í¬í•¨ ë˜ëŠ”ì§€ ì•Œì•„ì•¼ í•˜ëŠ”ë°, í•´ë‹¹ ë¶€ë¶„ì€ ë‹¤ìŒê³¼ ê°™ì€ ê³¼ì •ìœ¼ë¡œ ìˆ˜í–‰ëë‹¤.\ncorpus ì „ì²´ ë‚´ìš©ì—ì„œ oktë¥¼ í†µí•´ ë‹¨ì–´ë¥¼ ì¶”ì¶œí•˜ê³ , ì¤‘ë³µ ë‹¨ì–´ë¥¼ ì œê±°í•˜ì—¬ ë‹¨ì–´ ëª©ë¡ì„ ë§Œë“ ë‹¤.\nwords = [] for doc in dic.values(): doc_term = dict(Counter(okt.nouns(doc[0]))) words.extend(list(doc_term)) df = {} words = set(words) # print(words) ë‹¨ì–´ ëª©ë¡ì—ì„œ ë‹¨ì–´ë§ˆë‹¤ ëª‡ ê°œì˜ ë¬¸ì„œì— í¬í•¨ ë˜ëŠ”ì§€ í™•ì¸í•œë‹¤.\nfor w in words: count = 0 for doc in dic.values(): if w in doc[0]: count += 1 df[w] = count df = dict(sorted(df.items(), key=lambda x: x[1], reverse=True)) ë‹¨ì–´ë§ˆë‹¤ í¬í•¨ë˜ëŠ” ë¬¸ì„œì˜ ê°œìˆ˜ë¥¼ dictionaryì— ì €ì¥í•œ í›„, í•´ë‹¹ ê°’ì„ idfë¡œ ë³€í™˜í•œë‹¤.\ndf_raw = list(df.values()) idf = list(map(idf_cal, df_raw)) ë‚˜ì¤‘ì— ë‹¨ì–´ì˜ idfë¥¼ íƒìƒ‰í•˜ê¸° ìœ„í•´ idf listë¥¼ dictionary í˜•íƒœ{key: word, value: idf}ë¡œ ì €ì¥í•œë‹¤.\ni = 0 for key in df: df[key] = idf[i] i += 1 word_idf = dict(sorted(df.items(), key=lambda x: x[1], reverse=True)) 5. ë¬¸ì„œì˜ tf-idf Weight ê³„ì‚°í•˜ê¸°.\r#\rğŸ“Œ tf-idf weighting\ntermì˜ tf-idf ê°€ì¤‘ì¹˜ëŠ” tf ê°€ì¤‘ì¹˜ì™€ idf ê°€ì¤‘ì¹˜ì˜ ê³±ì´ë‹¤.\n$$ W_{t,d}=(1+log_{10}tf_{t,d}) \\times log_{10}(N/df_t) $$\nIRì—ì„œ ê°€ì¥ í•µì‹¬ì ì¸ ê°€ì¤‘ì¹˜ ê³µì‹ì´ë‹¤.\ntf.idfë‚˜ tf x idfë¼ê³  ë¶€ë¥´ê¸°ë„ í•œë‹¤.\nê°€ì¤‘ì¹˜ëŠ” collectionì—ì„œ termì˜ ë°œìƒ ë¹ˆë„ì— ë”°ë¼ ì¦ê°€í•œë‹¤.\nê°€ì¤‘ì¹˜ëŠ” ì»¬ë ‰ì…˜ ë‚´ì— termì´ í¬ê·€í• ìˆ˜ë¡ ì¦ê°€í•œë‹¤.\nì´ì œ ê±°ì˜ ë‹¤ ì™”ë‹¤. ê·¸ì € ë¬¸ì„œ ê°ê°ì— í¬í•¨ëœ ë‹¨ì–´ë³„ tf ê°’ì—, í•´ë‹¹ ë‹¨ì–´ë¥¼ word-idf ì‚¬ì „ì— ê²€ìƒ‰í•˜ì—¬ ê°’ì„ ê³±í•´ì£¼ê¸°ë§Œ í•˜ë©´ ëœë‹¤.\nfor doc in dic.values(): for word in doc[1].keys(): doc[1][word] = round(word_idf[word] * doc[1][word], 4) # print(doc[1]) ì´ì œ ì¸ë±ì‹±ì€ ì™„ë£Œë˜ì—ˆìœ¼ë‹ˆ, Queryë¥¼ ì…ë ¥ ë°›ê³  scoreë¥¼ ê³„ì‚°í•˜ê¸°ë§Œ í•˜ë©´ ëœë‹¤.\n6. query ì…ë ¥ ì°½ êµ¬í˜„í•˜ê¸°\r#\rwhile (1): query = input(\u0026#34;Enter Your Query:\u0026#34;) print(\u0026#34;query: \u0026#34; + query) query_term = okt.nouns(query) print(query_term) 7. Score ê³„ì‚°í•˜ê¸°\r#\r$$ \\operatorname{Score}(q, d)=\\sum_{t \\in q\\urcorner d} t f . i d f_{t, d} $$\nìœ„ ìˆ˜ì‹ì€ q(query)ì™€ d(document)ì—ì„œ ê³µí†µë˜ëŠ” termì„ ê°€ì§„ documentì˜ scoreë§Œ ê³„ì‚°í•œë‹¤ëŠ” ì˜ë¯¸ì´ë‹¤.\nscore ê³„ì‚°ì€ ìœ„ ìˆ˜ì‹ì²˜ëŸ¼, ì¿¼ë¦¬ì— í¬í•¨ë˜ëŠ” ë‹¨ì–´ ì¤‘ ë¬¸ì„œì— í¬í•¨ëœ ë‹¨ì–´ì˜ tf-idfë¥¼ ë”í•˜ë©´ ëœë‹¤.\nfor doc in dic.values(): score = 0 for q in query_term: for word in doc[1].keys(): if word == q: score += doc[1][word] if len(doc) == 2: doc.append(score) else: doc[2] = score 8. ë¬¸ì„œì˜ Scoreê°€ ë†’ì€ ìˆœì„œëŒ€ë¡œ ë³´ì—¬ì£¼ê¸°\r#\rfin_dic = sorted(dic.items(), key=lambda x: x[1][2], reverse=True) i = 0 for result in fin_dic: if i \u0026gt;= OUT: break i += 1 if result[1][2]: print(i, result) else: print(i, \u0026#34;ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.\u0026#34;) "},{"id":33,"href":"/posts/2023-05-22-SG/","title":"SG: Skip-Gram","section":"Blog","content":" Word2Vecì„ í•™ìŠµí•˜ëŠ” ë°©ë²• ì¤‘ í•˜ë‚˜.\nCBOWê°€ ë’¤ì§‘ì–´ì§„ ëª¨ë¸\nCBOWì™€ ì…ë ¥ì¸µê³¼ ì¶œë ¥ì¸µì´ ë°˜ëŒ€ë¡œ êµ¬ì„±ë˜ì–´ ìˆë‹¤.\në²¡í„°ì˜ í‰ê· ì„ êµ¬í•˜ëŠ” ê³¼ì •ì´ ì—†ë‹¤.\nCBOWë³´ë‹¤ ì„±ëŠ¥ì´ ì¢‹ë‹¤ê³  ì•Œë ¤ì ¸ìˆë‹¤.\nì°¸ê³ ë¡œ ì´ ë•Œì˜ ì„±ëŠ¥ì€ í•™ìŠµ ê³¼ì •ì˜ Lossê°€ ì•„ë‹ˆë¼ ì„ë² ë”© ë²¡í„°ì˜ í‘œí˜„ë ¥ì„ ì˜ë¯¸í•œë‹¤.\nCBOWì™€ ë§ˆì°¬ê°€ì§€ë¡œ Multi-Classification Modelì— í•´ë‹¹í•œë‹¤.\n"},{"id":34,"href":"/posts/2023-05-22-SGNS/","title":"SGNS: Skip-Gram with Negative Sampling","section":"Blog","content":"SGë¥¼ ì´ì§„ ë¶„ë¥˜ ë¬¸ì œë¡œ ë°”ê¾¼ ëª¨ë¸\nNegative Sampling\r#\rì£¼ë³€ ë‹¨ì–´ê°€ ì•„ë‹Œ ë‹¨ì–´ë¥¼ Label 0ìœ¼ë¡œ Sampleì— í¬í•¨ì‹œí‚¤ëŠ” ê²ƒ\nNegative Samplingì˜ ê°œìˆ˜ëŠ” í•˜ì´í¼íŒŒë¼ë¯¸í„°ì— í•´ë‹¹í•œë‹¤.\ní•™ìŠµ ë°ì´í„°ê°€ ì ì€ ê²½ìš° 5-20, ì¶©ë¶„íˆ í° ê²½ìš° 2-5ê°€ ì ë‹¹í•˜ë‹¤. positive sample í•˜ë‚˜ë‹¹ kê°œ ìƒ˜í”Œë§ ì¤‘ì‹¬ ë‹¨ì–´ì™€ ì£¼ë³€ ë‹¨ì–´ê°€ ê°ê° ì„ë² ë”© ë²¡í„°ë¥¼ ë”°ë¡œ ê°€ì§„ë‹¤.\nSGNSì—ì„œ embeddingì„ ë‘ ê°œë¡œ ë‚˜ëˆ„ì–´ ì‚¬ìš©í•˜ëŠ” ì´ìœ  ë§Œì•½ input/output í˜¹ì€ word/context representationì„ ë™ì¼í•œ ê°’ìœ¼ë¡œ ì‚¬ìš©í•œë‹¤ê³  í•˜ë©´,\níŠ¹ì • ë‹¨ì–´, ê°€ë ¹ \u0026ldquo;dog\u0026quot;ì— ëŒ€í•´ P(dog|dog)ê°€ í˜„ì‹¤ì ìœ¼ë¡œëŠ” ë¶ˆê°€í•˜ì§€ë§Œ (í•œ ë¬¸ì¥ì— \u0026ldquo;dog dog\u0026quot;ë¥¼ ì—°ì†ìœ¼ë¡œ ì“¸ ì¼ì€ ì—†ìœ¼ë‹ˆ..) word2vec ëª¨ë¸ ìƒìœ¼ë¡œëŠ” ë†’ì€ ê°’ì„ ë±‰ì–´ë‚¼ ìˆ˜ ë°–ì— ì—†ìŠµë‹ˆë‹¤.\nì´ëŸ¬í•œ ì–¸ì–´ì˜Â íŠ¹ìˆ˜ì„±ì„ í†µí•´ ìœ ì¶”í•´ë³´ê±´ëŒ€,Â ë¬¸ì¥ ë‚´ì—ì„œëŠ” í•˜ë‚˜ì˜ ë‹¨ì–´ê°€ ì¤‘ì‹¬ ë‹¨ì–´ì˜ ì—­í• ì„ í•  ë•Œì™€ ì£¼ë³€(ë§¥ë½) ë‹¨ì–´ì˜ ì—­í• ì„ í•  ë•Œì—Â ì„œë¡œ ë‹¤ë¥¸ í‘œí˜„ë ¥(representation power)ì„ ê°€ì§€ëŠ” ê²ƒì´ ì•„ë‹ê¹Œ ì‹¶ìŠµë‹ˆë‹¤.\ní•´ë‹¹ stackoverflow ë‹µë³€ì—ì„œ í˜¹ìëŠ”Â â€œ\u0026lsquo;ë¬¸ì¥ ë‚´ ë‹¨ì–´ ê°„ ìœ ì‚¬ë„/ê±°ë¦¬\u0026rsquo;ë¥¼ ì¸¡ì •í•  ë•Œ í•˜ë‚˜ì˜ ë²¡í„° ê³µê°„ë§Œì„ ì‚¬ìš©í•˜ê²Œ ë˜ë©´ ê²°êµ­ ê·¸ëƒ¥ ë‘ ë‹¨ì–´ ì„ë² ë”© ê°„ì˜ ìœ ì‚¬ë„/ê±°ë¦¬ë¥¼ ì¸¡ì •í•˜ëŠ” ê²ƒê³¼ ë³„ë°˜ ë‹¤ë¥´ì§€ ì•Šê¸° ë•Œë¬¸ì—Â ë¬¸ì¥ì˜ ë¬¸ë§¥ì„ ë‹´ì„ ìˆ˜ ì—†ë‹¤â€ëŠ” ì‹ìœ¼ë¡œ ì„¤ëª…í•˜ëŠ”ë°,Â ì´ ë˜í•œ ë¹„ìŠ·í•œ ë§¥ë½ì´ë¼ê³  ë³¼ ìˆ˜ ìˆì„ ê²ƒ ê°™ìŠµë‹ˆë‹¤.\ní•™ìŠµ ê³¼ì •\r#\rì¤‘ì‹¬ ë‹¨ì–´ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì£¼ë³€ ë‹¨ì–´ë“¤ê³¼ì˜ ë‚´ì  ì—°ì‚° ìˆ˜í–‰\nì‹¤ìˆ˜ ë‚´ì ê°’ì— sigmoidì„ ì·¨í•˜ì—¬ ì˜ˆì¸¡ê°’ì„ êµ¬í•œë‹¤.\nì˜ˆì¸¡ê°’ê³¼ ë ˆì´ë¸”(0,1)ì˜ ì˜¤ì°¨ë¥¼ êµ¬í•œë‹¤.\nì—­ì „íŒŒ(backpropagation)ë¥¼ í†µí•´ ê° ì„ë² ë”©ì´ ê°±ì‹ ë˜ë©° ëª¨ë¸ì´ ìˆ˜ë ´í•œë‹¤.\nì´ ë•Œ, ìµœì¢…ì ìœ¼ë¡œ ìƒì„±ëœ ì›Œë“œ ì„ë² ë”©ì´ 2ê°œì´ë¯€ë¡œ ì„ íƒì ìœ¼ë¡œ í•˜ë‚˜ë§Œ ì‚¬ìš©í•˜ê±°ë‚˜ í‰ê· ë‚´ì–´ ì‚¬ìš©í•œë‹¤.\n"},{"id":35,"href":"/posts/2023-05-28-SVD-Singular-Value-Decomposition%ED%8A%B9%EC%9D%B4%EA%B0%92-%EB%B6%84%ED%95%B4/","title":"SVD: Singular Value Decomposition(íŠ¹ì´ê°’ ë¶„í•´)","section":"Blog","content":" ê°„ë‹¨ ìš”ì•½\n2ì°¨ì› í–‰ë ¬ì„ ë‘ ê°œì˜ ì ì¬ìš”ì¸ í–‰ë ¬ê³¼ í•˜ë‚˜ì˜ ëŒ€ê°í–‰ë ¬ë¡œ ë¶„í•´í•˜ëŠ” ê¸°ë²• {: .prompt-info } eigen vector, eigen value\n2ì°¨ì› í–‰ë ¬ ë¶„í•´ ê¸°ë²• ìœ ì € ì ì¬ìš”ì¸ í–‰ë ¬ â‡’ ìœ ì € ì„ë² ë”© ì ì¬ìš”ì¸ ëŒ€ê°í–‰ë ¬ â‡’ ì„ë² ë”©ì˜ ì¤‘ìš”ë„ ì•„ì´í…œ ì ì¬ìš”ì¸ í–‰ë ¬ â‡’ ì•„ì´í…œ ì„ë² ë”© ì°¨ì›ì¶•ì†Œ ê¸°ë²• í–‰ë ¬ì„ ëŒ€ê°í™”í•˜ëŠ” ë°©ë²• ëª¨ë“  m x n í–‰ë ¬ì— ëŒ€í•´ ì ìš© ê°€ëŠ¥ Rating Matrix $R$ ì— ëŒ€í•´ ìœ ì €ì™€ ì•„ì´í…œì˜ ì ì¬ ìš”ì¸ì„ í¬í•¨í•  ìˆ˜ ìˆëŠ” í–‰ë ¬ë¡œ ë¶„í•´í•œë‹¤.\nFull SVD\r#\rê¸°ì¡´ í–‰ë ¬ì„ ì˜¨ì „í•˜ê²Œ 3ê°œì˜ í–‰ë ¬ë¡œ ë¶„í•´í•œë‹¤.\n$$ \\tt Full\\ \\ SVD :R = U\\Sigma V^T $$\n$U$: ìœ ì €ì™€ Latent Factorì˜ ê´€ê³„\n$U$ì˜ ì—´ë²¡í„°ëŠ” $R$ì˜ left singular vector\n$V$: ì•„ì´í…œê³¼ Latent Factorì˜ ê´€ê³„\n$V$ì˜ ì—´ë²¡í„°ëŠ” $R$ì˜ right singular vector\n$\\Sigma$: Latent Factorì˜ ì¤‘ìš”ë„\n$RR^T$ì„ ê³ ìœ ê°’ ë¶„í•´í•´ì„œ ì–»ì€ ì§ì‚¬ê° ëŒ€ê° í–‰ë ¬\nëŒ€ê° ì›ì†Œë“¤ì€ $R$ì˜ singular value(íŠ¹ì´ì¹˜)\nTruncated SVD\r#\r$\\Sigma$ë¥¼ ì¼ë¶€ë§Œ ì‚¬ìš©í•œë‹¤.\n$$ \\tt Truncated \\ \\ SVD: R \\approx \\widehat{U} \\Sigma_k \\widehat{V^T}=\\hat{R} $$\n$\\Sigma$ëŠ” ì¤‘ìš”ë„ë¡œ ì •ë ¬ë˜ì–´ ìˆê¸° ë•Œë¬¸ì—, ìƒìœ„ kê°œë§Œ í™œìš©í•˜ì—¬ ê¸°ì¡´ì˜ í–‰ë ¬ì„ ê±°ì˜ ìœ ì‚¬í•˜ê²Œ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆë‹¤.\nì¦‰, ëª‡ ê°œì˜ íŠ¹ì´ì¹˜ë§Œì„ ê°€ì§€ê³ ë„ ìœ ìš©í•œ ì •ë³´ë¥¼ ìœ ì§€í•œë‹¤.\në¶„í•´ëœ í–‰ë ¬ì´ ë¶€ë¶„ ë³µì›ë˜ë©´ì„œ ê°€ì¥ ì¤‘ìš”í•œ ì •ë³´ë¡œ ìš”ì•½ëœë‹¤.\n$\\widehat R$ì€ ì¶•ì†Œëœ $\\widehat U, \\widehat {V^T}, \\Sigma_k$ì— ì˜í•´ ê³„ì‚°ëœë‹¤.\nê°ê°ì˜ Kê°œì˜ Latent FactorëŠ” ìœ ì¶”í•  ìˆ˜ ìˆì„ ë¿, ì •í™•íˆ ë¬´ì—‡ì„ ì˜ë¯¸í•˜ëŠ”ì§€ ì•Œ ìˆ˜ ì—†ë‹¤.\nSVDì˜ í•œê³„\r#\rë¶„í•´(Decomposition)í•˜ë ¤ëŠ” í–‰ë ¬ì— ê²°ì¸¡ì¹˜ê°€ ì—†ì–´ì•¼ í•œë‹¤.\nUser-Item í–‰ë ¬ì˜ ê²½ìš° ëª¨ë“  ê°’ì´ ì±„ì›Œì ¸ì•¼ í•œë‹¤.\nSparsityê°€ ë†’ì€ ë°ì´í„°ì˜ ê²½ìš° ê²°ì¸¡ì¹˜ê°€ ë§¤ìš° ë§ë‹¤.\nì‹¤ì œ ë°ì´í„°ëŠ” ëŒ€ë¶€ë¶„ Sparse Matrix\nImputation í›„ SVDë¥¼ ìˆ˜í–‰ â†’ Computation ë¹„ìš© ì¦ê°€\nImputationì€ ë°ì´í„°ì˜ ì–‘ì„ ìƒë‹¹íˆ ì¦ê°€ì‹œí‚¤ê¸° ë•Œë¬¸\nImputationì— ì˜í•´ ë°ì´í„° ì™œê³¡ ë°œìƒ ì‹œ ì„±ëŠ¥ ì €í•˜\ní–‰ë ¬ì˜ entryê°€ ë§¤ìš° ì ì„ ë•Œ SVDë¥¼ ì ìš©í•˜ë©´ ê³¼ì í•© ë˜ê¸° ì‰½ë‹¤.\nì°¸ê³ í•˜ë©´ ì¢‹ì€ ìë£Œ \u0026ndash; SVDì˜ ì˜ë¯¸\n"},{"id":36,"href":"/posts/2023-11-01-Taylor-Polynomials/","title":"Taylor polynomials","section":"Blog","content":"\rí…Œì¼ëŸ¬ ê·¼ì‚¬\r#\rë³µì¡í•œ í˜•íƒœì˜ ë¯¸ë¶„ ê°€ëŠ¥í•œ í•¨ìˆ˜ $f(x)$ë¥¼ ë‹¤í•­ì‹ì˜ í•©ìœ¼ë¡œ ê·¼ì‚¬í•˜ëŠ” ê²ƒ\n$a$ë¥¼ í¬í•¨í•˜ëŠ” êµ¬ê°„ì—ì„œ í•¨ìˆ˜ $f$ê°€ ë¬´í•œ ë¯¸ë¶„ì´ ê°€ëŠ¥ í•  ë•Œ\n$$ \\begin{aligned}f(x) \u0026amp; =\\sum_{n=0}^{\\infty} \\frac{f^{(n)}(a)}{n !}(x-a)^n \\\u0026amp; =f(a)+\\frac{f^{\\prime}(a)}{1 !}(x-a)+\\frac{f^{\\prime \\prime}(a)}{2 !}(x-a)^2+\\frac{f^{\\prime \\prime \\prime}(a)}{3 !}(x-a)^3+\\ldots\\end{aligned} $$\në¥¼ í…Œì¼ëŸ¬ ê¸‰ìˆ˜ë¼ê³  í•œë‹¤.\n$f(x)$ë¥¼ ì„ì˜ì˜ ìˆ˜ $a$ì— ëŒ€í•´ ì •ë¦¬í•˜ëŠ” ê³¼ì •ì´ë¼ ì´í•´í•˜ë©´ í¸í•˜ë‹¤.\nìˆ˜ì‹ì´ ì´ë ‡ê²Œ ìƒê¸´ ì´ìœ \n$f(x)$ë¥¼ $a$ì— ëŒ€í•´ ì •ë¦¬í•˜ê³  ì‹¶ì–´ ì‹ì„ $f(x) = t_n(x-a)^n + t_{n-1}(x-a)^{n-1 }+\\dots + t_1(x-a)^1$ì™€ ê°™ì´ ì •ì˜í–ˆì„ ë•Œ,\n$f^{(n)}$ì€ $f(x)$ë¥¼ në²ˆ ë¯¸ë¶„í•˜ë©´ì„œ ë‚˜ë¨¸ì§€ ëª¨ë“  í•­ì´ ë‚ ì•„ê°€ê³  nì°¨í•­ë§Œ ë‚¨ê²Œ ëœë‹¤.\në˜í•œ, ë¯¸ë¶„ ê³¼ì •ì—ì„œ ì°¨ìˆ˜ëŠ” ê³„ìˆ˜ì— ê³±í•´ì§„ë‹¤.\n$f^{(n)}(1) = t_1 \\times n!$\nìš°ë¦¬ëŠ” ì›ë˜ ì‹ì˜ ê³„ìˆ˜ì¸ $t_n,\u0026hellip;,t_1$ì„ ì›í•˜ê¸° ë•Œë¬¸ì—, $n!$ìœ¼ë¡œ ë‹¤ì‹œ ë‚˜ëˆ ì¤€ë‹¤.\nì˜ˆì œë¥¼ í†µí•´ ë‹¤ì‹œ í•œ ë²ˆ ì‚´í´ë³´ì.\n$f(x) = 7x^3 - 18x^2 + 20x - 1$ë¼ê³  í•  ë•Œ, $x=1$ì— ëŒ€í•´ í…Œì¼ëŸ¬ ê¸‰ìˆ˜ë¥¼ ì ìš©í•´ë³´ì.\n$x=1$ì—ì„œì˜ í•¨ìˆ˜ê°’, ë¯¸ë¶„ê°’, ì´ì°¨ ë¯¸ë¶„ ê°’ ë“±ì„ êµ¬í•´ë³´ë©´\n$f(1) = 8$ $f\u0026rsquo;(1) = 5$ $f\u0026rsquo;\u0026rsquo;(1) = 6$ $f\u0026rsquo;\u0026rsquo;\u0026rsquo;(1) = 42$ ê³¼ ê°™ë‹¤.\nê° ë¯¸ë¶„ê°’ì— $\\tt(ë¯¸ë¶„ ì°¨ìˆ˜)!$ìœ¼ë¡œ ë‚˜ëˆ ì£¼ë©´ ìš°ë¦¬ê°€ ì›í•˜ëŠ” ê³„ìˆ˜ë¥¼ êµ¬í•  ìˆ˜ ìˆë‹¤.\n$$ f(x) = 7(x-1)^3 + 3(x-1)^2 + 5(x-1) + 8 $$\n$f(x)$ëŠ” ìœ„ì²˜ëŸ¼ ìƒê²¼ëŠ”ë°, í•´ë‹¹ ì‹ì— ë‹¤ì‹œ ë¯¸ë¶„ì„ í•´ë³´ë©´ ê·¸ëŒ€ë¡œ ì¼ì¹˜í•˜ëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤.\nê³¡ì„ ì€ ë¬´ì—‡ì— ì˜í•´ ì •ì˜ë ê¹Œ?\níƒ„ì  íŠ¸(ë¯¸ë¶„), ê³¡ë¥ (2ì°¨ ë¯¸ë¶„), torsion(ë¹„í‹€ë¦¼, 3ì°¨ ë¯¸ë¶„)\nì¦‰, ê³¡ì„ ì„ ì œëŒ€ë¡œ ê·¼ì‚¬í•˜ê¸° ìœ„í•´ì„  ì ì–´ë„ 3ì°¨ ë¯¸ë¶„ê¹Œì§€ ê·¼ì‚¬ë¥¼ í•´ì•¼ í•œë‹¤.\nìˆ˜í•™ì ìœ¼ë¡œëŠ” ë¬´í•œí•˜ê²Œ ë¯¸ë¶„ ê°€ëŠ¥í•˜ë‹¤ê³  ì •ì˜í•˜ì§€ë§Œ, ìˆ˜ì¹˜ í•´ì„ì—ì„œëŠ” ì ˆë‹¨ ì˜¤ì°¨ ê°œë…ì„ ë„ì…í•˜ì—¬ ì ì • ìˆ˜ì¤€ê¹Œì§€ë§Œ í•­ì„ ì •ì˜í•œë‹¤.\ní…Œì¼ëŸ¬ ê¸‰ìˆ˜ë¥¼ í™œìš©í•˜ì—¬ íŠ¹ìˆ˜í•œ ê°’ $a$ì˜ $f(a), f\u0026rsquo;(a), f\u0026rsquo;\u0026rsquo;(a), \\dots$ë¥¼ í™œìš©í•˜ì—¬ $a$ ì£¼ë³€ì˜ ê°’ì„ ê·¼ì‚¬í•œë‹¤.\ní…Œì¼ëŸ¬ ê¸‰ìˆ˜ëŠ” $x$ê°€ $a$ì—ì„œ ë©€ì–´ì§ˆìˆ˜ë¡ ì˜¤ì°¨ê°€ ì»¤ì§„ë‹¤.\nì´ë ‡ê²Œ, $a$ì§€ì ì—ì„œ ê·¼ì‚¬í•œ $n$ì°¨ í…Œì¼ëŸ¬ ë‹¤í•­ì‹ì„ $p_n(x ; a)$ë¡œ í‘œê¸°í•œë‹¤.\ní…Œì¼ëŸ¬ ê·¼ì‚¬ì˜ ì˜¤ì°¨\r#\r$f(x)$ì— ëŒ€í•œ í…Œì¼ëŸ¬ ê·¼ì‚¬ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ì‚¬ìš©í•˜ë ¤ë©´ ì •í™•ë„ë¥¼ ê³„ì‚°í•˜ëŠ” ê³¼ì •ì´ í•„ìš”í•˜ë‹¤.\ní…Œì¼ëŸ¬ ê¸‰ìˆ˜ë¡œ $n$ì°¨ê¹Œì§€ ê·¼ì‚¬í•œ ì‹ $p_n(x)$ê³¼ ì›ë³¸ ì‹$f(x)$ì˜ ì˜¤ì°¨ë¥¼ êµ¬í•´ë³´ì.\n$$ R_n(x) = f(x) - p_n(x) $$\nì›ë³¸ ì‹ì€ ë¬´í•œëŒ€ë¡œ ë¯¸ë¶„ì´ ê°€ëŠ¥í•  ìˆ˜ë„ ìˆì§€ë§Œ, ë§Œì•½ $n+1$ì°¨ê¹Œì§€ë§Œ ë¯¸ë¶„ì´ ê°€ëŠ¥í•˜ë‹¤ë©´?\nì›ë³¸ ì‹ê³¼ ê·¼ì‚¬ì‹ì˜ ì˜¤ì°¨ëŠ”\n$$ R_n(x) = {f^{(n+1)}(a)\\over(n+1)!}(x-a)^{n+1} $$\nê°€ ë˜ê³ , $R_n(x)$ëŠ”\nì½”ì‹œì˜ í‰ê· ê°’ ì •ë¦¬ì— ì˜í•´\n$$ {f^{(n+1)}(a)\\over(n+1)!}(x-a)^{n+1}\u0026lt;R_n(cx) \u0026lt; {f^{(n+1)}(x)\\over(n+1)!}(x-a)^{n+1} $$\nì„ ë§Œì¡±í•œë‹¤.\nPolynomial form\r#\rPower form\r#\rì„ í˜• ê²°í•© í˜•íƒœ\n$$ p_n=a_0+a_1 x+a_2 x^2+\\cdots+a_n x^n=\\sum_{k=0}^n a_k x^k $$\nShifted Power form\r#\r$c$ê°€ ì¤‘ì‹¬ì´ ë˜ëŠ” ë‹¤í•­ì‹\n$$ p_n=b_0+b_1(x-c)+b_2(x-c)^2+\\cdots+b_n(x-c)^n $$\nTaylor form(Taylor Polynomial)\r#\rshifted power formì—ì„œì˜ ê³„ìˆ˜ê°€ $b_k=\\frac{p_n^{(k)}(c)}{k !}$ì¸ ê²½ìš°\n$$ p_n(x)=p_n(c)+(x-c) p_n^{\\prime}(c)+\\frac{(x-c)^2}{2 !} p_n^{\\prime \\prime}(c)+\\cdots+\\frac{(x-c)^n}{n!} p_n^{(n)}(c) $$\nNewton form\r#\rì¤‘ì‹¬ì´ $c_1,c_2,\u0026hellip;,c_n$ì¸ ê²½ìš°\n$$ \\begin{matrix}p_n\u0026amp;=\u0026amp;d_0+d_1\\left(x-c_1\\right)+d_2\\left(x-c_1\\right)\\left(x-c_2\\right)\\\u0026amp;\u0026amp;+\\cdots+d_n\\left(x-c_1\\right) \\ldots\\left(x-c_n\\right)\\\\ \u0026amp;=\u0026amp;d_0+\\sum_{k=1}^n d_k \\prod_{j=1}^k\\left(x-c_j\\right) \\end{matrix} $$\nnewton formì—ì„œ $c_1=\\cdots=c_n=c$ë¡œ ì„¤ì •í•˜ë©´ shifted power formì´ ë˜ê³ , $c_1=\\cdots=c_n=0$ì´ë©´ power formì´ ëœë‹¤.\nNested form(Honerâ€™s method)\r#\r$$ \\begin{aligned}p(x) \u0026amp; =a_0+a_1 x+a_2 x^2+a_3 x^3+\\cdots+a_n x^n \\\u0026amp; =a_0+x\\left(a_1+x\\left(a_2+x\\left(a_3+\\cdots+x\\left(a_{n-1}+x a_n\\right) \\cdots\\right)\\right)\\right)\\end{aligned} $$\nPolynomial evaluation\r#\rìˆ˜ì¹˜ í•´ì„ì€ í•¨ìˆ˜ë¥¼ ì •í™•í•˜ê²Œ ê·¼ì‚¬í•´ë‚´ëŠ” ëª©ì ì´ ìˆë‹¤.\nì´ë¥¼ ìœ„í•´ í…Œì¼ëŸ¬ ê¸‰ìˆ˜ë¥¼ í™œìš©í•œë‹¤.\ní…Œì¼ëŸ¬ ê¸‰ìˆ˜ë¥¼ í†µí•´ í•¨ìˆ˜ì˜ ëª¨ì–‘ì„ ë¹„ìŠ·í•˜ê²Œ ë”°ë¥´ë”ë¼ë„, ê°’ì´ ë™ì¼í•œì§€ë¥¼ ì²´í¬í•˜ê¸° ìœ„í•´ ë°˜ë“œì‹œ ì˜¤ì°¨ê°’ì„ êµ¬í•´ì•¼ í•œë‹¤.\nìš°ë¦¬ê°€ ì‚¬ìš©í•˜ëŠ” ëŒ€ë¶€ë¶„ì˜ ê¸°ê³„ëŠ” Taylor ë¶„ì„ì„ í†µí•œ ê·¼ì‚¬ë¡œ ë§Œë“¤ì–´ì§„ë‹¤.\nê·¼ì‚¬ì˜ ì†ë„ë¥¼ ì¸¡ì •í•˜ëŠ” ë°©ë²•\në‚˜ë¨¸ì§€ ì •ë¦¬ ì¡°ë¦½ ì œë²• í”„ë¡œê·¸ë˜ë¨¸ ê´€ì ìœ¼ë¡œ ë‹¤í•­ì‹ì„ í‰ê°€í•˜ì.\nìˆ˜ì‹ ê³„ì‚°ì—ì„œ ê³±ì…ˆ ì—°ì‚°ì´ ê½¤ ë¬´ê²ê¸° ë•Œë¬¸ì—, ê³±ì…ˆ íšŸìˆ˜ë¥¼ ìµœëŒ€í•œ ì¤„ì—¬ì•¼ í•œë‹¤.\n$p(x) = a_0 + a_1x + a_2x^2 + \u0026hellip; + a_nx^n$ì—ì„œ $a_0,a_1,\u0026hellip;,a_n$ê³¼ $x$ê°€ ì£¼ì–´ì§€ëŠ” ê²½ìš° ë‹¤í•­ì‹ì„ ê³„ì‚°í•˜ê¸° ìœ„í•´ ëª‡ ë²ˆì˜ ê³±ì…ˆì´ í•„ìš”í•œê°€?\nex) $p(x) = 3 - 4x -5x^2 - 6x^3 + 7x^4 - 8x^5$\r#\rìœ„ì˜ 5ì°¨ ë‹¤í•­ì‹ì„ ì§ê´€ì ìœ¼ë¡œ ê³„ì‚°í•˜ë©´ ì´ 15ë²ˆì˜ ê³±ì…ˆì´ í•„ìš”í•˜ë‹¤.\n$4\\times x$ â‡’ 1\n$5\\times x\\times x$ â‡’ 2\n$\\cdots$\nì´ ê³±ì…ˆì˜ ê°œìˆ˜: ${n(n+1)\\over 2}$ â‡’ $O(n^2)$\nì–´ë–»ê²Œ ë‹¤í•­ì‹ì„ ë” íš¨ìœ¨ì ìœ¼ë¡œ í‰ê°€í•  ìˆ˜ ìˆì„ê¹Œ? â‡’ ì¤‘ë³µ ê³„ì‚°ì„ ìµœì†Œí™”í•˜ì.\n$x^3$ì„ ê³„ì‚°í•  ë•Œ, $x\\times x\\times x$ë¡œ ê³„ì‚°í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼, ê¸°ì¡´ì— ê³„ì‚°ëœ $x^2$ë¥¼ í™œìš©í•˜ì—¬ $x\\times x^2$ë¥¼ ê³„ì‚°í•˜ê²Œ ë˜ë©´ ê³±ì…ˆ ê°œìˆ˜ë¥¼ ì¤„ì¼ ìˆ˜ ìˆë‹¤.\n$4 \\times x$ â‡’ 1\n$5\\times x\\times x$ â‡’ 2\n$6 \\times x \\times x^2$ â‡’ 2\n$\\cdots$\nì´ ê³±ì…ˆì˜ ê°œìˆ˜: $2n -1$ â‡’ $O(n)$\në©”ëª¨ì´ì œì´ì…˜ì„ ì ìš©í•˜ë©´ $O(N)$ì´ ëœë‹¤.\në©”ëª¨ì´ì œì´ì…˜ì„ ì ìš©í–ˆì„ ë•Œì˜ ë‹¨ì ìœ¼ë¡œëŠ”, ìˆœì„œê°€ ë°œìƒí•˜ê¸° ë•Œë¬¸ì— ë³‘ë ¬ ê³„ì‚°ì´ ì•ˆëœë‹¤.\nload balancingì„ ì—†ì• ë©° ë³‘ë ¬ ê³„ì‚°ì„ í•˜ëŠ” ê²ƒì´ ê°€ì¥ ì¤‘ìš”í•˜ë‹¤.\ní˜¸ë„ˆì˜ ë²•ì¹™ì„ í™œìš©í•˜ë©´ ì´ ê³±ì…ˆì˜ ê°œìˆ˜ê°€ **$n$**ìœ¼ë¡œ ì¤„ì–´ë“ ë‹¤.\n"},{"id":37,"href":"/posts/2023-09-19-Tensor/","title":"Tensorì— ëŒ€í•´ ì•Œì•„ë³´ì.","section":"Blog","content":" ê°„ë‹¨ ìš”ì•½\nautograd ì—°ì‚°ì„ ì§€ì›í•˜ëŠ” ë‹¤ì°¨ì› ë°°ì—´\ntensorì— ëŒ€í•œ ë¯¸ë¶„ê°’ì„ ê°€ì§„ë‹¤.\nreshapeë³´ë‹¤ viewë¥¼ ì“°ëŠ” ê²ƒì´ ì¢‹ë‹¤. squeezeì™€ unsqueezeì˜ ì°¨ì´ mm, dot, matmul ì°¨ì´\n{: .prompt-info } ì‹ ê²½ë§ì˜ ê°€ì¤‘ì¹˜(ë§¤ê°œë³€ìˆ˜)ë¥¼ í…ì„œë¡œ í‘œí˜„í•œë‹¤.\në‹¤ì°¨ì› Arraysë¥¼ í‘œí˜„í•˜ëŠ” PyTorch í´ë˜ìŠ¤\nnumpyì˜ ndarrayì™€ í˜¸í™˜ëœë‹¤.\nTensorFlowì˜ Tensorì™€ë„ ë™ì¼\nTensorì„ ìƒì„±í•˜ëŠ” í•¨ìˆ˜ë„ ê±°ì˜ ë™ì¼\nnumpy â€” ndarray\nimport numpy as np n_array = np.arange(10).reshape(2,5) print(n_array) print(\u0026#34;n_dim :\u0026#34;, n_array.ndim, \u0026#34;shape :\u0026#34;, n_array.shape) pytorch â€” tensor\nimport torch t_array = torch.FloatTensor(n_array) print(t_array) print(\u0026#34;n_dim :\u0026#34;, t_array.ndim, \u0026#34;shape :\u0026#34;, t_array.shape) listë‚˜ ndarrayë¥¼ Tensorë¡œ ë³€í™˜í•  ìˆ˜ ìˆë‹¤.\në°ì´í„° íƒ€ì…ì€ numpyì™€ ë™ì¼í•˜ë‹¤.\nGPUì— ì˜¬ë ¤ ì‚¬ìš©ì´ ê°€ëŠ¥í•˜ë‹¤.\nx_data.device # device(type=\u0026#39;cpu\u0026#39;) if torch.cuda.is_available(): x_data_cuda = x_data.to(\u0026#39;cuda\u0026#39;) x_data_cuda.device # device(type=\u0026#39;cuda\u0026#39;, index=0) view, squeeze, unsqueeze ë“±ìœ¼ë¡œ tensor ì¡°ì •ì´ ê°€ëŠ¥í•˜ë‹¤.\nview: reshapeì™€ ë™ì¼í•˜ê²Œ tensorì˜ shapeë¥¼ ë³€í™˜í•œë‹¤.\nviewì™€ reshapeì˜ ì°¨ì´: contiguity(ì ‘ê·¼) ë³´ì¥ ì—¬ë¶€\nviewì˜ ê²½ìš° í´ë˜ìŠ¤ë¡œì˜ ì ‘ê·¼ì„ ê³„ì† ë³´ì¥í•´ì£¼ì§€ë§Œ, reshapeëŠ” ì ‘ê·¼ì„ ë³´ì¥í•´ì£¼ì§€ ì•ŠëŠ”ë‹¤.\në§Œì•½ ì ‘ê·¼ì„ ë³´ì¥í•  ìˆ˜ ì—†ëŠ” ê²½ìš° copyë¥¼ í•´ë²„ë¦°ë‹¤.\nsqueeze: ì°¨ì›ì˜ ê°œìˆ˜ê°€ 1ì¸ ì°¨ì›ì„ ì‚­ì œí•œë‹¤. (ì••ì¶•)\nimport torch x = torch.rand(1, 1, 20, 128) x = x.squeeze() # [1, 1, 20, 128] -\u0026gt; [20, 128] x2 = torch.rand(1, 1, 20, 128) x2 = x2.squeeze(dim=1) # [1, 1, 20, 128] -\u0026gt; [1, 20, 128] unsqueeze: ì°¨ì›ì˜ ê°œìˆ˜ê°€ 1ì¸ ì°¨ì›ì„ ì¶”ê°€í•œë‹¤.\nì¶”ê°€í•  ìœ„ì¹˜ë¥¼ ì§€ì •í•´ì£¼ì–´ì•¼ í•œë‹¤.\nimport torch x = torch.rand(3, 20, 128) x = x.unsqueeze(dim=1) #[3, 20, 128] -\u0026gt; [3, 1, 20, 128] ë‹¤ë¥¸ ê¸°ë³¸ì ì¸ ì—°ì‚°ì€ Tensorì™€ Numpyê°€ ê±°ì˜ ë™ì¼í•˜ë‹¤.\ndot, mm, matmul ì°¨ì´\r#\rdot : ë‚´ì  ì—°ì‚°.\nmm : í–‰ë ¬ ê³±ì…ˆ (ë²¡í„° ì—°ì‚° ì§€ì› x).\ní–‰ë ¬ê³±ì…ˆ ì—°ì‚°ì´ Tensorì—ì„œëŠ” dot ëŒ€ì‹  mm(matrix multiplication)ìœ¼ë¡œ í‘œê¸°ëœë‹¤.\nmatmul : ì•Œì•„ì„œ broadcastingì„ ì§€ì›í•´ì¤€ë‹¤.\nì‰½ê²Œ ì—°ì‚°í•´ì¤€ë‹¤ëŠ” ì¥ì ì´ ìˆì§€ë§Œ, ì˜¤íˆë ¤ ê²°ê³¼ë¥¼ í—·ê°ˆë¦¬ê²Œ ë§Œë“œëŠ” ë‹¨ì ì´ ìˆë‹¤.\nTensorì˜ êµ¬ì¡°\r#\r1ì°¨ì›: iris ìƒ˜í”Œ í•˜ë‚˜ 2ì°¨ì›: iris ìƒ˜í”Œ ì—¬ëŸ¬ ê°œ, ëª…ì•” ì˜ìƒ í•œ ì¥ 3ì°¨ì›: ëª…ì•” ì˜ìƒ ì—¬ëŸ¬ ì¥, ì»¬ëŸ¬ ì˜ìƒ í•œ ì¥ 4ì°¨ì›: ì»¬ëŸ¬ ì˜ìƒ ì—¬ëŸ¬ ì¥, ì»¬ëŸ¬ ë™ì˜ìƒ í•˜ë‚˜ 5ì°¨ì›: ì»¬ëŸ¬ ë™ì˜ìƒ ì—¬ëŸ¬ ê°œ "},{"id":38,"href":"/posts/2023-05-26-WDN-Wide-Deep-Network/","title":"WDN: Wide \u0026 Deep Network","section":"Blog","content":"Wide \u0026amp; Deep Learning for Recommender Systems\nì„ í˜•ì ì¸ ëª¨ë¸(Wide)ê³¼ ë¹„ì„ í˜•ì ì¸ ëª¨ë¸(Deep)ì„ ê²°í•©í•˜ì—¬ ê¸°ì¡´ ëª¨ë¸ë“¤ì˜ ì¥ì ì„ ëª¨ë‘ ì·¨í•˜ê³ ì í•œ ë…¼ë¬¸\në“±ì¥ ë°°ê²½\r#\rì¶”ì²œì‹œìŠ¤í…œì—ì„œ í•´ê²°í•´ì•¼ í•  ë‘ ê°€ì§€ ê³¼ì œ\nMemorization â€” í•™ìŠµë°ì´í„°ì— ìì£¼ ë“±ì¥í•˜ëŠ” íŒ¨í„´ì€ ëª¨ë¸ì´ ì•”ê¸°í•´ì•¼ í•œë‹¤.\ní•¨ê»˜ ë¹ˆë²ˆíˆ ë“±ì¥í•˜ëŠ” ì•„ì´í…œ í˜¹ì€ íŠ¹ì„±(feature) ê´€ê³„ë¥¼ ê³¼ê±° ë°ì´í„°ë¡œë¶€í„° í•™ìŠµí•˜ëŠ” ê²ƒ\nLogistic Regressionê³¼ ê°™ì€ ì„ í˜• ëª¨ë¸\nëŒ€ê·œëª¨ ì¶”ì²œ ì‹œìŠ¤í…œ ë° ê²€ìƒ‰ ì—”ì§„ì—ì„œ ì‚¬ìš©í•´ì™”ë‹¤. í™•ì¥ ë° í•´ì„ì´ ìš©ì´í•˜ë‹¤. í•™ìŠµ ë°ì´í„°ì— ì—†ëŠ” feature ì¡°í•©ì— ì·¨ì•½í•˜ë‹¤. Generalization â€” í•™ìŠµë°ì´í„°ì— ë°œìƒí•˜ì§€ ì•ŠëŠ” íŒ¨í„´ì„ ì ì ˆí•˜ê²Œ í‘œí˜„í•´ì•¼ í•œë‹¤.\në“œë¬¼ê²Œ ë°œìƒí•˜ê±°ë‚˜ ì „í˜€ ë°œìƒí•œ ì  ì—†ëŠ” ì•„ì´í…œ/íŠ¹ì„± ì¡°í•©ì„ ê¸°ì¡´ ê´€ê³„ë¡œë¶€í„° ë°œê²¬í•˜ëŠ” ê²ƒ\nFM, DNNê³¼ ê°™ì€ ì„ë² ë”© ê¸°ë°˜ ëª¨ë¸\nì¼ë°˜í™”ê°€ ê°€ëŠ¥í•˜ë‹¤. ê³ ì°¨ì›ì˜ Sparse ë°ì´í„°ë¡œ ì„ë² ë”©ì„ ë§Œë“¤ê¸°ê°€ ì–´ë µë‹¤. ì´ ë‘˜ì„ ê²°í•©í•˜ì—¬ ì‚¬ìš©ìì˜ ê²€ìƒ‰ ì¿¼ë¦¬ì— ë§ëŠ” ì•±ì„ ì¶”ì²œí•˜ëŠ” ëª¨ë¸ì„ ì œì•ˆí•œë‹¤.\nëª¨ë¸ êµ¬ì¡°\r#\rWide(Memorization Model)\r#\rì„ í˜• ëª¨ë¸ê³¼ ê±°ì˜ ë¹„ìŠ·í•œ ëª¨ë¸\nGeneralized Linear Model\n$\\tt y = w^Tx + b$\n${\\tt w = [w_1,\u0026hellip;,w_n]}$ $\\tt x = [x_1,\u0026hellip;,x_n]$ $b \\in \\R$ ì´ì™€ ê°™ì€ êµ¬ì¡°ë§Œìœ¼ë¡œëŠ” ë‘ ë³€ìˆ˜ì˜ ê´€ê³„ë¥¼ íŒŒì•…í•  ìˆ˜ ì—†ë‹¤.\nCross-Product Transformation\nì„œë¡œë‹¤ë¥¸ ë‘ ë³€ìˆ˜ì˜ ê´€ê³„ë¥¼ í•™ìŠµí•˜ê¸° ìœ„í•´ Cross-Product Termì„ ì¶”ê°€í•´ì¤€ë‹¤.\n$$ \\tt \\phi_k(x) = \\Pi^d_{i=1}x_i^{c_{ki}}, \\quad c_{ki} \\in {0,1} $$\nì´ ë•Œ, ê°€ëŠ¥í•œ ëª¨ë“  ë³€ìˆ˜ë“¤ ê°„ì˜ ë‚´ì ì„ í‘œí˜„í•˜ë©´ í•™ìŠµí•´ì•¼ í•  íŒŒë¼ë¯¸í„°ê°€ ë„ˆë¬´ ë§ì•„ì§€ê²Œ ëœë‹¤.\në”°ë¼ì„œ, í•´ë‹¹ ëª¨ë¸ì—ì„œëŠ” ì£¼ìš” feature 2ê°œì— ëŒ€í•œ second-order Cross Productë§Œ ì‚¬ìš©í•œë‹¤.\nìœ„ì˜ ëª¨ë¸ë§ì€ Polynomial Logistic Regressionê³¼ ê±°ì˜ ë™ì¼í•˜ë‹¤.\n$$ \\hat y(x)=\\left(w_0+\\sum_{i=1}^n w_i x_i{+\\sum_{i=1}^n \\sum_{j=i+1}^n w_{i j} x_i x_j}\\right), \\quad w_i, w_{i j} \\in \\mathbb{R} $$\nì´ ëª¨ë¸ë¡œëŠ” $n^2$ë§Œí¼ í•™ìŠµ íŒŒë¼ë¯¸í„°ê°€ ëŠ˜ì–´ë‚˜ê²Œ ëœë‹¤.\nì¦‰, Wide Componentë§Œìœ¼ë¡œëŠ” í‘œí˜„í•  ìˆ˜ ìˆëŠ” ìƒí˜¸ì‘ìš©ì˜ í•œê³„ê°€ ëª…í™•í•˜ë‹¤.\nDeep(Generalization Model)\r#\rë‹¨ìˆœí•œ êµ¬ì¡°.\nFeed-Forward Neural Network\n3 layerë¡œ êµ¬ì„±ë˜ì—ˆìœ¼ë©°, ReLU í•¨ìˆ˜ë¥¼ ì‚¬ìš©\nì—°ì†í˜• ë³€ìˆ˜ëŠ” ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ê³ , ì¹´í…Œê³ ë¦¬í˜• ë³€ìˆ˜ëŠ” í”¼ì³ ì„ë² ë”© í›„ ì‚¬ìš©\nì „ì²´ êµ¬ì¡° ë° ì†ì‹¤ í•¨ìˆ˜\r#\r$$ P(Y=1|x) = \\tt\\sigma(w^T_{wide}[x,\\phi(x)] + w^T_{deep}a^{(lf)} + b) $$\n$\\tt x:$ ì£¼ì–´ì§„ nê°œì˜ ë³€ìˆ˜\n$\\tt \\phi(x):$ nê°œ ë³€ìˆ˜ê°„ì˜ ìƒí˜¸ì‘ìš©(Cross-Product)\nìœ„ì—ì„œ ì–¸ê¸‰í•œ ê²ƒì²˜ëŸ¼, [ì‚¬ìš©ìê°€ ê³¼ê±°ì— ì„¤ì¹˜í•œ ì•±]ê³¼ [ì‚¬ìš©ìê°€ í˜„ì¬ CTRì„ ì˜ˆì¸¡í•  ì•±]ì˜ ìƒí˜¸ì‘ìš©ë§Œ ë°˜ì˜í•œë‹¤.\nëª¨ë¸ ì„±ëŠ¥\r#\rBaselineì¸ Wide ëª¨ë¸ê³¼ Deep ëª¨ë¸ì€ ê°ê° Offline, Onlineì—ì„œ ì„œë¡œ ë‹¤ë¥¸ ì–‘ìƒì„ ë³´ì´ì§€ë§Œ, ë‘ ê°œ ëª¨ë¸ì„ ê²°í•©í•˜ì—¬ ë§Œë“  Wide \u0026amp; Deep ëª¨ë¸ì€ ëª¨ë‘ ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì˜€ë‹¤.\n"},{"id":39,"href":"/posts/2023-09-23-%EC%88%98%EC%B9%98-%EB%AF%B8%EB%B6%84/","title":"ê²½ì‚¬ í•˜ê°•ë²•ì— ì˜¤ì°¨ ì—­ì „íŒŒê°€ ì—†ë‹¤ë©´ ë¬´ìŠ¨ ì¼ì´ ì¼ì–´ë‚ ê¹Œ?","section":"Blog","content":" ì†ì‹¤ í•¨ìˆ˜, Gradient Descent, Back Propagation\nìˆ˜ì¹˜ ë¯¸ë¶„\r#\rí•œ ì ì—ì„œì˜ ê¸°ìš¸ê¸°. ë³€í™”ëŸ‰ì„ ì˜ë¯¸í•œë‹¤.\nê²½ì‚¬ í•˜ê°•ë²•ì„ ì‚¬ìš©í•˜ê¸° ìœ„í•´ì„œëŠ” ë¯¸ë¶„ê°’ì´ í•„ìš”í•˜ë‹¤.\n$$ {df(x)\\over dx} = \\lim_{h \\to 0} {f(x+h) - f(x)\\over h} $$\nìˆ˜ì¹˜ ë¯¸ë¶„ì´ ê²½ì‚¬ í•˜ê°•ë²•ì— ì‚¬ìš©ë˜ëŠ” ë°©ë²•\r#\rê²½ì‚¬ í•˜ê°•ë²•ì—ì„œëŠ” $f(x)$ê°€ ì†ì‹¤ í•¨ìˆ˜ì´ê³ , xê°€ í˜„ì¬ì˜ ê°€ì¤‘ì¹˜ë‚˜ í¸í–¥ì´ ëœë‹¤.\nì†ì‹¤ í•¨ìˆ˜ëŠ” ëŒ€ìƒ ê°’ê³¼ ì˜ˆì¸¡ ê°’ì˜ ì˜¤ì°¨ë¥¼ ì˜ë¯¸í•˜ë¯€ë¡œ,\nì†ì‹¤ í•¨ìˆ˜ì— ëŒ€í•œ ë¯¸ë¶„ ê°’ì„ êµ¬í•œ í›„, ì˜¤ì°¨ë¥¼ ì¤„ì´ëŠ” ë°©í–¥ìœ¼ë¡œ ê°€ì¤‘ì¹˜ì™€ í¸í–¥ì„ ìˆ˜ì •í•  ìˆ˜ ìˆë‹¤. ì‰½ê²Œ ë‚©ë“ì´ ê°€ëŠ¥í•œ ê³¼ì •ì„ êµ³ì´ ìˆ«ìê¹Œì§€ ë¶™ì—¬ê°€ë©° ë‚˜ëˆˆ ì´ìœ ê°€ ìˆë‹¤.\nì´ ê³¼ì •ì—ì„œëŠ” ì¹˜ëª…ì ì¸ ë¬¸ì œê°€ ì¡´ì¬í•œë‹¤.\nìœ„ì˜ ë¯¸ë¶„ ê°’ ê³µì‹ì„ ìì„¸íˆ ì‚´í´ë³´ì.\nê²½ì‚¬ í•˜ê°•ë²•ì—ì„œëŠ” $f(x)$ê°€ ì†ì‹¤ í•¨ìˆ˜ë¥¼ ì˜ë¯¸í•˜ëŠ”ë°, ì†ì‹¤ í•¨ìˆ˜ëŠ” ì˜ˆì¸¡ ê°’ê³¼ ëª©í‘œ ê°’ì˜ ì˜¤ë¥˜ë¥¼ ì˜ë¯¸í•œë‹¤.\në˜í•œ xëŠ” í˜„ì¬ì˜ ê°€ì¤‘ì¹˜ë‚˜ biasë¥¼ ì˜ë¯¸í•œë‹¤.\nì¦‰, $f(x)$ë¥¼ ê³„ì‚°í•˜ê¸° ìœ„í•´ì„œëŠ” ì‹ ê²½ë§ì´ ì˜ˆì¸¡ì„ í•œ ë²ˆ ìˆ˜í–‰í•˜ê³ , ëª©í‘œ ê°’ê³¼ì˜ ì°¨ì´ë¥¼ ê³„ì‚°í•´ì•¼ í•œë‹¤.\nì—¬ê¸°ê¹Œì§€ë§Œ í•˜ë”ë¼ë„ ê³„ì‚°ì´ ìƒë‹¹íˆ í¬ë‹¤ëŠ” ê²ƒì„ ì§ì‘í•  ìˆ˜ ìˆìœ¼ë‚˜, ë¯¸ë¶„ ê°’ì„ êµ¬í•˜ê¸° ìœ„í•´ì„œëŠ”\n$f(x), f(x + h)$ë¥¼ êµ¬í•´ì•¼ í•˜ë¯€ë¡œ, ë‘ ì§€ì ì—ì„œ ì‹ ê²½ë§ì˜ ì˜ˆì¸¡ì´ í•„ìš”í•˜ë‹¤.\nìš”ì•½í•˜ë©´, ë‹¨ í•˜ë‚˜ì˜ ê°€ì¤‘ì¹˜ë¥¼ ê³„ì‚°í•˜ëŠ” ë° ì‹ ê²½ë§ì´ ë‘ ë²ˆì´ë‚˜ ë™ì‘í•œë‹¤.\ní•˜ë‚˜ì˜ epochì— ê°€ì¤‘ì¹˜ í•˜ë‚˜, bias í•˜ë‚˜ê°€ ìˆë‹¤ê³  ìƒê°í•˜ë©´, í•™ìŠµì„ ìœ„í•œ ë§ ê³„ì‚°ì€ ì´ 4ë²ˆì´ ëœë‹¤.\nì•„ë¬´ë¦¬ ë‹¨ìˆœí•œ ëª¨ë¸ì„ êµ¬ì„±í•˜ë”ë¼ë„, ì—°ì‚°ëŸ‰ì´ ë„ˆë¬´ ë§ë‹¤.\nex) layer 3ì¸µ, ì€ë‹‰ì¸µ ë…¸ë“œ 10ê°œ, epoch 1000ì¸ ê²½ìš°\nê°€ì¤‘ì¹˜ ë­‰ì¹˜ 3ê°œ, í¸ì°¨ 3ê°œ ì¡´ì¬\n(4 * 3) * 1000 = 12,000ë²ˆì˜ ì‹ ê²½ë§ ì˜ˆì¸¡.\nìˆ˜ì¹˜ ë¯¸ë¶„ì€ ì†ë„ê°€ ë„ˆë¬´ ëŠë¦¬ë‹¤.\nì´ë¥¼ ë³´ì™„í•˜ê¸° ìœ„í•´ ì˜¤ë¥˜ ì—­ì „íŒŒ ì•Œê³ ë¦¬ì¦˜ì´ ë“±ì¥í–ˆë‹¤.\nì˜¤ë¥˜ ì—­ì „íŒŒ ì•Œê³ ë¦¬ì¦˜ì€ ìœ„ì™€ ê°™ì€ ìƒí™©ì—ì„œ ì‹ ê²½ë§ì´ ë‹¨ ë‘ ë²ˆë§Œ ë™ì‘í•˜ì—¬ í•œ ë²ˆì˜ ê°€ì¤‘ì¹˜ í•™ìŠµì„ ì™„ë£Œí•œë‹¤.\nì¦‰, ìœ„ì˜ ì˜ˆì‹œì—ì„œ ì˜¤ë¥˜ ì—­ì „íŒŒë¥¼ ì‚¬ìš©í•˜ë©´ ì´ 2000ë²ˆì˜ ë§ ê³„ì‚°ë§Œ í•„ìš”í•˜ë‹¤.\n"},{"id":40,"href":"/posts/2023-08-15-%EB%84%A4%EC%9D%B4%EB%B2%84-%EB%B6%80%EC%8A%A4%ED%8A%B8%EC%BA%A0%ED%94%84-AI-Tech-%ED%9A%8C%EA%B3%A0-1-%EA%B0%95%EC%9D%98-%EB%8C%80%ED%9A%8C/","title":"ë„¤ì´ë²„ ë¶€ìŠ¤íŠ¸ìº í”„ AI Tech íšŒê³  1 â€” ê°•ì˜, ëŒ€íšŒ","section":"Blog","content":"\r{: lqip=\u0026quot;/assets/post_imgs/boostcamp_logo.png\u0026quot; }\në“œë””ì–´! ìµœì¢… í”„ë¡œì íŠ¸ë¥¼ ì œì¶œí•˜ë©´ì„œ, ê¸¸ê³  ê¸¸ì—ˆë˜ ë¶€ìŠ¤íŠ¸ìº í”„ AI Tech 5ê¸°ì˜ ëª¨ë“  ì¼ì •ì´ ì¢…ë£Œëë‹¤.\në¶€ìŠ¤íŠ¸ìº í”„ ì „ì²´ì— ëŒ€í•œ íšŒê³ ë„ í•˜ë©´ì„œ, ê¸°ì—…ì—°ê³„ í”„ë¡œì íŠ¸ì— ëŒ€í•œ íšŒê³ ë„ ë‹´ì•„ë³´ê³ ì í•œë‹¤.\nì²˜ìŒì—ëŠ” í•˜ë‚˜ì˜ í¬ìŠ¤íŠ¸ë¡œ ì‘ì„±í–ˆëŠ”ë°, ì“°ë‹¤ë³´ë‹ˆ ë„ˆë¬´ ê¸¸ì–´ì ¸ êµ¬ë¶„í•˜ì—¬ ê¸€ì„ ì‘ì„±í–ˆë‹¤.\nì™œ ë¶€ìŠ¤íŠ¸ìº í”„ ì¶”ì²œ íŠ¸ë™ì— ì§€ì›í–ˆë‚˜?\r#\rAI ë¶€ìŠ¤íŠ¸ìº í”„ ì§€ì›ìë¥¼ ëª¨ì§‘í•  ë‹¹ì‹œ(22ë…„ í•˜ë°˜ê¸°), ë‚˜ëŠ” ì¹´ì¹´ì˜¤ DS ìµœì¢…ë©´ì ‘ì„ ì¤€ë¹„í•˜ê³  ìˆì—ˆë‹¤.\n1ë…„ê°„ í†µê³„í•™ê³¼ì™€ ìˆ˜í•™ê³¼ë¥¼ ë„˜ë‚˜ë“¤ë©° ë‚˜ë¦„ AIì˜ ìˆ˜í•™ì  ì§€ì‹ê¸°ë°˜ì„ ìŒ“ì•˜(ë‹¤ê³  ìƒê°í–ˆ)ê³ , ì—¬ëŸ¬ ê²½ì§„ëŒ€íšŒë¥¼ ê²½í—˜í•˜ê³  AI ê´€ë ¨ ê³¼ëª©ì„ ìˆ˜ê°•í•˜ë©´ì„œ ì ì  ìì‹ ê°ì´ ì°¨ì˜¤ë¥´ë˜ ì‹œê¸°ì˜€ë‹¤.\ní•´ë‹¹ ì‹œì ì— ì±„ìš© í”„ë¡œì„¸ìŠ¤ë¥¼ ë¯¸ë¦¬ ê²½í—˜í•´ë³´ìëŠ” ì·¨ì§€ì—ì„œ ì¡¸ì—…ì€ ì—¼ë‘ì— ë‘ì§€ ì•Šì€ ì±„ë¡œ ì¹´ì¹´ì˜¤ ê³µì±„ì— ì§€ì›í–ˆë‹¤. ML ì‘ìš©ë¶„ì„ê³¼ ì¶”ì²œì‹œìŠ¤í…œ ì§ë¬´ì¤‘ ì„ íƒí•´ì„œ ì§€ì›í•  ìˆ˜ ìˆì—ˆëŠ”ë°, ë‚˜ëŠ” ì´ ë•Œ ì²˜ìŒìœ¼ë¡œ ì¶”ì²œì‹œìŠ¤í…œì´ë¼ëŠ” ìš©ì–´ë¥¼ ì ‘í–ˆë‹¤.\nì´í›„, ì¹´ì¹´ì˜¤ ìµœì¢…ë©´ì ‘ì—ì„œë„ ì¶”ì²œì‹œìŠ¤í…œê³¼ ê´€ë ¨ëœ ì§ˆë¬¸ì´ ë“¤ì–´ì™”ìœ¼ë‚˜ ì œëŒ€ë¡œ ëŒ€ë‹µí•˜ì§€ ëª»í–ˆë‹¤.\nê·¸ ë‹¹ì‹œì—ëŠ” ë³„ ìƒê° ì—†ì´ ì§€ì›í–ˆë‹¤ê°€ ìµœì¢… ë©´ì ‘ê¹Œì§€ ì˜¬ë¼ê°€ê²Œ ë˜ë©° ì ì  ê°„ì ˆí•´ì¡Œì—ˆìœ¼ë‚˜ ì‹¤ë ¥ì´ ë§ì´ ë¶€ì¡±í–ˆë‹¤.\në‚˜ëŠ” AI ê°œë°œìë¥¼ ìì²˜í–ˆì§€ë§Œ, AI ëª¨ë¸ì€ ìœ ëª…í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ í™œìš©í•˜ì—¬ ê°€ì ¸ë‹¤ ì“°ëŠ” ê²ƒì´ ì „ë¶€ì˜€ê³ , ëª¨ë¸ì˜ êµ¬ì¡°ì— ëŒ€í•œ ì´í•´ë„ ì—†ì—ˆê³ , ëª¨ë¸ì„ ì§ì ‘ êµ¬í˜„í•  ëŠ¥ë ¥ë„ ì—†ì—ˆë‹¤.\në˜í•œ ëŒ€íšŒ ê²½í—˜ë„ ë§ì§€ ì•Šì€ ìƒíƒœì˜€ê³  ìˆ˜ìƒ ê²½ë ¥ë„ ì—†ì—ˆê¸° ë•Œë¬¸ì— ë‚´ì„¸ìš¸ ê²ƒì´ ì „í˜€ ì—†ì—ˆë‹¤.\nì‹¬ì§€ì–´ ë©´ì ‘ë•Œ ì§ˆë¬¸ìœ¼ë¡œ ë‚˜ì™”ë˜ ì¶”ì²œì‹œìŠ¤í…œì— ëŒ€í•´ì„œëŠ” ê¸ˆì‹œì´ˆë¬¸ì´ì—ˆë‹¤. í•™êµì—ì„œ ë°°ìš¸ ìˆ˜ë„ ì—†ì—ˆê¸° ë•Œë¬¸ì— ë¶€ìº  AI Techì˜ ì»¤ë¦¬í˜ëŸ¼ì„ í™•ì¸í•˜ê³ ëŠ” ê³ ë¯¼ì—†ì´ ë°”ë¡œ ì¶”ì²œì‹œìŠ¤í…œì„ ì„ íƒí–ˆë‹¤.\nëŒ€íšŒ ê²½í—˜ì„ ì±„ìš°ê³  ëª¨ë¸ì„ ì§ì ‘ ë§Œë“¤ì–´ ì„œë¹„ìŠ¤ì— í¬í•¨í•˜ëŠ” ì—­ëŸ‰ì„ ê¸°ë¥´ê³  ì‹¶ì—ˆìœ¼ë©°, CVì™€ NLPëŠ” ë°°ìš¸ ìˆ˜ ìˆëŠ” ë°©ë²•ì´ ë§ë‹¤ê³  ìƒê°í–ˆê¸° ë•Œë¬¸ì´ë‹¤.\në¶€ìŠ¤íŠ¸ìº í”„ë¥¼ ì°¸ê°€í•˜ë©° ì–»ì–´ê°€ê³ ì í–ˆë˜ ê²ƒ (ì§€ì› ë™ê¸°)\r#\rì¸ë§¥ ëª¨ë¸ë§ ì—­ëŸ‰ ì¶”ì²œ ë„ë©”ì¸ ì •ë³´ ìŠµë“ ëŒ€íšŒ ê²½í—˜ end-to-end ML í”„ë¡œì íŠ¸ ê²½í—˜ Data Engineering ê²½í—˜ Front / Backend í”„ë ˆì„ì›Œí¬ì— ìµìˆ™í•´ì§€ê¸° 5ë²ˆë¶€í„°ëŠ” ìµœì¢… í”„ë¡œì íŠ¸ë¥¼ í†µí•´ ì–»ì–´ê°€ê³ ì í–ˆë˜ ê²ƒì¸ë°, ì›í•˜ëŠ” ë§Œí¼ ìŠµë“í•˜ì§€ëŠ” ëª»í•œ ê²ƒ ê°™ì•„ ì•„ì‰¬ì›€ì´ ìˆë‹¤.\nì–»ì–´ê°€ê³ ì í•˜ëŠ” ê²ƒì„ ë¶€ìŠ¤íŠ¸ì½”ìŠ¤ì˜ ì–´ë–¤ ê°•ì˜ì—ì„œ ë°°ì› ëŠ”ì§€ë¥¼ ëŒì´ì¼œë³´ë©´, ë‚´ê°€ ìƒê°í•˜ëŠ” ìµœê³ ì˜ ê°•ì˜ë„ ì‰½ê²Œ ë§í•  ìˆ˜ ìˆë‹¤.\në¶€ìŠ¤íŠ¸ìº í”„ AI Tech ê°•ì˜ Best 3\r#\r1. Pytorch (ìµœì„±ì²  ë§ˆìŠ¤í„°ë‹˜)\r#\rPytorch ê°•ì˜ë¥¼ í†µí•´ ëª¨í˜¸í•˜ê¸°ë§Œ í–ˆë˜ ëª¨ë¸ë§ ê³¼ì •ì„ ì„¸ë¶€ì ìœ¼ë¡œ ë°°ìš¸ ìˆ˜ ìˆê²Œ ë˜ì—ˆê³ , ì´ëŠ” í•™ìŠµì˜ ê¸°ë°˜ì´ ë˜ì–´ ì´ì œëŠ” ë…¼ë¬¸ì— ì–¸ê¸‰ëœ ëª¨ë¸ì˜ êµ¬ì¡°ë¥¼ ë³´ë©° ëª¨ë¸ì„ êµ¬í˜„í•  ìˆ˜ ìˆê²Œ ë˜ì—ˆë‹¤.\në¶€ë•ì´ì™€ í•¨ê»˜í•˜ëŠ” Pytorch ê³¼ì œë„ ë§ì€ ë„ì›€ì´ ëë‹¤.\nê°œì¸ì ìœ¼ë¡œëŠ” í•´ë‹¹ ê°•ì˜ë¥¼ í†µí•´ ê°€ì¥ í¬ê²Œ ë°œì „í–ˆë‹¤ê³  ìƒê°í•œë‹¤.\n2. Recsys (ì´ì¤€ì› ë§ˆìŠ¤í„°ë‹˜)\r#\rRecsys ê°•ì˜ì—ì„œëŠ” ì¶”ì²œ ëª¨ë¸ë“¤ì˜ ë°œì „ ìˆœì„œì— ë”°ë¼ í•µì‹¬ ê°œë…ë“¤ì„ ë°°ìš°ê³ , í° í‹€ì„ ì•Œë ¤ì¤€ë‹¤.\në˜í•œ Recsysì˜ ë‚´ìš©ì€ ì´í›„ì— ë‹¤ë¥¸ ê°•ì¢Œì—ì„œ ëª‡ ë²ˆì´ë‚˜ ë™ì¼í•œ ë‚´ìš©ì´ ë°˜ë³µëœë‹¤. ì´ ë•Œë¶€í„° ë‚˜ë„ ê·¸ë¬ì§€ë§Œ, ëª¨ë‘ê°€ ê°•ì˜ ì •ë¦¬ë¥¼ í˜ë“¤ì–´í–ˆë‹¤. ì´ì „ ê°•ì˜ì™€ ì¤‘ë³µë˜ëŠ” ë‚´ìš©ì´ ë§ì•„ êµ³ì´ ë‹¤ì‹œ ì •ë¦¬í•´ì•¼ í• ê¹Œ? ì‹¶ìœ¼ë©´ì„œë„, ë‹¤ë¥¸ ë‚´ìš©ì´ ê½¤ ë§ì•˜ê¸° ë•Œë¬¸ì— ê¸°ì¡´ì— ì •ë¦¬í•œ ë‚´ìš©ì„ ì˜ í™œìš©í•˜ëŠ” ê²ƒì´ ì¤‘ìš”í–ˆë‹¤.\nì¦‰, ì—°ê´€ ê°œë…ë“¤ì´ ëŠì„ì—†ì´ ê°€ì§€ì¹˜ê¸°ê°€ ë˜ë©° ê°•ì˜ê°€ ì§„í–‰ëëŠ”ë°, ë‚˜ëŠ” Recsys ê°•ì˜ë¥¼ í•µì‹¬ ì¤„ê¸°ë¡œ ì‚¼ì•„ ì´í›„ì˜ ê°•ì˜ ë‚´ìš©ë“¤ì„ ì¶”ê°€í•˜ë©° ì •ë¦¬í–ˆë‹¤.\në‚´ì‹¬ ì‹œê°„ì´ ì˜¤ë˜ ì§€ë‚¬ì„ ë•Œ, ì¶”ì²œê³¼ ê´€ë ¨í•˜ì—¬ ê¸°ì–µì— ë‚¨ëŠ” ë¶€ë¶„ì€ Recsys ê°•ì˜ì˜ íë¦„ ë¿ì´ë¼ëŠ” ìƒê°ë„ ìˆê¸° ë•Œë¬¸ì—, ë‚˜ëŠ” Recsys ê°•ì˜ë„ ì•„ì£¼ ë§ˆìŒì— ë“¤ì—ˆë‹¤.\n3. Product Serving (ë³€ì„±ìœ¤ ë§ˆìŠ¤í„°ë‹˜)\r#\rìµœì¢… í”„ë¡œì íŠ¸ë¥¼ ì „ê°œí•˜ëŠ”ë° í•„ìš”í•œ ë‹¤ì–‘í•œ ë°°ê²½ ì§€ì‹ì„ ì–»ì„ ìˆ˜ ìˆì—ˆë˜ ê°•ì˜.\ní­ë„“ì€ ë¶„ì•¼ë¥¼ ë‹¤ë£¨ëŠ” ë§Œí¼ ê¹Šê²Œ ë‹¤ë£¨ì§€ëŠ” ì•Šì§€ë§Œ, ê° ì˜ì—­ì„ ë” ì˜ ë°°ìš°ê³  í™œìš©í•  ìˆ˜ ìˆê²Œ ë§Œë“¤ì–´ì¤€ë‹¤. í•´ë‹¹ ê°•ì˜ë„ ì‹¤ë ¥ í–¥ìƒì— í¬ê²Œ ë„ì›€ì´ ë˜ì—ˆë‹¤.\në³´í†µ ì´ëŸ° ê°•ì˜ëŠ” ì‹œê°„ì´ ê¸¸ì–´ì§€ê³  ì§€ë£¨í•´ì§€ê¸° ì •ë§ ì‰½ì§€ë§Œ, ì•„ì£¼ ì§‘ì•½ì ìœ¼ë¡œ ê°•ì˜ê°€ êµ¬ì„±ë˜ì–´ìˆë‹¤ëŠ” ì ì´ ì¸ìƒ ê¹Šì—ˆë‹¤.\në‹¤ë§Œ ê°œì¸ì ìœ¼ë¡œëŠ” ê°•ì˜ ë‚´ìš©ì„ ì •ë¦¬í•˜ê¸°ê°€ ê½¤ ì–´ë ¤ì› ë‹¤. ë³€ì„±ìœ¤ ë§ˆìŠ¤í„°ë‹˜ì´ pptì— ê´‘ë²”ìœ„í•˜ê²Œ ì íŒ ë‚´ìš©ì„ ì„¸ë¶€ì ìœ¼ë¡œ ë°˜ë°•í•˜ê±°ë‚˜ ë³´ì¶©í•˜ë©° ê°•ì˜ë¥¼ ì „ê°œí•˜ëŠ” ê²½ìš°ê°€ ì¢…ì¢… ìˆê¸° ë•Œë¬¸ì¸ë°, ppt ì¤‘ì‹¬ì ìœ¼ë¡œ ê°•ì˜ ë‚´ìš©ì„ ì •ë¦¬í•˜ë©´ ë§ˆìŠ¤í„°ë‹˜ì´ ë§í•˜ê³ ì í•˜ëŠ” ë°”ê°€ ì œëŒ€ë¡œ ë‹´ê¸°ì§€ ì•Šì„ ë•Œê°€ ìˆë‹¤.\në¬¼ë¡  ê°•ì˜ ì£¼ì œê°€ ê°œë…ë³´ë‹¤ëŠ” êµ¬í˜„ì— ì´ˆì ì„ ë§ì¶”ì—ˆë‹¤ëŠ” ì ë„ ê°•ì˜ ë‚´ìš© ì •ë¦¬ê°€ ì–´ë ¤ìš´ ì´ìœ ì— í•´ë‹¹ëœë‹¤.\nëŒ€íšŒ ê²½í—˜\r#\rëŒ€íšŒ ê²½í—˜ë„ ë¶€ìŠ¤íŠ¸ìº í”„ë¥¼ í†µí•´ ì–»ì„ ìˆ˜ ìˆëŠ” ì•„ì£¼ í° ê²½í—˜ì¹˜ì´ë‹¤.\n9ì£¼ì°¨ë¶€í„° ì§„í–‰ë˜ëŠ” ì„¸ ë²ˆì˜ ëŒ€íšŒë¥¼ í†µí•´ ë°°ìš´ ì§€ì‹ì„ í™œìš©í•  ìˆ˜ ìˆì—ˆë‹¤.\nì•ì—ì„  ì–¸ê¸‰í•œ Recsys ê°•ì˜ë¥¼ ëë‚¸ ì§í›„, ë°°ìš´ ë‚´ìš©ì„ í¡ìˆ˜í•  ìˆ˜ ìˆë„ë¡ ëŒ€íšŒë¥¼ ì§„í–‰í•œ ê²ƒì´ ì•„ì£¼ ë§ˆìŒì— ë“¤ì—ˆë‹¤.\nì„¸ ë²ˆì˜ ëŒ€íšŒ ëª¨ë‘ Baselineì´ ì œê³µë˜ì—ˆê¸° ë•Œë¬¸ì—, Pytorchë¡œ ëª¨ë¸ì´ êµ¬í˜„ë  ë•Œì˜ í”„ë¡œì íŠ¸ êµ¬ì¡°ì— ìµìˆ™í•´ì§ˆ ìˆ˜ ìˆì—ˆë‹¤.\nì²« ë²ˆì§¸ ëŒ€íšŒ: ì±… í‰ì  ì˜ˆì¸¡\r#\rì²˜ìŒìœ¼ë¡œ ìˆ˜í–‰í•˜ê²Œ ëœ ëŒ€íšŒ. 2ì£¼ ë™ì•ˆ ì§„í–‰ëë‹¤.\nëŒ€íšŒì˜ ëª©í‘œëŠ” ì‚¬ìš©ìì˜ ì±…ì— ëŒ€í•œ í‰ì ì„ ì˜ˆì¸¡í•˜ëŠ” ê²ƒì¸ë°, ë‹¨ìˆœíˆ ìœ ì €-ì•„ì´í…œ ìƒí˜¸ì‘ìš© ì •ë³´ ë¿ë§Œ ì•„ë‹ˆë¼ ì±…ì˜ í‘œì§€ë¶€í„° ì±…ì— ëŒ€í•œ ë‹¤ì–‘í•œ ì •ë³´ê°€ í•¨ê»˜ ì œê³µë˜ê¸° ë•Œë¬¸ì—, CARs(ë§¥ë½ ê¸°ë°˜ ì¶”ì²œì‹œìŠ¤í…œ)ì„ ì˜ í™œìš©í•´ì•¼ í•˜ëŠ” ëŒ€íšŒì˜€ë‹¤.\ní•´ë‹¹ ëŒ€íšŒë¥¼ í†µí•´ FM, FFM, WDN, DeepCoNN, GBDT ë“± ë‹¤ì–‘í•œ ëª¨ë¸ì˜ êµ¬ì¡°ë¥¼ ë¹„êµí•˜ê³  ì´í•´í•  ìˆ˜ ìˆì—ˆë‹¤.\ní‰ì ë³„ ì ìˆ˜ ë¶„í¬ë¥¼ í™•ì¸í•¨ìœ¼ë¡œì¨ ê° ëª¨ë¸ì˜ ì˜ˆì¸¡ì´ ì–¼ë§ˆë‚˜ ìƒì´í•œ ì§€ë¥¼ íŒŒì•…í•  ìˆ˜ ìˆì—ˆê³ , ì´ëŠ” ì•™ìƒë¸”ì„ ìˆ˜í–‰í•  ë•Œ í° ë„ì›€ì´ ë˜ì—ˆë‹¤.\në¬´ì—‡ë³´ë‹¤, ëª¨ë¸ë“¤ì„ ì§ì ‘ êµ¬í˜„í•˜ë©° êµ¬ë¦„ì²˜ëŸ¼ ë‘¥ë‘¥ ë– ë‹¤ë‹ˆë˜ Pytorch ê°•ì¢Œì˜ ë‚´ìš©ì„ ì œëŒ€ë¡œ ìŠµë“í•  ìˆ˜ ìˆì—ˆë‹¤.\nëŒì´ì¼œë³´ë©´ ì§§ì€ ê¸°ê°„ì¸ ë§Œí¼ ê°€ì¥ ì§‘ì•½ì ìœ¼ë¡œ ìˆ˜í–‰í•œ ëŒ€íšŒë¼ê³  ìƒê°í•œë‹¤.\nPreprocess, Bagging, Boosting, ensemble, Stacking, Hybrid Model, Postprocess(min max í™•ì¸í•˜ê¸°), Visualization, CV\në‘ ë²ˆì§¸ ëŒ€íšŒ: DKT\r#\rì²« ë²ˆì§¸ ëŒ€íšŒì™€ ì•½ê°„ì˜ ê°„ê²©ì„ ë‘ê³  ì§„í–‰ëœ ë‘ ë²ˆì§¸ ëŒ€íšŒ.\nì‚¬ìš©ìê°€ í‘¼ ë¬¸ì œ ê¸°ë¡ì„ í•™ìŠµí•˜ì—¬ íŠ¹ì • ë¬¸ì œë¥¼ ë§íì§€, í‹€ë¦´ì§€ ì˜ˆì¸¡í•˜ëŠ” ëŒ€íšŒì´ë‹¤.\nìˆœì°¨ ë°ì´í„°ì…‹ìœ¼ë¡œ ë³€ê²½ë˜ë©´ì„œ ì´ì „ ëŒ€íšŒì—ì„œ ì“°ë˜ ëª¨ë¸ì˜ ëŒ€ë¶€ë¶„ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ì—ˆê¸° ë•Œë¬¸ì— ëŒ€íšŒê°€ ê½¤ ì–´ë µê²Œ ëŠê»´ì¡Œë‹¤.\në‚˜ëŠ” í•´ë‹¹ ëŒ€íšŒì—ì„œ ë°ì´í„°ë¥¼ ë‹´ë‹¹í•˜ì—¬ ì „ì²˜ë¦¬ì™€ EDAë¥¼ í†µí•´ íŒŒìƒë³€ìˆ˜ë¥¼ ì”ëœ© ìƒì„±í•˜ê³ , ìƒì„±í•œ ë³€ìˆ˜ë“¤ì— ëŒ€í•´ ë‹¤ì–‘í•œ ë³€ìˆ˜ì„ íƒ ê¸°ë²•ê³¼ ì°¨ì›ì¶•ì†Œ ê¸°ë²•ì„ ì ìš©í•˜ì—¬ LightGBMì˜ ì„±ëŠ¥ì„ ê°œì„ í–ˆë‹¤.\në¹„ë¡ ìµœì¢… ì•™ìƒë¸”ì—ì„œ LGBMì´ Transformerì™€ ì„±ëŠ¥ ì°¨ì´ê°€ ì‹¬í•´ ê°€ì¤‘ì¹˜ë¥¼ ë§ì´ ë¶€ì—¬í•˜ì§€ëŠ” ëª»í–ˆì§€ë§Œ, ì¼ë°˜í™” ì„±ëŠ¥ì„ ë†’ì´ëŠ” ë° ì¼ì¡°í–ˆë‹¤ê³  ìƒê°í•œë‹¤.\nì„¸ ë²ˆì§¸ ëŒ€íšŒ: Movie Rec\r#\rë‘ë²ˆì§¸ ëŒ€íšŒê°€ ëë‚˜ìë§ˆì ì§„í–‰ëœ ë§ˆì§€ë§‰ ëŒ€íšŒ.\nì´ì „ê³¼ ë™ì¼í•˜ê²Œ ì˜í™” ì‹œì²­ ê¸°ë¡ì„ í•™ìŠµí•˜ì—¬ ë‹¤ìŒì— ë³¼ ì˜í™”ë¥¼ ì˜ˆì¸¡(ì¶”ì²œ)í•˜ëŠ” ëŒ€íšŒì´ë‹¤.\në‹¤ë§Œ DKTì™€ ë‹¤ë¥¸ ì ì€ ë§ˆì§€ë§‰ ì˜í™”ë§Œ ì˜ˆì¸¡í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼ ì¤‘ê°„ ì¤‘ê°„ì— ë¹„ì–´ ìˆëŠ” ì˜í™”ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ê²ƒì´ ëª©í‘œì˜€ë‹¤.\níŒ€ì›ë“¤ì˜ ë©´ì ‘ ì´ìŠˆë„ ìˆì—ˆê³ , ìµœì¢… í”„ë¡œì íŠ¸ë„ ì•ë‘” ìƒí™©ì´ë¼ ë§ì´ ì§‘ì¤‘í•˜ì§€ëŠ” ëª»í•œ ëŒ€íšŒì˜€ìœ¼ë‚˜, Recbole ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ í™œìš©í•´ë³¸ ê²ƒì´ ìœ ì˜ë¯¸í–ˆë‹¤.\nëŒ€íšŒ í›„ê¸°\r#\rëª¨ë¸ë³„ë¡œ ì˜ˆì¸¡ ì¶”ì´ê°€ ì–´ë–»ê²Œ ë‹¤ë¥¸ì§€ë¥¼ íŒŒì•…í•˜ëŠ” ê²ƒì´ ì•™ìƒë¸”ì„ í†µí•œ ì„±ëŠ¥ ê°œì„ ì— í¬ê²Œ ë„ì›€ì´ ë˜ì—ˆë‹¤. í‰ê°€ ë©”íŠ¸ë¦­ì„ ë‹¤ì–‘í•˜ê²Œ í™•ì¸í•´ë„ ì¢‹ê³ , ì‹œê°í™”ê°€ ê°€ëŠ¥í•˜ë‹¤ë©´ ì¶”ë¡  ê²°ê³¼ë¥¼ ì‹œê°ì ìœ¼ë¡œ í™•ì¸í•˜ì—¬ ë¶„í¬ì˜ ì°¨ì´ë¥¼ í™•ì¸í•˜ëŠ” ê²ƒë„ ìœ ì˜ë¯¸í•˜ë‹¤ê³  ëŠê»´ì¡Œë‹¤.\në˜í•œ, íŒŒìƒ ë³€ìˆ˜ë¥¼ ì—´ì‹¬íˆ ìƒì„±í•´ë‚´ëŠ” ê²ƒë³´ë‹¤ ëª¨ë¸ì„ ë³€ê²½í•˜ê±°ë‚˜ ëª¨ë¸ì˜ êµ¬ì¡°ë¥¼ ì‚´ì§ ë³€ê²½í•˜ëŠ” ê²ƒì´ ì„±ëŠ¥ì„ í›¨ì”¬ í¬ê²Œ ëŒì–´ì˜¬ë¦¬ëŠ” ê²½ìš°ë„ ë§ì•˜ê¸° ë•Œë¬¸ì—, ì‚¬ìš©í•˜ëŠ” ëª¨ë¸ì„ ì •í™•íˆ ì´í•´í•˜ëŠ” ê²ƒì´ ì¤‘ìš”í–ˆë‹¤.\nëŒ€íšŒì˜ ìˆœìœ„ì— ì–½ë§¤ì´ë‹¤ ë³´ë©´ ê°•ì˜ì— ì†Œí™€í•´ì§€ê¸°ê°€ ì‰¬ì› ê³ , ìˆœìœ„ê°€ ë†’ì€ ìƒíƒœì´ê±°ë‚˜ ëŒ€íšŒ ê¸°ê°„ì´ 4ì£¼ì¼ ë•ŒëŠ” ë§ˆìŒì— ì—¬ìœ ê°€ ìƒê²¨ ì‹œê°„ì„ ì•Œëœ°í•˜ê²Œ ì“°ê¸°ê°€ ì–´ë ¤ì› ë‹¤.\në¬´ì—‡ë³´ë‹¤ ê°•ì˜ê°€ ëŒ€íšŒ ê¸¸ë¼ì¡ì´ì²˜ëŸ¼ êµ¬ì„±ë˜ì–´ìˆê¸° ë•Œë¬¸ì— ê°•ì˜ì—ì„œ ì–¸ê¸‰ë˜ëŠ” ë‚´ìš©ë“¤ì„ ìµœëŒ€í•œ í™œìš©í•˜ëŠ” ë°©í–¥ìœ¼ë¡œ ëŒ€íšŒë¥¼ ì§„í–‰í•˜ëŠ” ê²ƒì´ í›¨ì”¬ ë” ë„ì›€ì´ ëë‹¤.\nì´í›„ì— ì§„í–‰ëœ í”„ë¡œì íŠ¸ëŠ” ë‹¤ìŒ ê¸€ì—ì„œ ë§ˆì € í›„ê¸°ë¥¼ ì‘ì„±í•˜ê² ë‹¤.\n"},{"id":41,"href":"/posts/2023-09-04-%EB%84%A4%EC%9D%B4%EB%B2%84-%EB%B6%80%EC%8A%A4%ED%8A%B8%EC%BA%A0%ED%94%84-AI-Tech-%ED%9A%8C%EA%B3%A0-2-Upstage-%EA%B8%B0%EC%97%85%EC%97%B0%EA%B3%84-%ED%94%84%EB%A1%9C%EC%A0%9D%ED%8A%B8/","title":"ë„¤ì´ë²„ ë¶€ìŠ¤íŠ¸ìº í”„ AI Tech íšŒê³  2 â€” Upstage ê¸°ì—…ì—°ê³„ í”„ë¡œì íŠ¸","section":"Blog","content":"\rìµœì¢… í”„ë¡œì íŠ¸ëŠ” ë³€ì„±ìœ¤ ë§ˆìŠ¤í„°ë‹˜(ìœ íŠœë²„ ì¹´ì¼ìŠ¤ì¿¨)ì˜ Product Serving ê°•ì¢Œì™€ í•¨ê»˜ ì§„í–‰ëë‹¤.\në‚´ê°€ ê°•ì˜ë¥¼ í†µí•´ ìµœì¢… í”„ë¡œì íŠ¸ì˜ íë¦„ì— ëŒ€í•´ ì •ë¦¬í•œ ë°”ë¡œëŠ” AIë¥¼ í™œìš©í•˜ëŠ” End-to-End ì„œë¹„ìŠ¤ë¥¼ êµ¬ì¶•í•˜ëŠ” ê²ƒì´ 1ì°¨ ëª©í‘œì´ê³ , 2ì°¨ ëª©í‘œë¡œëŠ” ì‚¬ìš©ì í”¼ë“œë°±ì„ ë°›ì•„ ì„œë¹„ìŠ¤ë¥¼ ê°œì„ í•˜ëŠ” ê²ƒì´ë‹¤.\në‹¤ë§Œ ìš°ë¦¬ íŒ€ì€ ê¸°ì—… ì—°ê³„ í”„ë¡œì íŠ¸ë¥¼ ì§„í–‰í•˜ì—¬ AI í™œìš© ì„œë¹„ìŠ¤ë¥¼ ê°œë°œí•˜ëŠ” ê²ƒì— ì´ˆì ì„ ë‘ì§€ ì•Šì•˜ë‹¤.\nì—…ìŠ¤í…Œì´ì§€ ê¸°ì—…ì—°ê³„ í”„ë¡œì íŠ¸\r#\rì „ì²´ íŠ¸ë™ì—ì„œ ì§€ì›ì„ ë°›ì•„ ìµœì¢…ì ìœ¼ë¡œ ë½‘íŒ íŒ€ì´ ê¸°ì—…ì´ ì œì•ˆí•œ ì£¼ì œë¡œ í”„ë¡œì íŠ¸ë¥¼ ì§„í–‰í•  ìˆ˜ ìˆë‹¤.\nì§€ì› ë°°ê²½\r#\rê¸°ì—°í”„ì— ì§€ì›í•˜ê¸° ìœ„í•´ì„  íŒ€ì› ëª¨ë‘ê°€ ê¸°ì—…ì—ì„œ ì œì•ˆí•œ í”„ë¡œì íŠ¸ ì£¼ì œê°€ ë§ˆìŒì— ë“¤ì–´ì•¼ í•œë‹¤.\nìš°ë¦¬ íŒ€ì€ ëŒ€í™”í˜• ì¶”ì²œ ì‹œìŠ¤í…œì˜ ëª¨ë‹ˆí„°ë§ì„ ìœ„í•œ interactive admin í˜ì´ì§€ (web) ê°œë°œ ì´ë¼ëŠ” ì£¼ì œì— ëª¨ë‘ ë™ì˜í•˜ì—¬ ê¸°ì—… ì—°ê³„ í”„ë¡œì íŠ¸ë¥¼ ì‹ ì²­í–ˆë‹¤.\nì• ì´ˆì— ëŒ€í™”í˜• ë°ì´í„°ì…‹ì„ í™œìš©í•˜ëŠ” ì„œë¹„ìŠ¤ë¥¼ ê³„íší•˜ê¸°ë„ í–ˆê³ , ìš°ë¦¬ íŒ€ì€ DAë‚˜ DSì— ê´€ì‹¬ ìˆëŠ” íŒ€ì›ì´ ë§ì•˜ê¸° ë•Œë¬¸ì— ì•„ì£¼ ë§¤ë ¥ì ì¸ ì„ íƒì§€ì˜€ë‹¤.\nê²Œë‹¤ê°€ íŒ€ ì „ì›ì´ Looker Studioë¥¼ í™œìš©í•˜ì—¬ ëŒ€ì‹œë³´ë“œë¥¼ ë§Œë“œëŠ” ë²•ì„ ì•Œê³  ìˆì—ˆê¸° ë•Œë¬¸ì—, ì§€ì› ê¸°ê°„ ë™ì•ˆ ëŒ€ì‹œë³´ë“œ í”„ë¡œí† íƒ€ì…ì„ ì™„ì„±í•˜ì—¬ ê¸°ì—°í”„ ì‹ ì²­ì„œì— í•¨ê»˜ ì²¨ë¶€í–ˆë‹¤.\në•ë¶„ì— ì›í•˜ëŠ” ëŒ€ë¡œ ê¸°ì—… ì—°ê³„ í”„ë¡œì íŠ¸ë¥¼ ì§„í–‰í•  ìˆ˜ ìˆì—ˆë‹¤.\nê¸°ì—… ì—°ê³„ í”„ë¡œì íŠ¸ì˜ ì¥ì \r#\r1. ì‹œë‹ˆì–´ ê°œë°œìì—ê²Œ ë¬´í•œëŒ€ì˜ ì§ˆë¬¸ ê¶Œí•œì´ ì£¼ì–´ì§„ë‹¤.\r#\rì‚¬ì‹¤ ë‚˜ëŠ” ì´ ì  í•˜ë‚˜ë§Œ ë†“ê³  ë³´ë”ë¼ë„, ê¸°ì—… ì—°ê³„ì˜ ë©”ë¦¬íŠ¸ëŠ” ì¶©ë¶„í•˜ë‹¤ê³  ìƒê°í•œë‹¤.\ní”„ë¡œì íŠ¸ë¥¼ ì§„í–‰í•˜ë©° ëª¨ë¥´ëŠ” ì ì´ë‚˜, ê¸°ìˆ ì ìœ¼ë¡œ ì–´ë ¤ìš´ ë¶€ë¶„ì´ ë°œìƒí–ˆì„ ê²½ìš° ë…¸ì…˜ í˜ì´ì§€ë¥¼ í†µí•´ ë°”ë¡œ ì§ˆë¬¸í•  ìˆ˜ ìˆë„ë¡ ê³µê°„ì´ ë§ˆë ¨ëœë‹¤.\nì´ ì¥ì ì€ í›„ì— ë¶€ìŠ¤íŠ¸ìº í”„ì—ì„œ ì§„í–‰í•˜ëŠ” í”„ë¡œì íŠ¸ ê³µê°œ ë©˜í† ë§ ì‹œê°„ì— ì²´ê°í–ˆì—ˆëŠ”ë°, ë‹¤ë¥¸ íŒ€ë“¤ì€ ë©˜í† ë§ 1ì‹œê°„ë™ì•ˆ ì§ˆë¬¸ì„ í†µí•´ ì–»ì–´ê°€ëŠ” ë‚´ìš©ë“¤ì„ ìš°ë¦¬ íŒ€ì€ ë¯¸ë¦¬ ë‹¤ ë“¤ì–´ì„œ ì•Œê³  ìˆì—ˆë‹¤.\n2. ë‘ ë²ˆì˜ ë¯¸íŒ… ê¸°íšŒê°€ ì œê³µëœë‹¤.\r#\rí•œ ë²ˆì€ ìƒê²¬ë¡€ì´ê³  í•œ ë²ˆì€ ë©˜í† ë§ì´ì§€ë§Œ, ì„œë¡œ ì–¼êµ´ ë§ëŒ€ê³  ì›í•˜ëŠ” ì§ˆë¬¸ì„ ê³µìœ í•  ìˆ˜ ìˆëŠ” ìë¦¬ë¼ëŠ” ì ì—ì„œ, ìƒê²¬ë¡€ ë˜í•œ ë§ì€ ì •ë³´ë¥¼ ì–»ì„ ìˆ˜ ìˆë‹¤.\në‹¨, ìƒê²¬ë¡€ë¥¼ ì§„í–‰í•˜ê¸° ì „ì— êµ¬ì²´ì ì¸ ì§ˆë¬¸ì„ ë§ì´ ì¤€ë¹„í•´ê°€ì•¼ ë” ë§ì€ ë‹µë³€ì„ ì–»ì„ ìˆ˜ ìˆë‹¤.\nìƒê²¬ë¡€ ë¯¸íŒ… ë•Œ ê¸°ì—…ì—ì„œ í”„ë¡œì íŠ¸ì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ê³ , ì–´ë–¤ ì—…ë¬´ë¥¼ ìˆ˜í–‰í•´ì•¼ í•˜ëŠ”ì§€ ì„¤ëª…í•´ì¤„ ê²ƒì´ë¼ ìƒê°í•˜ë©´ ì•ˆëœë‹¤.\nìš°ë¦¬ íŒ€ì˜ ê²½ìš° ì§ˆë¬¸ì„ 5ê°œ ì •ë„ ì¤€ë¹„í–ˆëŠ”ë°, ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì´ ì™„ë£Œëœ í›„ 15ë¶„ ë§Œì— ëë‚˜ë²„ë ¸ì—ˆë‹¤. ìµœëŒ€ 1ì‹œê°„ì˜ ì‹œê°„ì´ ì£¼ì–´ì§„ ê²ƒì„ ê°ì•ˆí•˜ë©´ ë” ë§ì€ ì§ˆë¬¸ì„ ì¤€ë¹„í•´ê°€ì§€ ì•Šì€ ê²ƒì´ ì‚´ì§ ì•„ì‰¬ì› ë‹¤.\në©˜í† ë§ì€ ëª¨ì˜ ë©´ì ‘ í˜•ì‹ìœ¼ë¡œ ì§„í–‰ë˜ì—ˆê³ , ë‚˜ì˜ ì„±ì¥ì—ëŠ” í° ë„ì›€ì´ ë˜ì—ˆì§€ë§Œ ì•„ì‰½ê²Œë„ í”„ë¡œì íŠ¸ì—ëŠ” ë„ì›€ì´ ë˜ì§€ ì•Šì•˜ë‹¤.\në©˜í† ë§ ì „ì— ì œì¶œí•œ êµ¬ì„±ì› ì—­í•  í˜ì´ì§€ë¥¼ ë³´ë©° ì–´ë–¤ ì–´ë ¤ì›€ì´ ìˆì—ˆëŠ”ì§€, ì–´ë–¤ ë°©ì‹ìœ¼ë¡œ ë¬¸ì œë¥¼ í•´ê²°í–ˆëŠ”ì§€ ë“±ì„ ì—¬ì­¤ë³´ì‹œë©° ë©´ì ‘ í˜•ì‹ìœ¼ë¡œ ì§„í–‰ëë‹¤.\në‚˜ëŠ” ë©´ì ‘ ê²½í—˜ì´ ë¶€ì¡±í–ˆê¸° ë•Œë¬¸ì— í•´ë‹¹ ì‹œê°„ì€ ë‚˜ì—ê²Œ í° ì˜ë¯¸ê°€ ìˆì—ˆë‹¤. (ë‹¤ë§Œ ì‚¬ì „ ì˜ˆê³  ì—†ì´ ê°‘ì‘ìŠ¤ëŸ½ê²Œ ì§„í–‰ëœ í„°ë¼ ë©´ì ‘ ì¤€ë¹„ë¥¼ í•˜ì§€ ì•Šê³  ì§„í–‰ëœ ê²ƒì€ ì•„ì‰¬ì› ë‹¤.)\nê·¸ë˜ë„ ì‹¤ì œ ë©´ì ‘ì—ì„œëŠ” í”¼ë“œë°±ì„ ì–»ì„ ìˆ˜ ì—†ê¸° ë•Œë¬¸ì—, ì‹œë‹ˆì–´ ê°œë°œìì˜ í”¼ë“œë°±ì´ ì¡´ì¬í•˜ëŠ” ëª¨ì˜ ë©´ì ‘ì€ ì—„ì²­ë‚œ ê¸°íšŒì˜€ë‹¤ê³  ìƒê°í•œë‹¤.\në©´ì ‘ì—ì„œ ë°›ì€ í”¼ë“œë°±\nì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ í•˜ê¸° ì „ì— ìƒê°ì„ ì •ë¦¬í•œ í›„ ì—¬ìœ ë¥¼ ê°€ì§€ê³  ì²œì²œíˆ ë‹µë³€í•˜ì.\nì”ì”í•˜ê²Œ, ëŠë¦¬ê²Œ, ê¹”ë”í•˜ê²Œ, ì°¨ë¶„í•˜ê²Œ ë§í•˜ê¸°\ní”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ì— ëŒ€í•´ ê³µë¶€í•´ë³´ê¸°. í”„ë¡œí† íƒ€ì…ì„ ë¹ ë¥´ê²Œ ë§Œë“¤ì–´ë‚´ëŠ” ëŠ¥ë ¥ì´ ëŒ€ë‹¨íˆ ì¤‘ìš”í•˜ë‹¤. 3. ì—°ê³„ ê¸°ì—…ì— ì–¼êµ´ ë„ì¥ì„ ì°ì„ ìˆ˜ ìˆë‹¤.\r#\rì‚¬ì‹¤ ì´ ë¶€ë¶„ì„ ëª…í™•í•˜ê²Œ ì–¸ê¸‰í•˜ê¸°ê°€ ì• ë§¤í•´ì„œ ì´ë ‡ê²Œ ì ì—ˆë‹¤. ì•„ë¬´ë˜ë„ í”„ë¡œì íŠ¸ë¥¼ ì‚¬ì „ì— í•¨ê»˜ ì§„í–‰í•˜ê³ , ì‹¬ì§€ì–´ ì˜ ì§„í–‰í–ˆë‹¤ë©´ ë‹¹ì—°íˆ ì±„ìš©ì—ì„œ ì´ì ì´ ìˆì§€ ì•Šì„ê¹Œ?\n4. ê¸°ì—…ì—ì„œ í•„ìš”í•œ í”„ë¡œì íŠ¸ì— ëŒ€í•œ ê²½í—˜\r#\rì¥ì ì´ì ë‹¨ì ì¼ ìˆ˜ ìˆëŠ”ë°, ë‚˜ëŠ” í”„ë¡œì íŠ¸ë¥¼ ì§„í–‰í•˜ë©° ë¶€ìº ì—ì„œ ë°°ìš´ ê²ƒê³¼ ê¸°ì—…ì—ì„œ í•„ìš”ë¡œ í•˜ëŠ” ê²ƒì´ ë‹¤ë¥´ë‹¤ëŠ” ëŠë‚Œì„ ë°›ì•˜ë‹¤.\nì‹¤ì œë¡œ ì¼ì„ í•˜ê²Œ ëì„ ë•Œë„ ë™ì¼í•˜ê²Œ ê²ªì„ ìˆ˜ ìˆëŠ” ë¬¸ì œ ìƒí™©ì´ë¼ ìƒê°í•˜ê¸° ë•Œë¬¸ì—, ì¢‹ì€ ê²½í—˜ì„ ìŒ“ì•˜ë‹¤ê³  ìƒê°í•œë‹¤.\nê¸°ì—… ì—°ê³„ í”„ë¡œì íŠ¸ì˜ ë‹¨ì \r#\rë‚˜ëŠ” í•´ë‹¹ ê¸€ì„ ë‚˜ì˜ í”„ë¡œì íŠ¸ ê²½í—˜ì„ ë°”íƒ•ìœ¼ë¡œ ì‘ì„±í–ˆê¸° ë•Œë¬¸ì— ë‚˜ì¤‘ì—ëŠ” ìƒí™©ì´ ë°”ë€Œê²Œ ë  ìˆ˜ë„ ìˆë‹¤.\nê·¸ì € ë¶€ìº  AI Tech 5ê¸° ë•Œì˜ ê¸°ì—… ì—°ê³„ í”„ë¡œì íŠ¸ ê²½í—˜ì€ ì´ë¬êµ¬ë‚˜ í•˜ê³  ë„˜ì–´ê°€ë©´ ì¢‹ê² ë‹¤.\n1. ììœ ë¡œìš´ ë“¯ ììœ ë¡­ì§€ ì•Šì€ í”„ë¡œì íŠ¸ ë°©í–¥ì„±\r#\rê¸°ì—… ì—°ê³„ í”„ë¡œì íŠ¸ë¼ê³  í•˜ì—¬ ìˆ˜í–‰í•´ì•¼ í•˜ëŠ” ì—…ë¬´ê°€ ëª…í™•í•  ê²ƒìœ¼ë¡œ ê¸°ëŒ€í•˜ê³  ìˆì—ˆìœ¼ë‚˜ ê·¸ë ‡ì§€ ì•Šì•˜ë‹¤. ê¸°ì—…ì—ì„œëŠ” ì›í•˜ëŠ” ì•„ì´ë””ì–´ê°€ ìˆì—ˆìœ¼ë‚˜ ì„¸ë¶€ì ì¸ ë°©í–¥ì— ëŒ€í•´ì„œëŠ” ì´ì•¼ê¸°ë¥¼ ë“¤ì„ ìˆ˜ ì—†ì—ˆë‹¤.\në”°ë¼ì„œ ìš°ë¦¬ê°€ ê¸°ì—… ì—°ê³„ë¥¼ ì¤€ë¹„í•˜ë©° ì±™ê²¨ ê°„ êµ¬ì²´ì ì¸ ì•„ì´ë””ì–´ëŠ” ì‚¬ìš©í•  ìˆ˜ ì—†ì—ˆê³ , ê¸°ì—…ì—ì„œ ì œì•ˆí•œ ì•„ì´ë””ì–´ë¥¼ ì–´ë–¤ ë°©í–¥ìœ¼ë¡œ êµ¬ì²´í™”í•  ê²ƒì¸ì§€ ë‹¤ì‹œ ë…¼ì˜í•´ì•¼ë§Œ í–ˆë‹¤.\nê²Œë‹¤ê°€ ìš°ë¦¬ íŒ€ì˜ ê²½ìš° ìš´ì´ ë‚˜ì˜ê²Œë„ ì¼ì •ì´ ë§ì§€ ì•Šì•„ ìƒê²¬ë¡€ ë•Œì™€ ë©˜í† ë§ ë•Œ ì°¸ê°€í•˜ì‹  ê¸°ì—… ë©˜í† ë‹˜ì´ ë‹¬ëëŠ”ë°, ë‘ ë¶„ì˜ ì˜ê²¬ì´ ì„œë¡œ ê½¤ ë‹¬ë¼ì„œ í”„ë¡œì íŠ¸ ë§ˆê° ì§ì „ì— ì•„ì‰¬ì›€ì´ ì»¸ë‹¤.\n2. ì§€ì—°ë˜ëŠ” ì¼ì •\r#\rìµœì¢… í”„ë¡œì íŠ¸ëŠ” 4ì£¼ê°„ ì§„í–‰ëœë‹¤. ë°°ìš´ ë‚´ìš©ì„ ì‹¤í˜„í•˜ê¸°ì— ì´ë¯¸ ë¹ ë“¯í•œ ì‹œê°„ì´ë¼ ìƒê°ë˜ì§€ë§Œ, ê¸°ì—… ì—°ê³„ í”„ë¡œì íŠ¸ì— ì°¸ê°€í•˜ê²Œ ë˜ë©´ ì´ ì¼ì •ì€ ì¶”ê°€ë¡œ ì¡°ê¸ˆì”© ë°€ë¦°ë‹¤.\ní”„ë¡œì íŠ¸ì˜ ë°©í–¥ì„±ì„ ë…¼ì˜í•˜ê¸° ìœ„í•´ ìƒê²¬ë¡€ ìë¦¬ê°€ í•„ìš”í•˜ë‹¤ê³  ìƒê°í–ˆì—ˆìœ¼ë‚˜, ìƒê²¬ë¡€ ì¼ì •ì„ ê¸°ë‹¤ë¦¬ëŠë¼ í”„ë¡œì íŠ¸ êµ¬í˜„ì— ì£¼ì–´ì§„ ì‹œê°„ì´ 3ì£¼ë°–ì— ë‚¨ì§€ ì•Šê²Œ ë˜ì—ˆì—ˆë‹¤.\nì´ ë¬¸ì œì ì€ 1,3ë²ˆ ìƒí™©ê³¼ ê²°í•©ë˜ì–´ ìƒê²¬ë¡€ ì§í›„ íŒ€ì› ëª¨ë‘ê°€ ë§ì´ ê±±ì •í–ˆë‹¤.\n3. ì œê³µë˜ëŠ” ë°ì´í„° ì—†ìŒ\r#\rí”„ë¡œì íŠ¸ì˜ ì£¼ì œê°€ ë°ì´í„°ì— ëŒ€í•œ ëª¨ë‹ˆí„°ë§ íˆ´ ê°œë°œì´ ëª©ì ì´ì—ˆê¸° ë•Œë¬¸ì— ë‹¹ì—°íˆ ë¶„ì„ ëª©ì ì´ ë˜ëŠ” ë°ì´í„°ë¥¼ ì œê³µí•´ì¤„ ê²ƒì´ë¼ ìƒê°í–ˆë‹¤.\ní•˜ì§€ë§Œ ì œê³µëœ ë°ì´í„° ì—†ì´ ëª¨ë‹ˆí„°ë§ íˆ´ ê°œë°œì„ ìš”ì²­ ë°›ì•˜ê¸° ë•Œë¬¸ì—, ëŒ€ì‹œë³´ë“œë¥¼ ë²”ìš©ì ìœ¼ë¡œ ì‚¬ìš© ê°€ëŠ¥í•˜ë„ë¡ êµ¬ì„±í•´ì•¼ í•˜ëŠ”ì§€ íŠ¹ì • ë°ì´í„°ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë§Œë“¤ì–´ì•¼ í•  ì§€ í™•ì • ì§“ê¸°ê°€ ì–´ë ¤ì› ë‹¤.\në°ì´í„° í”„ë¼ì´ë²„ì‹œ ì¸¡ë©´ì—ì„œ ì–´ë ¤ì›€ì´ ìˆì—ˆê² ì§€ë§Œ, ë³´ì•ˆ ìœ ì§€ ì„œì•½ì„œì— ë™ì˜ë¥¼ í•˜ê³  ë°ì´í„°ë¥¼ ì œê³µ ë°›ì§€ ëª»í•œ ê²ƒì´ ì¢€ ì•„ì‰¬ì› ë‹¤. (ë°›ì€ ë°ì´í„°ê°€ ì—†ì–´ ìœ ì§€í•  ë³´ì•ˆì´ ì—†ì—ˆë‹¤.)\ní”„ë¡œì íŠ¸ ì§„í–‰\r#\rì´ë²ˆ í”„ë¡œì íŠ¸ì—ì„œ ë‚˜ëŠ” ì›¹ìœ¼ë¡œ ëŒ€ì‹œë³´ë“œë¥¼ êµ¬í˜„í•˜ëŠ” ì—…ë¬´ë¥¼ ë‹´ë‹¹í–ˆë‹¤.\nì²˜ìŒ í”„ë¡œí† íƒ€ì…ì„ ë§Œë“¤ ë• Looker Studioë¥¼ í™œìš©í–ˆìœ¼ë‚˜, ìš°ë¦¬ íŒ€ì—ì„œ í™œìš©í–ˆë˜ ëŒ€í™”í˜• ë°ì´í„°ëŠ” ë°˜ì •í˜• ë°ì´í„°ì´ê¸° ë•Œë¬¸ì— ììœ ë„ê°€ ë” ë†’ì€ ì‹œê°í™” íˆ´ì´ í•„ìš”í–ˆë‹¤.\nì•Œì•„ë³´ë‹ˆ Python ë¬¸ë²•ì„ ì§€ì›í•˜ëŠ” Dashë¼ëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ìˆì–´ ì´ë¥¼ í™œìš©í•˜ê¸°ë¡œ í–ˆë‹¤. DashëŠ” Plotlyì™€ Flaskë¥¼ í¬í•¨í•˜ê¸° ë•Œë¬¸ì— Plotlyë¡œ ê·¸ë¦° Figureì„ Dashì—ì„œ ë°”ë¡œ í™œìš©í•  ìˆ˜ ìˆì—ˆë‹¤.\në‚˜ëŠ” PM ì—­í• ë„ í•¨ê»˜ ìˆ˜í–‰í–ˆë‹¤. í”„ë¡œì íŠ¸ì˜ ë°©í–¥ì€ ê¸°ì—…ì—ì„œ ì›í•˜ëŠ” ëŒ€ë¡œ ì§„í–‰í–ˆê¸° ë•Œë¬¸ì— ë‚˜ëŠ” ê¹ƒí—ˆë¸Œë¥¼ ê´€ë¦¬í•˜ê³ , ë¬¸ì œ ìƒí™©ì„ ì •ë¦¬í•˜ê³  ì˜ê²¬ì„ ëª¨ìœ¼ëŠ” ê²ƒì— í˜ì¼ë‹¤.\në¸Œëœì¹˜ëŠ” Github flow ë°©ì‹ìœ¼ë¡œ ê´€ë¦¬í–ˆë‹¤. íŒ€ì›ë“¤ì´ ì•„ì§ ê¹ƒí—ˆë¸Œì— ì ì‘í•˜ëŠ” ë‹¨ê³„ì˜€ê¸° ë•Œë¬¸ì— ê·¸ë‚˜ë§ˆ ë” ì‰¬ìš´ ë°©ë²•ì„ ì ìš©í•˜ê¸°ë„ í–ˆê³ , ë¡œì»¬ ì¤‘ì‹¬ì¸ git flow ë°©ì‹ìœ¼ë¡œ ì½”ë“œë¥¼ ê´€ë¦¬í•˜ê²Œ ë˜ë©´ ì›ê²© ë ˆí¬ì§€í† ë¦¬ì— pushí•˜ëŠ” ê°„ê²©ì´ ë„ˆë¬´ ê¸¸ì–´ì§ˆ ê²ƒ ê°™ì•˜ê¸° ë•Œë¬¸ì´ë‹¤.\nê¸°ê°„ì´ ë„ˆë¬´ ì§§ì•˜ê¸° ë•Œë¬¸ì— ì¼ì •ì„ ê´€ë¦¬í•˜ê¸°ëŠ” ì‰¬ì› ë‹¤. ë‚´ê°€ PM ì—­í• ì„ ë§¡ì€ ì‹œì ì— ë‚¨ì€ ê¸°ê°„ì´ 2ì£¼ì˜€ê¸° ë•Œë¬¸ì—, í•œ ì£¼ ë§Œì— ë…¼ì˜ëœ ê¸°ëŠ¥ì— ëŒ€í•œ êµ¬í˜„ì„ ëë‚´ê³  ë‚¨ì€ í•œ ì£¼ ë™ì•ˆ ë¶€ê°€ ê¸°ëŠ¥ì„ ì¶”ê°€í–ˆë‹¤.\nê°œìš”\r#\rì ˆëŒ€ì ì¸ ì‹œê°„ì´ ë„ˆë¬´ë‚˜ ë¶€ì¡±í–ˆê¸° ë•Œë¬¸ì— ê¸°ëŠ¥ ë‹¨ìœ„ë¡œ ë…¼ì˜ë¥¼ í•˜ê³ , ë…¼ì˜ê°€ ëë‚˜ë©´ ë‚˜ëŠ” ë°”ë¡œ êµ¬í˜„ì— ë“¤ì–´ê°”ë‹¤. ë”°ë¼ì„œ ê¸°ëŠ¥ì„ ì¶”ê°€í•´ê°€ë©´ì„œ í˜ì´ì§€ì˜ ì»¨ì…‰ì„ ê³„ì† ë³€ê²½ì‹œì¼°ë‹¤.\ní”Œë¡¯ êµ¬ìƒ\nëŒ€ì‹œë³´ë“œì— ì–´ë–¤ í”Œë¡¯ì„ ì¶”ê°€í•  ê²ƒì¸ê°€ì— ëŒ€í•œ ë…¼ì˜ëŠ” í”„ë¡œì íŠ¸ê°€ ì§„í–‰ë˜ëŠ” 3ì£¼ ë‚´ë‚´ ìˆì—ˆë‹¤.\nê¸°ì—… ë©˜í† ë‹˜ê³¼ ë¶€ìŠ¤íŠ¸ìº í”„ ë©˜í† ë‹˜ê»˜ ê°ê° ì¡°ì–¸ì„ êµ¬í•˜ê¸°ë„ í•˜ê³ , ë…¼ë¬¸ë„ ì°¾ì•„ë³´ë©° ì¶”ê°€í•  ê¸°ëŠ¥ì„ ì„ íƒí–ˆë‹¤.\nêµ¬ìƒí•œ í”Œë¡¯ì„ Plotlyë¡œ ê·¸ë¦¬ê¸°\nplotly ìì²´ì ìœ¼ë¡œ ì§€ì›í•˜ëŠ” callbackì´ ì•„ì£¼ ë§ì•˜ê¸° ë•Œë¬¸ì— ëŒ€ì‹œë³´ë“œê°€ í’ë¶€í•´ë³´ì˜€ë‹¤.\nê¸°ë³¸ì ìœ¼ë¡œ ì¤Œ ì¸/ì•„ì›ƒ, ì„ íƒí•œ ì˜ì—­ë§Œ ë¶„í¬ í™•ì¸í•˜ê¸° ë“±ì˜ ê¸°ëŠ¥ì„ ì§€ì›í•œë‹¤.\nì„œë²„ì— í•¨ìˆ˜ í˜•íƒœë¡œ ì¶”ê°€í•˜ì—¬ Dashì—ì„œ fig ê°ì²´ ê·¸ë¦¬ê¸°\nPlotlyë¡œ fig ê·¸ë¦¬ê¸°ë¥¼ ì™„ë£Œí–ˆìœ¼ë©´ ì„œë²„ì—ì„œ callback í•¨ìˆ˜ë¥¼ í†µí•´ í˜¸ì¶œì´ ê°€ëŠ¥í•˜ë„ë¡ í•¨ìˆ˜ í˜•íƒœë¡œ ë³€ê²½í•´ì¤¬ë‹¤.\nì»¨íŠ¸ë¡¤ íŒ¨ë„ ì¶”ê°€í•˜ê³  fig ê°ì²´ì™€ ì—°ê²°í•˜ê¸°\ní”Œë¡¯ì˜ ê°’ì„ ë³€ê²½í•  ìˆ˜ ìˆëŠ” ì»¨íŠ¸ë¡¤ íŒ¨ë„(e.g. ë¼ë””ì˜¤ ë²„íŠ¼, ìŠ¬ë¼ì´ë”)ì„ ì¶”ê°€í•œ í›„, callback í•¨ìˆ˜ì˜ Inputìœ¼ë¡œ ë„˜ê²¨ì¤¬ë‹¤.\në””ìì¸ê³¼ ì‚¬ìš©ì„±ì„ ê³ ë ¤í•˜ì—¬ í”Œë¡¯ ë°°ì¹˜í•˜ê¸°\nê¸°ëŠ¥ì´ ì™„ì„±ëœ í”Œë¡¯ì˜ í¬ê¸°ë¥¼ ì¡°ì ˆí•˜ê³  ì£¼ì œê°€ ì¼ì¹˜í•˜ë„ë¡ ë°°ì¹˜í–ˆë‹¤.\nëŒ€í™”í˜• ì¶”ì²œ ë°ì´í„°\r#\rì´ë²ˆ í”„ë¡œì íŠ¸ì—ì„œ ê°€ì¥ ì¤‘ìš”í•œ ê²ƒì€ ëŒ€í™”í˜• ì¶”ì²œ ë°ì´í„°ë¡œë¶€í„° ì–´ë–¤ ì •ë³´ë¥¼ ë½‘ì•„ë‚´ì–´ ë³´ì—¬ì¤„ ìˆ˜ ìˆëŠ”ê°€ì— ëŒ€í•œ ê³ ë¯¼ì´ì—ˆë‹¤. ëŒ€í™”í˜• ì¶”ì²œ ë°ì´í„°ë¼ëŠ” í¬ë§· ìì²´ê°€ ìµìˆ™í•˜ì§€ ì•Šì•˜ê¸° ë•Œë¬¸ì—, í”„ë¡œì íŠ¸ë¥¼ ì§„í–‰í•˜ê¸° ìœ„í•´ì„  ìš°ì„  ëŒ€í™”í˜• ì¶”ì²œ ë°ì´í„°ì— ëŒ€í•´ì„œ ì˜ ì•Œì•„ì•¼ í–ˆë‹¤. ì´ë¥¼ ìœ„í•´ íŒ€ì› ì¤‘ í•œ ëª…ì´ ê´€ë ¨ ë…¼ë¬¸ì„ ë¦¬ë·°í–ˆìœ¼ë©°, ë¸”ë¡œê·¸ ë¦¬ë·°ë„ ì°¸ê³ í–ˆë‹¤.\nê²°ë¡ ì ìœ¼ë¡œ ë§í•˜ìë©´, ëŒ€ì‹œë³´ë“œ ì‚¬ìš©ìì—ê²Œ ë„ì›€ì´ ë˜ëŠ” í”Œë¡¯ì„ êµ¬ì„±í•˜ê¸° ìœ„í•´ ëŒ€í™”í˜• ë°ì´í„°ë¼ëŠ” ì ê³¼ ì¶”ì²œ ë°ì´í„°ë¼ëŠ” ì ì„ êµ¬ë¶„í–ˆë‹¤.\nëŒ€í™”ì˜ í’ˆì§ˆì„ ì•Œë ¤ì£¼ê¸° ìœ„í•´ ì–´íœ˜ì˜ ë‹¤ì–‘ì„±(Distinct n-gram), ëª¨ë¸ì´ í—·ê°ˆë¦¬ëŠ” ì •ë„(perplexity) ë“±ì˜ í‰ê°€ ê¸°ì¤€ì„ ì ìš©í–ˆìœ¼ë©°, ì¶”ì²œì˜ íš¨ê³¼ ë° íš¨ìœ¨ì€ Precisionê³¼ í•¨ê»˜ ê°ì • ë¶„ì„ì„ í†µí•´ ì„±ê³µì ì¸ ì¶”ì²œì¸ì§€ íŒë³„í•˜ì—¬ í†µê³„ì¹˜ë¥¼ ì œê³µí–ˆë‹¤.\në˜í•œ, ëŒ€í™” ìƒí™©ì„ ê·¸ë˜í”„ë¡œ í‘œí˜„í•˜ì—¬ ë°œí™” ê°„ ê±°ë¦¬ì™€, ëŒ€í™”ì˜ íë¦„ì„ ì§ê´€ì ìœ¼ë¡œ í™•ì¸í•  ìˆ˜ ìˆë„ë¡ êµ¬ì„±í–ˆë‹¤.\nDash\r#\rDashë¥¼ ì²˜ìŒ ì‚¬ìš©í•´ë³¸ ê²ƒì´ê¸° ë•Œë¬¸ì— í•„ìš”í•œ ê¸°ëŠ¥ì´ ìˆì„ ë•Œë§ˆë‹¤ ê³µì‹ ë¬¸ì„œë¥¼ ì°¸ê³ í–ˆë‹¤.\nDashì—ì„œ ì§€ì›í•˜ëŠ” Callback í•¨ìˆ˜ë¥¼ í†µí•´ elementì˜ ê°’ì´ ë³€ë™ë  ë•Œ íŠ¹ì • í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•˜ì—¬ ë™ì ìœ¼ë¡œ ì›¹í˜ì´ì§€ë¥¼ êµ¬ì„±í•  ìˆ˜ ìˆë‹¤.\në˜í•œ, dcc(dash core component)ì—ì„œ ì§€ì›í•˜ëŠ” store ê°ì²´ë¥¼ í™œìš©í•˜ì—¬ ì—¬ëŸ¬ í˜ì´ì§€ì—ì„œ ê°’ì„ ê³µìœ í•  ìˆ˜ ìˆë‹¤.\nCallback í•¨ìˆ˜ì˜ ì‚¬ìš©ì´ ì²˜ìŒì— ì‚´ì§ í—·ê°ˆë¦´ ìˆ˜ ìˆëŠ”ë° ê°„ëµí•˜ê²Œ ì •ë¦¬í•˜ìë©´ ê¸°ë³¸ì ì¸ í˜•íƒœëŠ” ì•„ë˜ì™€ ê°™ë‹¤. Dashì—ì„œ ì§€ì›í•˜ëŠ” Output, Input, State ê°ì²´ê°€ ì¡´ì¬í•˜ê³ , í•´ë‹¹ ê°ì²´ë¥¼ í™œìš©í•˜ì—¬ ì½”ë“œë¥¼ ê°„ê²°í•˜ê²Œ êµ¬ì„±í•  ìˆ˜ ìˆë‹¤.\n@callback( Output(\u0026#34;element1\u0026#34;, \u0026#34;className\u0026#34;), Output(\u0026#34;element2\u0026#34;, \u0026#34;value\u0026#34;), Input(\u0026#34;element3\u0026#34;, \u0026#34;className\u0026#34;), State(\u0026#34;element4\u0026#34;, \u0026#34;value\u0026#34;), ) def function(input1, input2): ... return output1, output2 Output â€” í•¨ìˆ˜ì˜ returnê°’ì„ ì…ë ¥í•  element idì™€ íŒŒë¼ë¯¸í„° ì„ ì–¸ Input, State â€” í•¨ìˆ˜ì˜ íŒŒë¼ë¯¸í„°ë¡œ ë„˜ê¸¸ element idì™€ íŒŒë¼ë¯¸í„° ì„ ì–¸ Inputì€ Trigger ì—­í• ì„ ìˆ˜í–‰í•˜ê¸° ë•Œë¬¸ì—, elementì˜ ê°’ì´ ë°”ë€Œë©´ callback í•¨ìˆ˜ê°€ í˜¸ì¶œëœë‹¤.\nTailwind, Grid\r#\rì´ë²ˆì— ëŒ€ì‹œë³´ë“œë¥¼ êµ¬í˜„í•˜ê¸° ìœ„í•´ ì–´ì©” ìˆ˜ ì—†ì´ UIì— ëŒ€í•œ ê³ ë¯¼ë„ ë§ì´ í–ˆë‹¤.\nUIë¥¼ ê³ ë¯¼í•˜ëŠ” ê³¼ì •ì—ì„œ ì–¼ë ëš±ë•… Tailwindë„ ë°°ìš°ê³  Display ë°©ì‹ìœ¼ë¡œ Gridë„ ë§ì´ í™œìš©í–ˆë‹¤.\nì´ ë‘ ê°€ì§€ëŠ” ê°ê° ì‘ì—…ì´ ëë‚œ í›„ì— ì•Œê²Œ ë˜ì–´ ìƒˆë¡œ ì½”ë“œë¥¼ ìˆ˜ì •í•˜ëŠ” ê³¼ì •ì„ ê²ªì—ˆëŠ”ë°, ë¯¸ë¦¬ ì•Œê³  ì‹œì‘í–ˆë”ë¼ë©´ ì‹œê°„ì„ ë§ì´ ë‹¨ì¶•í•  ìˆ˜ ìˆì—ˆì„ ê²ƒ ê°™ì•„ ì•„ì‰¬ì›€ì´ ë‚¨ëŠ”ë‹¤.\nCheat Sheet\r#\rCheat Sheetì—ëŠ” ìì£¼ í™œìš©ë˜ëŠ” ê¸°ëŠ¥ì´ ë¹¼ê³¡í•˜ê²Œ ë‚˜ì—´ë˜ì–´ ìˆì–´ ì•„ì§ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ìµìˆ™í•˜ì§€ ì•Šì€ ìƒí™©ì—ì„œ êµ¬ê¸€ë§ ì‹œê°„ì´ ë§ì´ ì¤„ì–´ë“ ë‹¤.\në‚˜ëŠ” Tailwindì™€ Dash(Plotly)ì˜ Cheat Sheetë¥¼ ì´ë²ˆì— ë°œê²¬í•˜ê³  í™œìš©í•˜ì—¬ ì‘ì—… ëŠ¥ë¥ ì„ ì˜¬ë¦´ ìˆ˜ ìˆì—ˆë‹¤. ìœ ëª…í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë§ˆë‹¤ Cheat Sheetê°€ ì¡´ì¬í•˜ëŠ” ê²ƒ ê°™ìœ¼ë‹ˆ ìµìˆ™ì§€ ì•Šì€ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ë§ì´ ì‚¬ìš©í•´ì•¼ í•˜ëŠ” ì¼ì´ ìƒê¸°ë©´ Cheat Sheetë¥¼ ê²€ìƒ‰í•´ë³´ì.\nCheatsheet\nì¢‹ì•˜ë˜ ì ë“¤\r#\rì´ë²ˆ í”„ë¡œì íŠ¸ë¥¼ í†µí•´ ë¹„ë¡œì†Œ ë‚˜ëŠ” ê¹ƒí—ˆë¸Œë¥¼ ì œëŒ€ë¡œ í™œìš©í•˜ê²Œ ë˜ì—ˆë‹¤ê³  ìƒê°í•œë‹¤. ëŒ€í™”í˜• ì¶”ì²œ ë°ì´í„°ì…‹ì— ëŒ€í•´ ë³´ë‹¤ ì˜ ì´í•´í•˜ê²Œ ë˜ì—ˆë‹¤. í”¼ë“œë°±ì„ í†µí•´ ì‚¬ìš©ìë¥¼ ì¢€ ë” ê³ ë ¤í•˜ëŠ” ëŒ€ì‹œë³´ë“œë¥¼ ë§Œë“¤ ìˆ˜ ìˆê²Œ ë˜ì—ˆë‹¤. Dashì™€ Plotlyë¥¼ ìµíŒ ë•ë¶„ì—, Streamlit, Tableau, Looker Studioë“±ë³´ë‹¤ ììœ ë„ê°€ ë†’ì€ ëŒ€ì‹œë³´ë“œë¥¼ ë§Œë“¤ ìˆ˜ ìˆê²Œ ë˜ì—ˆë‹¤. ë˜í•œ, ë‚´ê°€ ëª¨ë¸ì„ êµ¬ì¶•í•˜ê±°ë‚˜ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ìƒì„±í•´ëƒˆì„ ë•Œ ì´ë¥¼ ì˜ í‘œí˜„í•´ì£¼ê³  ì‚¬ìš©í•˜ê¸° ì¢‹ê²Œ ë§Œë“¤ì–´ì£¼ëŠ” ë°©ë²•ì„ ìµí ìˆ˜ ìˆì—ˆë‹¤. ì•„ì‰¬ì› ë˜ ì ë“¤\r#\rí•˜ê³  ì‹¶ì€ ì¼ì€ ë§ì•˜ìœ¼ë‚˜, ì‹œê°„ì´ ì§§ì•„ ì•„ì‰¬ì›€ì´ í° í”„ë¡œì íŠ¸ì˜€ë‹¤. ê²Œë‹¤ê°€ ì•„ì‰¬ìš´ ë§ˆìŒ ë•Œë¬¸ì— ë§ˆì§€ë§‰ìœ¼ë¡œ ê°ˆìˆ˜ë¡ ì˜¨ í˜ì„ ë‹¤í•˜ì§€ ëª»í–ˆë‹¤. UIì˜ êµ¬ì„±ì— ì‹œê°„ì„ ëœ ìŸê³ , ë‹¤ë¥¸ ë¶€ë¶„ì— ì‹œê°„ì„ í• ì• í–ˆë‹¤ë©´ ë” ë§ì€ ê²ƒì„ ë°°ìš¸ ìˆ˜ ìˆì—ˆì„ ê²ƒì´ë¼ê³  ìƒê°í•œë‹¤. ë§ˆìŒì€ ì•„í”„ì§€ë§Œ ë‹¤ìŒ í”„ë¡œì íŠ¸ì˜ ë°œì „ì„ ìœ„í•´ ê¸°ë¡í•´ë‘”ë‹¤.\nDashì˜ í™œìš© ì‹œê°„ì´ ë¶€ì¡±í•˜ì—¬ ëŒ€ì‹œë³´ë“œì— ë¹„ë™ê¸° ì²˜ë¦¬ë¥¼ ë„ì…í•˜ì§€ ëª»í•œ ê²ƒì´ ì•„ì‰¬ì› ë‹¤. Backend \u0026amp; Ops Dashì—ì„œ ìì²´ì ìœ¼ë¡œ Flaskë¥¼ ì§€ì›í•˜ê¸° ë•Œë¬¸ì—, API ì„œë²„ë¥¼ ë”°ë¡œ êµ¬ì„±í•˜ì§€ ì•Šê³  ì„œë¹„ìŠ¤ë¥¼ ì™„ì„±í•  ìˆ˜ ìˆì—ˆë‹¤. ì´ë¥¼ ìœ„í•´ ë°˜ì •í˜• ë°ì´í„°ì¸ ëŒ€í™”í˜• ì¶”ì²œ ë°ì´í„°ì…‹ì„ ì •í˜• ë°ì´í„°ë¡œ ë³€í™˜í•˜ëŠ” ì‘ì—…ì„ ìˆ˜í–‰í–ˆë‹¤. í•˜ì§€ë§Œ ë°ì´í„° ì²˜ë¦¬ ì†ë„ ì¸¡ë©´ì—ì„œ ì´ìŠˆê°€ ê½¤ ìˆì—ˆê¸°ì— Elastic Searchë¡œ ë°ì´í„°ë¥¼ ê´€ë¦¬í–ˆë”ë¼ë©´ ë” ë‚˜ì•˜ì„ ê²ƒì´ë¼ëŠ” ìƒê°ì´ ë“ ë‹¤. í”„ë¡œì íŠ¸ë¥¼ ê´€ë¦¬í•˜ë©° Github Actionì´ë‚˜ í…ŒìŠ¤íŒ…ë„ ì¶”ê°€í•´ë³´ê³  ì‹¶ì—ˆìœ¼ë‚˜ ê·¸ëŸ¬ì§€ ëª»í–ˆë‹¤. ê¹ƒí—ˆë¸Œì— PR, Issue Templateì„ ì¶”ê°€í•˜ê³  Pre-commitì„ ë„ì…í•´ë³´ì•˜ìœ¼ë‚˜, ìµìˆ™í•˜ì§€ ì•Šì€ íŒ€ì›ë“¤ì´ ì œëŒ€ë¡œ í™œìš©í•˜ë„ë¡ ì´ëŒì–´ì£¼ì§„ ëª»í–ˆë˜ ê²ƒ ê°™ë‹¤. Log ê´€ë¦¬, A/B Test, ETL, ì„¸ì…˜ ê´€ë¦¬ ë“± ë‹¤ìŒ í”„ë¡œì íŠ¸ì—ì„œ í•´ë³´ê³  ì‹¶ì€ ê²ƒë“¤ì´ ë” ë§ì•„ì¡Œë‹¤.\nê¸€ì„ ë§ˆë¬´ë¦¬í•˜ë©°\r#\rë¹„ë¡ í”„ë¡œì íŠ¸ ê²°ê³¼ë¬¼ì€ ì•„ì‰¬ì› ìœ¼ë‚˜ í›Œë¥­í•œ ì‚¬ëŒë“¤ê³¼ì˜ í˜‘ì—…ìœ¼ë¡œ ê²°ê³¼ë¥¼ ë‚´ê³ , ë˜ ë‹¤ë¥¸ íŒ€ë“¤ê³¼ ê²°ê³¼ë¬¼ì„ ê³µìœ í•˜ë©° í”¼ë“œë°±ì„ ë°›ì„ ìˆ˜ ìˆì–´ ì¢‹ì•˜ë‹¤.\ní”„ë¡œì íŠ¸ë¥¼ í•˜ë©´ì„œë„ ë§ì´ ë°°ì› ì§€ë§Œ, ë‹¤ë¥¸ íŒ€ë“¤ì˜ í”„ë¡œì íŠ¸ë¥¼ ë³´ë©´ì„œ ë˜ ë§ì´ ë°°ìš¸ ìˆ˜ ìˆì—ˆë‹¤.\në‹¤ìŒ ë²ˆì— ìˆ˜í–‰í•  í”„ë¡œì íŠ¸ëŠ” ë” ë‚˜ì•„ì§ˆ ê²ƒì´ë¼ ìƒê°í•œë‹¤.\n"},{"id":42,"href":"/posts/2023-09-24-1-epoch/","title":"ëª¨ë¸ í•™ìŠµ ì‹œ 1 epochì— ì–´ë–¤ ì¼ì´ ë°œìƒí•˜ë‚˜ìš”?","section":"Blog","content":"criterion = torch.nn.MSELoss() optimizer = torch.optim.SGD(model.parameters(), lr=learningRate) ... for epoch in range(epochs): optimizer.zero_grad() outputs = model(inputs) loss = criterion(outputs, labels) loss.backward() optimizer.step() 1. optimizer.zero_grad() : ì´ì „ epochì˜ ë¯¸ë¶„ê°’ ì´ˆê¸°í™”\r#\roptimizerì—ì„œ ì—…ë°ì´íŠ¸í•˜ëŠ” íŒŒë¼ë¯¸í„°ì— ì €ì¥ëœ ê·¸ë˜ë””ì–¸íŠ¸ë¥¼ ëª¨ë‘ 0ìœ¼ë¡œ ë§Œë“¤ì–´ì¤€ë‹¤.\ní•´ë‹¹ ì½”ë“œëŠ” ì™œ í•„ìš”í• ê¹Œ?\nzero_grad()ë¥¼ ì‹¤í–‰í•´ì£¼ì§€ ì•Šìœ¼ë©´ ì´í›„ì˜ backwardì—ì„œ í•´ë‹¹ stepì˜ gradient ê°’ì´ ê³„ì† ëˆ„ì ìœ¼ë¡œ ë”í•´ì ¸ ëª¨ë¸ì´ ì´ìƒí•˜ê²Œ í•™ìŠµí•  ìˆ˜ ìˆê¸° ë•Œë¬¸ì´ë‹¤.\nì™œ êµ³ì´ defaultë¥¼ ì´ì „ì˜ gradientê°€ ë„˜ì–´ì˜¤ë„ë¡ ì„¤ì •í–ˆì„ê¹Œ?\nRNN ê³„ì—´ì˜ ëª¨ë¸ì´ë‚˜, ê°€ì¤‘ì¹˜ ê³µìœ ê°€ í•„ìš”í•œ ëª¨ë¸ì˜ ê²½ìš° ì´ì „ gradientë¥¼ ê·¸ëŒ€ë¡œ ê°€ì ¸ì˜¤ëŠ” ê²ƒì´ í•„ìš”í•˜ê¸° ë•Œë¬¸ì´ë‹¤.\nê° epochì˜ ì†ì‹¤ í•¨ìˆ˜ì—ì„œ ë°˜í™˜í•˜ëŠ” í…ì„œ ê°ì²´ëŠ” ì„œë¡œ ë‹¤ë¥¸ë°, ì–´ë–»ê²Œ ì´ì „ í•™ìŠµ ë‹¨ê³„ì˜ ë¯¸ë¶„ ê°’ì„ ë„˜ê²¨ë°›ì„ê¹Œ?\nOptimizer ê°ì²´ì—ì„œ ë¯¸ë¶„ ê°’ì„ ì €ì¥í•˜ê¸° ë•Œë¬¸ì´ë‹¤.\nzero_grad() ì½”ë“œ ë¦¬ë·°\nmodel.zero_grad()ì™€ optimizer.zero_grad()ì˜ ì°¨ì´ëŠ” ë­˜ê¹Œ?\nmodel.zero_grad()ì™€ optimizer.zero_grad()ì˜ ì°¨ì´\n2. outputs = model(inputs) : ëª¨ë¸ ì˜ˆì¸¡ ìˆ˜í–‰\r#\r3. loss = criterion(outputs, labels) : ì†ì‹¤ í•¨ìˆ˜ë¥¼ í†µí•œ loss ê³„ì‚°\r#\r4. loss.backward() : lossì˜ ë¯¸ë¶„ê°’ ê³„ì‚°\r#\r$\\mathbf w$ì—ì„œì˜ lossì— ëŒ€í•œ ë¯¸ë¶„ ì—°ì‚°ì„ ìˆ˜í–‰í•œë‹¤.\nìˆ˜ì¹˜ ë¯¸ë¶„ì—ëŠ” ë§ì€ ì—°ì‚°ì´ í•„ìš”í•œë°, ì´ë¥¼ Back Propagation(ì˜¤ì°¨ ì—­ì „íŒŒ ì•Œê³ ë¦¬ì¦˜)ì„ í†µí•´ í•´ê²°í•œë‹¤.\nê³„ì‚°ëœ ë¯¸ë¶„ ê°’ì„ GD: gradient descent(ê²½ì‚¬í•˜ê°•ë²•)ì„ í†µí•´ $\\mathbf w$ì— ë°˜ì˜í•œë‹¤. Backward\n5. optimizer.step() : ë¯¸ë¶„ê°’ì„ parameterì— ë°˜ì˜\r#\r"},{"id":43,"href":"/posts/2023-09-19-PyTorch-Datasets-DataLoaders/","title":"ëª¨ë¸ì— ë°ì´í„°ë¥¼ ë¨¹ì´ëŠ” ë°©ë²•(PyTorch Datasets \u0026 DataLoaders)","section":"Blog","content":"ëª¨ë¸ì— ë°ì´í„°ë¥¼ ë¨¹ì´ëŠ” ë°©ë²•\n1. Dataset\r#\rëª¨ì•„ë†“ì€ ë°ì´í„°ì— ëŒ€í•´ Datasetì´ë¼ëŠ” í´ë˜ìŠ¤ë¥¼ í†µí•´ ì‹œì‘, ê¸¸ì´, mapstyle ë“±ì„ ì„ ì–¸í•´ì¤€ë‹¤.\n__getitems__() : í•˜ë‚˜ì˜ ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¬ ë•Œ ì–´ë–¤ ì‹ìœ¼ë¡œ ë°ì´í„°ë¥¼ ë°˜í™˜í•  ì§€ë¥¼ ì„ ì–¸í•´ì¤€ë‹¤.\në°ì´í„° ì…ë ¥ í˜•íƒœë¥¼ ì •ì˜í•˜ëŠ” í´ë˜ìŠ¤\në°ì´í„°ë¥¼ ì…ë ¥í•˜ëŠ” ë°©ì‹ì˜ í‘œì¤€í™”\nImage, Text, Audio ë“±ì— ë”°ë¼ ë‹¤ë¥´ê²Œ ì…ë ¥ì´ ì •ì˜ëœë‹¤.\në°ì´í„°ì˜ í˜•íƒœì— ë”°ë¼ ê° í•¨ìˆ˜ë¥¼ ë‹¤ë¥´ê²Œ ì •ì˜í•œë‹¤.\nëª¨ë“  ê²ƒì„ ë°ì´í„° ìƒì„± ì‹œì ì— ì²˜ë¦¬í•  í•„ìš”ëŠ” ì—†ë‹¤.\nimageì˜ Tensor ë³€í™”ëŠ” í•™ìŠµì— í•„ìš”í•œ ì‹œì ì— ë³€í™˜í•´ì£¼ë©´ ëœë‹¤.\në°ì´í„° ì…‹ì— ëŒ€í•œ í‘œì¤€í™”ëœ ì²˜ë¦¬ ë°©ë²• ì œê³µì´ í•„ìš”í•˜ë‹¤.\ní›„ì† ì—°êµ¬ì ë˜ëŠ” ë™ë£Œë“¤ì—ê²ŒëŠ” ë¹›ê³¼ ê°™ì€ ì¡´ì¬ê°€ ë  ìˆ˜ ìˆë‹¤.\nìµœê·¼ì—ëŠ” HuggingFace ë“± í‘œì¤€í™”ëœ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•œë‹¤.\nimport torch from torch.utils.data import Dataset class CustomDataset(Dataset): def __init__(self, text, labels): # ì´ˆê¸° ë°ì´í„° ìƒì„± ë°©ë²•ì„ ì§€ì • self.labels = labels self.data = text def __len__(self): return len(self.labels) # ë°ì´í„°ì˜ ì „ì²´ ê¸¸ì´ def __getitem__(self, idx): # idx ê°’ì„ ì…ë ¥ìœ¼ë¡œ ë°›ê³ , dict íƒ€ì…ìœ¼ë¡œ ë°ì´í„°ë¥¼ ë°˜í™˜í•´ì¤€ë‹¤. label = self.labels[idx] # ë°˜í™˜ë˜ëŠ” ë°ì´í„°ì˜ í˜•íƒœ (X,y) text = self.data[idx] sample = {\u0026#34;Text\u0026#34;: text, \u0026#34;Class\u0026#34;: label} return sample 2. Transforms\r#\rData Augumentation ë“±ì˜ ë™ì‘ì„ ìˆ˜í–‰í•œë‹¤.\nToTensor() : ëª¨ì•„ë†“ì€ ë°ì´í„°ë¥¼ Tensorë¡œ ë³€í™˜í•´ì¤€ë‹¤.\n3. DataLoader\r#\rDataì˜ Batchë¥¼ ìƒì„±í•´ì£¼ëŠ” í´ë˜ìŠ¤\nì •ë¦¬ëœ ë°ì´í„°ë¥¼ ë¬¶ì–´ì„œ ëª¨ë¸ì— ë„£ì–´ì¤€ë‹¤.\nbatchë¥¼ ë§Œë“¤ê±°ë‚˜, shuffle ë“±ì˜ ì—­í• ì„ ìˆ˜í–‰í•œë‹¤.\ní•™ìŠµ ì§ì „(GPU feed ì „) ë°ì´í„°ì˜ ë³€í™˜ì„ ì±…ì„ì§„ë‹¤.\nTensorë¡œ ë³€í™˜ + Batch ì²˜ë¦¬ê°€ ë©”ì¸ ì—…ë¬´ì´ë‹¤.\në³‘ë ¬ì ì¸ ë°ì´í„° ì „ì²˜ë¦¬ ì½”ë“œë¥¼ ê³ ë¯¼í•˜ê²Œ ëœë‹¤.\nDataLoader(dataset, batch_size=1, shuffle=False, **sampler=None**, **batch_sampler=None**, num_workers=0, collate_fn=None, pin_memory=False, drop_last=False, timeout=0, worker_init_fn=None, *, prefetch_factor=2, persistent_workers=False) sampler : Dataë¥¼ ì–´ë–»ê²Œ ë½‘ì„ ì§€ indexë¥¼ ì •í•´ì£¼ëŠ” ê¸°ë²•\ncollate_fn : [[data,label],[data,label]] í˜•íƒœë¡œ ë¬¶ì¸ ë°ì´í„°ë¥¼ [data,data],[label,label]ë¡œ ë°”ê¿”ì¤€ë‹¤.\ní”í•˜ê²Œ ì‚¬ìš©ë˜ì§€ëŠ” ì•ŠëŠ”ë‹¤.\nvariable length(ê°€ë³€ì¸ì) í…ìŠ¤íŠ¸ ì²˜ë¦¬ì—ì„œ paddingì„ ìœ„í•´ ë§ì´ ì“°ì¸ë‹¤. Sequenceí˜• ë°ì´í„°ë¥¼ ì²˜ë¦¬í•  ë•Œë„ ë§ì´ ì“°ì¸ë‹¤. 4. Model\r#\rì˜ˆì œ\r#\rDatasets â€” Torchvision main documentation\nDatasets \u0026amp; DataLoaders â€” PyTorch Tutorials 2.0.1+cu117 documentation\nDataLoaderì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ê° samplerë“¤ì„ ì–¸ì œ ì‚¬ìš©í•˜ë©´ ì¢‹ì„ì§€ ë…¼ì˜í•´ë³´ê¸° ë°ì´í„°ì˜ í¬ê¸°ê°€ ë„ˆë¬´ ì»¤ì„œ ë©”ëª¨ë¦¬ì— í•œë²ˆì— ì˜¬ë¦´ ìˆ˜ ì—†ì„ ë•Œ Datasetì—ì„œ ì–´ë–»ê²Œ ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¤ëŠ” ê²ƒì´ ì¢‹ì„ì§€ ë…¼ì˜í•´ë³´ê¸° "},{"id":44,"href":"/posts/2023-09-21-Hyperparameter_tuning/","title":"ëª¨ë¸ì˜ ì„±ëŠ¥ì´ ë”ì´ìƒ ì˜¤ë¥´ì§€ ì•Šì„ ë•Œ (Hyper-Parameter Tuning)","section":"Blog","content":"í•˜ì´í¼ íŒŒë¼ë¯¸í„°\nëª¨ë¸ ìŠ¤ìŠ¤ë¡œ í•™ìŠµí•˜ì§€ ì•ŠëŠ” ê°’.\nì‚¬ëŒì´ ì§ì ‘ ì§€ì •í•´ì£¼ì–´ì•¼ í•œë‹¤.\nê²°ê³¼ë¥¼ ê°œì„ í•˜ê³  ì‹¶ì„ ë•Œ\r#\rëª¨ë¸ì„ ë°”ê¾¸ê¸°\nì¤‘ìš”í•˜ì§€ë§Œ, ì´ë¯¸ ë†’ì€ ì„±ëŠ¥ì˜ ëª¨ë¸ì´ ê³µê°œë˜ì–´ìˆê¸° ë•Œë¬¸ì— ìƒëŒ€ì ìœ¼ë¡œ ëœ ì¤‘ìš”.\në°ì´í„°ë¥¼ ë°”ê¾¸ê¸° â†’ ì„±ëŠ¥ ê°œì„ ì„ ìœ„í•´ ê°€ì¥ ì¤‘ìš”í•˜ë‹¤.\ní•˜ì´í¼ íŒŒë¼ë¯¸í„° Tuning\nì•½ê°„ì˜ ì„±ëŠ¥ ê°œì„ ì´ ê°„ì ˆí•œ ê²½ìš° ìˆ˜í–‰í•œë‹¤.\në§ˆì§€ë§‰ 0.01ì˜ ì„±ëŠ¥ ê°œì„ ì´ë¼ë„ í•„ìš”í•œ ê²½ìš° ì‚¬ìš©í•œë‹¤.\ngeneralization ë“± ì ìš©\nHyperparameter Tuning\r#\rê°€ì¥ ê¸°ë³¸ì ì¸ ë°©ë²• - grid vs random\ngrid\nì ì ˆí•œ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ì°¾ì„ ë•Œ, ê°’ë“¤ì„ ì¼ì •í•œ ë²”ìœ„ë¥¼ ì •í•´ ì„ íƒí•˜ëŠ” ê²ƒ.\nrandom\nê°’ì„ ëœë¤í•˜ê²Œ ì°¾ì•„ì„œ ê°€ì¥ ì„±ëŠ¥ì´ ì˜ë‚˜ì˜¤ëŠ” ê²ƒì„ ì„ íƒí•œë‹¤.\nìš”ì¦˜ì—ëŠ” ì˜ ì“°ì´ì§€ ì•Šê³ , ë² ì´ì§€ì•ˆ ê¸°ë°˜ ê¸°ë²•ì´ ë§ì´ ì“°ì¸ë‹¤.\nRay\r#\rmulti-node multi processing ì§€ì› ëª¨ë“ˆ\nML/DLì˜ ë³‘ë ¬ ì²˜ë¦¬ë¥¼ ìœ„í•´ ê°œë°œëœ ëª¨ë“ˆ\nê¸°ë³¸ì ìœ¼ë¡œ í˜„ì¬ì˜ ë¶„ì‚°ë³‘ë ¬ ML/DL ëª¨ë“ˆì˜ í‘œì¤€\nHyperparameter Searchë¥¼ ìœ„í•œ ë‹¤ì–‘í•œ ëª¨ë“ˆ ì œê³µ\ndata_dir = os.path.abspath(\u0026#34;./data\u0026#34;) load_data(data_dir) # search space ì§€ì • config = { \u0026#34;l1\u0026#34;: tune.sample_from(lambda _: 2 ** np.random.randint(2, 9)), \u0026#34;l2\u0026#34;: tune.sample_from(lambda _: 2 ** np.random.randint(2, 9)), \u0026#34;lr\u0026#34;: tune.loguniform(1e-4, 1e-1), \u0026#34;batch_size\u0026#34;: tune.choice([2, 4, 8, 16]) } # í•™ìŠµ ìŠ¤ì¼€ì¤„ë§ ì•Œê³ ë¦¬ì¦˜ ì§€ì • scheduler = ASHAScheduler( # ASHAS : ì‹¤í–‰ ë„ì¤‘ ë‚®ì€ lossë¥¼ ê°€ì§€ëŠ” metricë“¤ì„ ë²„ë¦¬ëŠ” ì•Œê³ ë¦¬ì¦˜ metric=\u0026#34;loss\u0026#34;, mode=\u0026#34;min\u0026#34;, max_t=max_num_epochs, grace_period=1, reduction_factor=2) # ê²°ê³¼ ì¶œë ¥ ì–‘ì‹ ì§€ì • reporter = CLIReporter( metric_columns=[\u0026#34;loss\u0026#34;, \u0026#34;accuracy\u0026#34;, \u0026#34;training_iteration\u0026#34;]) # ë³‘ë ¬ ì²˜ë¦¬ ì–‘ì‹ìœ¼ë¡œ í•™ìŠµ ì‹œí–‰ result = tune.run( partial(train_cifar, data_dir=data_dir), resources_per_trial={\u0026#34;cpu\u0026#34;: 2, \u0026#34;gpu\u0026#34;: gpus_per_trial}, config=config, num_samples=num_samples, scheduler=scheduler, progress_reporter=reporter) data_dir = os.path.abspath(\u0026#34;./data\u0026#34;) load_data(data_dir) config = { \u0026#34;l1\u0026#34;: tune.sample_from(lambda _: 2 ** np.random.randint(2, 9)), \u0026#34;l2\u0026#34;: tune.sample_from(lambda _: 2 ** np.random.randint(2, 9)), \u0026#34;lr\u0026#34;: tune.loguniform(1e-4, 1e-1), \u0026#34;batch_size\u0026#34;: tune.choice([2, 4, 8, 16]) } scheduler = ASHAScheduler( metric=\u0026#34;loss\u0026#34;, mode=\u0026#34;min\u0026#34;, max_t=max_num_epochs, grace_period=1, reduction_factor=2) reporter = CLIReporter( # parameter_columns=[\u0026#34;l1\u0026#34;, \u0026#34;l2\u0026#34;, \u0026#34;lr\u0026#34;, \u0026#34;batch_size\u0026#34;], metric_columns=[\u0026#34;loss\u0026#34;, \u0026#34;accuracy\u0026#34;, \u0026#34;training_iteration\u0026#34;]) result = tune.run( partial(train_cifar, data_dir=data_dir), resources_per_trial={\u0026#34;cpu\u0026#34;: 2, \u0026#34;gpu\u0026#34;: gpus_per_trial}, config=config, num_samples=num_samples, scheduler=scheduler, progress_reporter=reporter) ëª¨ë¸ì˜ ëª¨ë“  layerì—ì„œ learning rateê°€ í•­ìƒ ê°™ì•„ì•¼ í• ê¹Œ? í•˜ì´í¼ íŒŒë¼ë¯¸í„° íƒìƒ‰ì˜ ìš°ì„ ìˆœìœ„ ì–´ë–»ê²Œ ë ê¹Œ? Pytorchì™€ Ray ê°™ì´ ì‚¬ìš©í•˜ê¸° "},{"id":45,"href":"/posts/2023-11-15-TF-IDF/","title":"ë¬¸ì„œì˜ ìˆœìœ„ë¥¼ ë§¤ê¸°ëŠ” ë°©ë²•, TF-IDF","section":"Blog","content":"\rBackground\rBackground\r#\rì´ë•Œê¹Œì§€ ìš°ë¦¬ì˜ ì¿¼ë¦¬ëŠ” ëª¨ë‘ Booleanì„ í™œìš©í•œ ê²ƒì´ì—ˆë‹¤.\nBooleanì€ ìì‹ ë“¤ì˜ ì›í•˜ëŠ” ê²€ìƒ‰ê²°ê³¼ë¥¼ ì •í™•í•˜ê²Œ ì•„ëŠ” ì „ë¬¸ê°€ë“¤ì´ ì‚¬ìš©í•˜ê¸°ì— ì¢‹ë‹¤.\në˜í•œ ë§ì€ ì •ë³´ë¥¼ íƒìƒ‰í•˜ëŠ” ì „ë¬¸ê°€ë“¤ì—ê²Œ ìœ ìš©í•˜ë‹¤.\ní•˜ì§€ë§Œ ëŒ€ë¶€ë¶„ì˜ ì‚¬ëŒë“¤ì´ í¸í•˜ê²Œ ì‚¬ìš©í•˜ê¸°ì—” ì ì ˆí•˜ì§€ ì•Šë‹¤.\nëŒ€ë¶€ë¶„ì˜ ìœ ì €ë“¤ì€ boolean ì¿¼ë¦¬ë¥¼ ì‘ì„±í•˜ì§€ ëª»í•œë‹¤.\nì‚¬ìš©ìë“¤ì€ ê·¸ë ‡ê²Œ ë§ì€ ê²°ê³¼ë¬¼ì´ í•„ìš”í•˜ì§€ ì•Šë‹¤.\nëŒ€ë¶€ë¶„ Boolean queryëŠ” ë¬¸ì„œê°€ ë„ˆë¬´ ë§ì´ ë‚˜ì˜¤ê±°ë‚˜, ë„ˆë¬´ ì ê²Œ ë‚˜ì˜¨ë‹¤.\nQuery 1: â€œstandard user dlink 650â€ â†’ 200,000 hits\nQuery 2: â€œstandard user dlink 650 no card foundâ€: 0 hits\nANDëŠ” ë„ˆë¬´ ì ê²Œ ë‚˜ì˜¤ê³ , ORì€ ë„ˆë¬´ ë§ì´ ë‚˜ì˜¨ë‹¤.\nì‹¬ì§€ì–´ ìˆœì„œë„ ì—†ì´ ë’¤ì£½ë°•ì£½ ë‚˜ì˜¨ë‹¤.\nQuiz 1: Search results\nX AND Yì—ëŒ€í•´ êµ¬ê¸€ ê²€ìƒ‰í•  ë•Œ, ì´ 2000ê°œì˜ ê²°ê³¼ê°€ ë¦¬í„´ X AND Y AND Zì—ëŒ€í•´ êµ¬ê¸€ ê²€ìƒ‰í•  ë•Œ, ì´ 3500ê°œì˜ ê²°ê³¼ê°€ ë¦¬í„´ ì™œ 2ë²ˆì§¸ ì¿¼ë¦¬ê°€ ë” ë§ì€ ê²°ê³¼ë¥¼ ë¦¬í„´í•˜ëŠ”ê°€?\nRanked Retreival Models\r#\rAND, OR ì´ë ‡ê²Œ ê²€ìƒ‰í•˜ì§€ ì•Šê³ , ì‚¬ëŒì—ê²Œ ë§í•˜ë“¯ì´ ê²€ìƒ‰í•˜ê³ ì í•œë‹¤.\nì¦‰, ì¿¼ë¦¬ í‘œí˜„ì‹ì„ ë§Œì¡±í•˜ëŠ” ë¬¸ì„œì˜ ì§‘í•©ì´ ì•„ë‹Œ, ì‹œìŠ¤í…œì´ ì¿¼ë¦¬ì— ëŒ€í•œ ì»¬ë ‰ì…˜ì— ëŒ€í•œ ìƒìœ„ ë­í‚¹ì˜ ë¬¸ì„œë¥¼ ë¦¬í„´í•œë‹¤.\nfree text query: ì¿¼ë¦¬ ì–¸ì–´ì˜ precise query(ì—°ì‚°ìë‚˜ í‘œí˜„) ë³´ë‹¤ëŠ” ì‚¬ìš©ìì˜ ì¿¼ë¦¬ëŠ” í•œë‹¨ì–´ ì´ìƒì˜ ìì—°ì–´ê°€ ì…ë ¥ëœë‹¤. ì‹¤ì „ì—ì„œëŠ” ë³´í†µ ranked retreivalì´ free text ì¿¼ë¦¬ë‘ ì¡°í•©ëœë‹¤. ì´ ë•Œ, ë­í‚¹ ì•Œê³ ë¦¬ì¦˜ í’ˆì§ˆì„ ì¢‹ê²Œ ìœ ì§€í•´ì•¼ í•œë‹¤.\në­í‚¹ì„ ë§¤ê¸°ëŠ” ë¬¸ì„œì˜ ì–‘ì„ ì¤„ì´ëŠ” ê²ƒì€ í’ˆì§ˆë©´ì—ì„œëŠ” í° ì´ìŠˆê°€ ì•„ë‹ˆë‹¤.\r#\rêµ¬ê¸€ ê²€ìƒ‰ì˜ í´ë¦­ë¥ ì„ ë³´ë©´ ëŒ€ë¶€ë¶„ 1í˜ì´ì§€ì˜ 10ê°œì˜ ê²€ìƒ‰ê²°ê³¼ì˜ í´ë¦­ë¥ ì´ 94%ì´ë‹¤.\nìœ ì €ëŠ” 2í˜ì´ì§€ë¡œ ì´ë™í•  ê°€ëŠ¥ì„± ì¡°ì°¨ ë‚®ë‹¤.\nRanked Retreivalì˜ ê¸°ì¤€ ì ìˆ˜ ë§¤ê¸°ê¸°\r#\rê°€ì¥ ê·¸ëŸ´ë“¯í•œ ë¬¸ì„œ ìˆœì„œëŒ€ë¡œ ë¦¬í„´í•´ì£¼ê³  ì‹¶ì€ë°, ì¿¼ë¦¬ë§ˆë‹¤ ë¬¸ì„œì˜ ë­í‚¹ì„ ì–´ë–»ê²Œ ì •í•  ìˆ˜ ìˆì„ê¹Œ?\nê° ë¬¸ì„œê°€ ì¿¼ë¦¬ì— ë§¤ì¹­ë˜ëŠ” ì •ë„ë¥¼ 0ê³¼ 1 ì‚¬ì´ì˜ ê°’ìœ¼ë¡œ ì ìˆ˜ë¥¼ ë§¤ê¸´ë‹¤.\ní•´ë‹¹ ì ìˆ˜ëŠ” query timeì— ê³„ì‚°ëœë‹¤.\nê³„ì‚°ì´ ì˜¤ë˜ ê±¸ë ¤ì„  ì•ˆëœë‹¤.\nQuery-document matching scores\r#\rì¿¼ë¦¬-ë¬¸ì„œ ìŒì— ëŒ€í•´ ì ìˆ˜ë¥¼ í• ë‹¹í•˜ëŠ” ë°©ë²•\në³µì¡í•œ ê²ƒì„ ë°°ìš°ê¸° ì „ì— term í•œê°œì§œë¦¬ ì¿¼ë¦¬ë¶€í„° ì‹œì‘í•´ë³´ì.\në§Œì•½ query termì´ ë¬¸ì„œì— ì—†ë‹¤ë©´ 0ì  query termì˜ ì¶œí˜„ ë¹ˆë„ê°€ í´ìˆ˜ë¡ ë†’ì€ ì ìˆ˜ë¥¼ í• ë‹¹ ì•ìœ¼ë¡œ ì´ ë°©ì‹ì— ëŒ€í•œ ëª‡ê°€ì§€ ëŒ€ì•ˆì„ ì‚´í´ë³¼ ì˜ˆì • Jaccard coefficient\r#\rì•ì—ì„œ ë‹¨ì–´ê°„ ìœ ì‚¬ë„ë¥¼ í™•ì¸í•  ë•Œ ì‚¬ìš©í–ˆì—ˆë‹¤.\nì¼ë°˜ì ìœ¼ë¡œ ì“°ì—¬ì§€ëŠ” Aì™€ Bì§‘í•©ì˜ ê´€ê³„ëŠ” ì•„ë˜ì™€ ê°™ì´ ë‚˜íƒ€ë‚¸ë‹¤. jaccard(A,B)=$|Aâˆ©B||AâˆªB|$ jaccard(A,A)=1 jaccard(A,B)=0Â if $Aâˆ©B=0$ Aì™€ Bê°€ ê°™ì€ í¬ê¸°ì¼ í•„ìš”ëŠ” ì—†ë‹¤. ë¬¸ì„œì˜ term ê°œìˆ˜ë³´ë‹¤ query termì´ ì ì€ ê²ƒì´ ë‹¹ì—°í•˜ë‹¤. í•­ìƒ 0~1 ê°’ì„ ê°€ì§„ë‹¤. ì¿¼ë¦¬-ë¬¸ì„œ match ì ìˆ˜ì—ì„œ Jaccard coefficientëŠ” ì–´ë–»ê²Œ ê³„ì‚°í• ê¹Œ?\nQuery: ides of march ì¼ ë•Œ\nDocument 1: caesar died in march â‡’ 1 / 3 + 4 - 1 Document 2: the long march â‡’ 1 / 3 + 3 - 1 doc1 :Â $1\\over6$Â doc2 :Â $1\\over5$Â ì¦‰ doc2ê°€ ì ìˆ˜ê°€ ì¡°ê¸ˆ ë” ë†’ë‹¤.\nJaccard coefficientì˜ ë¬¸ì œì \r#\rterm frequencyë¥¼ ê³ ë ¤í•˜ì§€ ì•ŠëŠ”ë‹¤.\në¬¸ì„œ ì•ˆì—ì„œ ë™ì¼í•œ termì´ ëª‡ ë²ˆ ë°œìƒí–ˆëŠ”ì§€ëŠ” ê´€ì‹¬ì´ ì—†ë‹¤.\ní•˜ì§€ë§Œ ì´ ê°’ì€ ì¤‘ìš”í•˜ë‹¤.\nâ–ªï¸ frequency\rterm frequency($tf_{term}$)\nì–´ë–¤ document ì•ˆì—ì„œ í•´ë‹¹ termì´ ë‚˜íƒ€ë‚œ íšŸìˆ˜\ndocument frequency($df_{term}$)\nì–´ë–¤ termì´ ë‚˜íƒ€ë‚˜ëŠ” documentì˜ ê°œìˆ˜\ncollection frequency($cf_{term}$)\ncollection ì „ì²´ì—ì„œ ì–´ë–¤ termì´ ë‚˜íƒ€ë‚œ íšŸìˆ˜\nScoringì˜ ëª©ì ì´ queryì™€ document ì‚¬ì´ì˜ ê´€ê³„ì´ë‹¤.\ncollectionì—ì„œ ë“œë¬¼ê²Œ ë‚˜íƒ€ë‚˜ëŠ” termì€ í”í•œ termë³´ë‹¤ ë¬¸ì„œë¥¼ íŠ¹ì •í•˜ê¸°ì— í›¨ì”¬ ë” ìœ ìš©í•˜ë‹¤.\ní•˜ì§€ë§Œ JaccardëŠ” frequencyë¥¼ ê³ ë ¤í•˜ì§€ ì•Šê¸° ë•Œë¬¸ì— ì´ ì •ë³´ë¥¼ ê³ ë ¤í•˜ì§€ ì•ŠëŠ”ë‹¤.\nê¸¸ì´ë¥¼ normalize í•  ë” ì„¸ë ¨ëœ ë°©ë²•ì´ í•„ìš”í•˜ë‹¤.\në¬¸ì„œì˜ ê¸¸ì´ê°€ ê¸´ ê²½ìš° ë™ì¼í•œ ë‹¨ì–´ê°€ ë” ë§ì´ ì¶œëª°í•  ê²ƒì´ë‹¤.\nì´ë¥¼ ë¬¸ì„œì˜ ê¸¸ì´ê°€ ì§§ì€ ê²½ìš°ì˜ ë‹¨ì–´ ì¶œëª° íšŸìˆ˜ë‘ ë¹„êµí•˜ê¸° ìœ„í•´ ì •ê·œí™”í•  í•„ìš”ê°€ ìˆë‹¤.\në‚˜ì¤‘ì— ì´ ìˆ˜ì‹ì„ ì‚¬ìš©í•œë‹¤.\n$|Aâˆ©B||AâˆªB|$\nRecall: Binary term-document incidence matrix\r#\rì˜ˆë¥¼ ë“¤ì–´ antony and brutus and not(calpurnia)ë¼ë©´\n$110001$ $110100$ $101111 -\u0026gt; 100000$ ìœ„ ë¹„íŠ¸ì™€ì´ì¦ˆ ì—°ì‚°ì„ í†µí•´ â€œAntony and Cleopatraâ€ê°€ ë§Œì¡±í•˜ëŠ” ì†Œì„¤ì±…ì„ì„ ì°¾ì„ ìˆ˜ ìˆë‹¤.\nê° ë¬¸ì„œë¥¼ ë°”ì´ë„ˆë¦¬ ë²¡í„°ë¡œ í‘œì‹œí•œë‹¤.\n$vector âˆˆ {0,1}^{V}$\nTerm-document count matrices\r#\rfrequencyë¥¼ ê³ ë ¤í•˜ê¸° ìœ„í•´ 0/1 ë²¡í„° ëŒ€ì‹ ì— íšŸìˆ˜ë¥¼ ë²¡í„°í™”í•œë‹¤.\në¬¸ì„œì— termì´ ë“±ì¥í•œ íšŸìˆ˜ë¥¼ ê³ ë ¤í•¨\nê° ë¬¸ì„œëŠ”Â ìì—°ìˆ˜ë¥¼ ë‹´ì€ count vectorë¡œ í‘œì‹œ\në¬¸ì„œì—ì„œ ë‹¨ì–´ì˜ ë“±ì¥íšŸìˆ˜ê°€ ë§ì€ ê²ƒë“¤ì´ ì¤‘ìš”í•˜ë‹¤.\nBag of words model\r#\rë²¡í„° í‘œí˜„ì€ ë¬¸ì„œì•ˆì˜ ë‹¨ì–´ì˜ ìˆœì„œë¥¼ ê³ ë ¤í•˜ì§€ ì•ŠëŠ”ë‹¤.\nJohn is quicker than Mary Mary is quicker than John ìœ„ì˜ ë‘ ë¬¸ì¥ì€ ë‹¤ë¥¸ ëœ»ì´ì§€ë§Œ ê°™ì€ ë²¡í„°ë¥¼ ê°€ì§€ê²Œ ëœë‹¤.\nì´ë¥¼Â BOW(bag of words)Â ëª¨ë¸ì´ë¼ê³  ë¶€ë¥¸ë‹¤.\nìœ„ì˜ ë‘ ë¬¸ì¥ì˜ ì°¨ì´ë¥¼ êµ¬ë³„í•  ìˆ˜ ìˆëŠ” positional indexì— ë¹„í•´ BOWëŠ” í›„í‡´í•œ ê²ƒì²˜ëŸ¼ ë³´ì¸ë‹¤.\ní•˜ì§€ë§Œ positional indexëŠ” presentationì„ íŒë³„í•˜ê¸° ìœ„í•´ ì‚¬ìš©ëë˜ ê²ƒì´ë‹¤.\në‚˜ì¤‘ì— positional indexê°€ ë‹¤ì‹œ ì–¸ê¸‰ëœë‹¤.\nTF: Term frequency\r#\rterm frequency($tf_{t,d}$): ë¬¸ì„œ dì—ì„œ term tê°€ ë°œìƒí•œ ë¹ˆë„\ntfë¥¼ query-documentê°€ ì–¼ë§ˆë‚˜ ì¼ì¹˜í•˜ëŠ”ì§€ ê³„ì‚°í•˜ê¸° ìœ„í•´ ì“°ê³  ì‹¶ë‹¤.\nraw tf ê°’ì€ í™œìš©í•˜ê¸° ë¶ˆí¸í•˜ë‹¤.\ntf 10ì¸ ë¬¸ì„œê°€ tf 1ì¸ ë¬¸ì„œë³´ë‹¤ ë” ì—°ê´€ë„ê°€ ë†’ë‹¤.\ní•˜ì§€ë§Œ ìˆ«ìì˜ í¬ê¸°ê°€ í•´ë‹¹ ë¬¸ì„œê°€ 10ë°° ë” ìœ ì˜ë¯¸í•˜ë‹¤ëŠ” ì˜ë¯¸ëŠ”Â ì•„ë‹ˆë‹¤.\në¬¸ì„œì˜ ì—°ê´€ì„±ì€ tfì˜ ìˆ˜ì— ë”°ë¼ ë¹„ë¡€ì ìœ¼ë¡œ ì¦ê°€í•˜ì§€ëŠ” ì•ŠëŠ”ë‹¤.\nê²Œë‹¤ê°€, ë‹¨ì–´ì˜ ë¹ˆë„ê°€ 0ê°œì¸ ë¬¸ì„œì™€ 1ê°œì¸ ë¬¸ì„œì˜ ì°¨ì´ëŠ” ë§¤ìš° í¬ì§€ë§Œ,\n100ê°œì¸ ë¬¸ì„œì™€ 101ê°œì¸ ë¬¸ì„œì˜ ì°¨ì´ëŠ” ì•„ì£¼ ì‘ë‹¤.\në”°ë¼ì„œ ë‹¨ì–´ì˜ ê°œìˆ˜ì— ë”°ë¼ ë‹¬ë¼ì§€ëŠ” ì˜í–¥ë ¥ì„ í‘œí˜„í•˜ê¸° ìœ„í•´, ë‹¨ì–´ì˜ ë¹ˆë„ì— logë¥¼ ì·¨í•œë‹¤.\nì´ ë•Œ, ë‹¨ì–´ê°€ í•˜ë‚˜ ìˆëŠ” ê²ƒê³¼ ì•„ì˜ˆ ì—†ëŠ” ê²ƒì˜ ì°¨ì´ëŠ” í›¨ì”¬ í¬ê¸° ë•Œë¬¸ì—, 0ê³¼ 1ì€ ë”°ë¡œ êµ¬ë¶„í•œë‹¤.\n$$ \\tt score = âˆ‘_{tâˆˆqâˆ©d}(1+log tf_{t,d}) $$\në¬¸ì„œì˜ ì ìˆ˜ë¥¼ ê³„ì‚°í•˜ê¸° ìœ„í•´ì„ , queryì™€ documentì—ì„œ ê³µí†µìœ¼ë¡œ ë‚˜íƒ€ë‚˜ëŠ” ë‹¨ì–´ì˜ ë¹ˆë„ë¥¼ logë¡œ ê³„ì‚°í•˜ë©´ ëœë‹¤.\në¬¸ì„œì—ì„œ query termì´ í•˜ë‚˜ë„ ë°œê²¬ë˜ì§€ ì•Šìœ¼ë©´ ì ìˆ˜ëŠ” 0ì´ë‹¤. ex term frequency weight 0 0 1 1 2 1.3 10 2 1000 4 DF: Document Frequency\r#\rí¬ê·€í•œ termì€ í”í•œ term ëŒ€ë¹„ ë” ìœ ìš©í•˜ë‹¤.\nstop wordì™€ ê°™ì€ ê²ƒë“¤ì€ ë¬¸ì„œë¥¼ êµ¬ë³„ì§“ëŠ”ë° ë„ì›€ì´ ë˜ì§€ ì•ŠëŠ”ë‹¤.\narachnocentricì´ ë¬¸ì„œë¥¼ êµ¬ë³„ì§“ëŠ”ë° í›¨ì”¬ ë§ì€ ë„ì›€ì´ ëœë‹¤.\nì´ë ‡ê²Œ í¬ê·€í•œ termì„ í¬í•¨í•œ ë¬¸ì„œëŠ” ì¿¼ë¦¬ termì— ë§¤ìš° ì—°ê´€ë„ê°€ ë†’ì„ ê²ƒì´ë‹¤.\ní¬ê·€í•œ arachnocentricê°™ì€ termì— ê°€ì¤‘ì¹˜ë¥¼ ë¶€ì—¬í•´ì•¼ í•œë‹¤.\nìš”ì•½í•˜ë©´, Document frequencyê°€ ì‘ì€ ë‹¨ì–´ì¼ìˆ˜ë¡ ìœ ìš©í•˜ê³ , term frequencyê°€ í° ë‹¨ì–´ì¼ìˆ˜ë¡ ë¬¸ì„œë¥¼ íŠ¹ì •ì§“ëŠ”ë° ìœ ìš©í•˜ë‹¤.\níš¨ê³¼ì ì¸ ê²€ìƒ‰ì„ ìœ„í•´ document frequency($df$)ì— ëŒ€í•œ ì •ë³´ë„ í™œìš©í•´ì•¼ í•œë‹¤.\nëª¨ë“  documentì—ì„œ ë‚˜íƒ€ë‚˜ëŠ” termë“¤ì€ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•˜ëŠ”ë° ë„ì›€ì´ í¬ê²Œ ë˜ì§€ ì•ŠëŠ”ë‹¤.\nì»¬ë ‰ì…˜ ë‚´ì—ì„œ í”í•œ termì„ ìƒê°í•´ë³´ì. (ex: high, increase, line)\në¬¼ë¡  ì´ëŸ¬í•œ termì„ í¬í•¨í•˜ëŠ” ë¬¸ì„œê°€ ê·¸ëŸ¬ì§€ ì•Šì€ ë¬¸ì„œë³´ë‹¤ëŠ” ì—°ê´€ìˆì„ ê°€ëŠ¥ì„±ì´ ë†’ë‹¤.\ní•˜ì§€ë§Œ í™•ì‹¤í•œ ì—°ê´€ë„ì˜ ì²™ë„ê°€ ë  ìˆ˜ëŠ” ì—†ë‹¤.\nterm frequencyê°€ ë†’ì€ ë‹¨ì–´ë“¤ì—ê²Œ ë†’ì€ ê°€ì¤‘ì¹˜ë¥¼ ë¶€ì—¬í•´ì•¼ í•œë‹¤.\ndocument frecuencyê°€ ë‚®ì€ ë‹¨ì–´ë“¤ì—ê²Œ ë†’ì€ ê°€ì¤‘ì¹˜ë¥¼ ë¶€ì—¬í•´ì•¼ í•œë‹¤.\nIDF: Inverse Document Frequency\r#\r$$ \\tt idf_t=log_{10}(N/df_t) $$\nN = ì „ì²´ document ìˆ˜ dfê°€ ì‘ì€ termì˜ ì ìˆ˜ë¥¼ ë” ë†’ê²Œ ì£¼ê¸° ìœ„í•´ dfë¥¼ ë’¤ì§‘ì–´ì„œ ë¶„ëª¨ë¡œ ì‚¬ìš©í•œë‹¤.\nidfê°’ì„ ì™„í™”ì‹œí‚¤ê¸° ìœ„í•´ logë¥¼ ì·¨í•´ì¤€ë‹¤.\nlogì˜ baseê°€ ê¼­ 10ì¼ í•„ìš”ëŠ” ì—†ë‹¤.\nex) N = 1 million\nN(ë¬¸ì„œì˜ ê°œìˆ˜) = 1,000,000ì´ê³ ,\n$idf_t=log_{10}(N/df_t)$ì¸ ê²½ìš°\nterm df_t idf_t calpurnia 1 6 animal 100 4 sunday 1,000 3 fly 10,000 2 under 100,000 1 the 1,000,000 0 theì²˜ëŸ¼ ëª¨ë“  documentì—ì„œ ë‚˜íƒ€ë‚˜ëŠ” ë‹¨ì–´ëŠ” ê°€ì¤‘ì¹˜ê°€ 0ì´ ë˜ì–´ë²„ë¦°ë‹¤. ì „ì²´ collectionì— ì¡´ì¬í•˜ëŠ” termë§ˆë‹¤ ê³ ìœ í•œ idf ê°’ì´ ì¡´ì¬í•œë‹¤. ì‰½ê²Œ ì´í•´í•˜ìë©´ idfë¥¼ ì ìš©í•˜ë©´ calpurniaëŠ” promotingí•˜ê³  theëŠ” demotingí•œë‹¤. idfëŠ” one term ì¿¼ë¦¬ì— ìˆì–´ì„œëŠ” ë­í‚¹ì— ë³€í™”ê°€ ì—†ë‹¤.\nì–´ì°¨í”¼ ê°€ì¤‘ì¹˜ë¥¼ êµ¬í•  ë•Œ ëª¨ë“  documentì— í•­ìƒ ê°™ì€ ê°’ì´ ê³±í•´ì§€ê²Œ ëœë‹¤.\nidfëŠ” ì ì–´ë„ 2ê°œì˜ term ì´ìƒì— ëŒ€í•´ íš¨ê³¼ê°€ ìˆë‹¤.\nâ€œcapricious personâ€ë¼ëŠ” ì¿¼ë¦¬ê°€ ìˆìœ¼ë©´ idf ê°€ì¤‘ì¹˜ëŠ” capricious ë¼ëŠ” í¬ê·€í•œ ë‹¨ì–´ì— person ì´ë¼ëŠ” í”í•œ ë‹¨ì–´ ë³´ë‹¤ ìƒëŒ€ì ìœ¼ë¡œ ë†’ì€ ê°€ì¤‘ì¹˜ë¥¼ ë¶€ì—¬í•˜ê²Œ ëœë‹¤.\nex) query : iphone box\nCF vs. DF\ntì— ëŒ€í•œ Collection FrequencyëŠ” ì „ì²´ ì»¬ë ‰ì…˜ ë‚´ì—ì„œ tê°€ ë°œìƒí•œ ë¹ˆë„ìˆ˜ë¥¼ ì§‘ê³„í•œë‹¤.\nì—¬ëŸ¬ë²ˆ ë“±ì¥í•œ ê²ƒì„ ëª¨ë‘ ì„¼ë‹¤.\nex\në‘ ë‹¨ì–´ì˜ collection frequencyê°€ ë¹„ìŠ·í•˜ì§€ë§Œ, insuranceì˜ document frequencyì— ë¹„í•´ tryì˜ document frequencyê°€ í›¨ì”¬ í¬ë‹¤.\nê·¸ëŸ¬ë¯€ë¡œ insuranceê°€ ë” ë†’ì€ ê°€ì¤‘ì¹˜ë¥¼ ë°›ì•„ì•¼ í•œë‹¤.\nTF-IDF\r#\rë¬¸ì„œì— ë“±ì¥í•œ ë‹¨ì–´ë“¤ì˜ ì¤‘ìš”ë„ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ê°’\në‹¨ì–´ë§ˆë‹¤ TF-IDF ê°’ì´ ê³„ì‚°ëœë‹¤.\nIRì—ì„œ ê°€ì¥ í•µì‹¬ì ì¸ ê°€ì¤‘ì¹˜ ê³µì‹\n$$ \\tt W_{t,d}=(1+log_{10}tf_{t,d}) \\times log_{10}(N/df_t) $$\ntermì˜ tf-idf ê°€ì¤‘ì¹˜ëŠ” tf ê°€ì¤‘ì¹˜ì™€ idf ê°€ì¤‘ì¹˜ì˜ ê³±ì´ë‹¤.\ntf.idfë‚˜ tf x idfë¼ê³  ë¶€ë¥´ê¸°ë„ í•œë‹¤.\nê°€ì¤‘ì¹˜ëŠ” collectionì—ì„œ termì˜ ë°œìƒë¹ˆë„ì— ë”°ë¼ ì¦ê°€í•œë‹¤.\nê°€ì¤‘ì¹˜ëŠ” ì»¬ë ‰ì…˜ ë‚´ì— termì´ í¬ê·€í• ìˆ˜ë¡ ì¦ê°€í•œë‹¤.\nì¿¼ë¦¬ì— ëŒ€í•œ ë¬¸ì„œì˜ Score ê³„ì‚°\r#\r$$ \\tt Score(q,d)=âˆ‘_{tâˆˆqâˆ©d}tf.idf_{t,d} $$\nìœ„ ìˆ˜ì‹ì€ q(query)ì™€ d(document)ì—ì„œ ê³µí†µë˜ëŠ” termì„ ê°€ì§„ documentì˜ scoreë§Œ ê³„ì‚°í•œë‹¤ëŠ” ì˜ë¯¸ì´ë‹¤.\në¬¸ì„œë“¤ì˜ Scoreì„ ê³„ì‚°í•  ë•Œ ë‹¤ì–‘í•œ ì˜µì…˜ì´ ì¡´ì¬í•œë‹¤.\ntfë¥¼ ê³„ì‚°í•˜ëŠ” ë°©ë²•\nlog ì ìš© ì—¬ë¶€ logì˜ base í¬ê¸° ì¿¼ë¦¬ë¥¼ êµ¬ì„±í•˜ëŠ” termì— ê°€ì¤‘ì¹˜ ë¶€ì—¬ ì—¬ë¶€\nê°€ì¤‘ì¹˜ ë¶€ì—¬ ì—†ì´, queryë„ í•˜ë‚˜ì˜ documentì²˜ëŸ¼ ì²˜ë¦¬í•˜ëŠ” ë°©ë²•ë„ ìˆë‹¤.\nqueryëŠ” documentì˜ í•œ ì¢…ë¥˜ë‹¤.\në¬¸ì„œë¥¼ ë‚˜íƒ€ë‚´ëŠ” ë°©ë²•\r#\rë¬¸ì„œë“¤ì„ tf-idf ê°€ì¤‘ì¹˜ í–‰ë ¬ë¡œ ë‚˜íƒ€ë‚¸ë‹¤.\nAntony and Cleopatra Julius Caesar The Tempest Hamlet Othello Macbeth d1 d2 d3 d4 d5 d6 Antony t1 5.25 3.18 0 0 0 0.35 Brutus t2 1.21 6.1 0 1 0 0 Caesar t3 8.59 2.54 0 1.51 0.25 0 Calpurnia t4 0 1.54 0 0 0 0 Cleopatra t5 2.85 0 0 0 0 0 Mercy t6 1.51 0 1.9 0.12 5.25 0.88 worser t7 1.37 0 0.11 4.15 0.25 1.95 ê° ë¬¸ì„œëŠ” tf-idf ê°€ì¤‘ì¹˜ì˜ ì‹¤ìˆ˜ê°’ ë²¡í„°ë¡œ í‘œí˜„ëœë‹¤.\nì¦‰, ê° ë¬¸ì„œë¥¼ êµ¬ì„±í•˜ëŠ” Termë“¤ì„ tf-idf ê°’ìœ¼ë¡œ ì „í™˜í•˜ì—¬ ë¬¸ì„œë¥¼ ë²¡í„°í™”í•œë‹¤.\ntf-idf ê°€ì¤‘ì¹˜ í–‰ë ¬ $âˆˆR^{|V|}$\n$V$ëŠ” ë¬¸ì„œì— í¬í•¨ëœ ë‹¨ì–´ì˜ ê°œìˆ˜ë¥¼ ì˜ë¯¸í•œë‹¤.\nê²°êµ­ $|V|$ ì°¨ì›ì˜ ë²¡í„° ê³µê°„ì„ ê°€ì§€ê²Œ ëœë‹¤.\ntermì€ ê³µê°„ì˜ ì°¨ì›ì´ ëœë‹¤.\në¬¸ì„œëŠ” ê³µê°„ì—ì„œì˜ ë²¡í„°(ì )ì´ë¼ê³  ì´í•´í•  ìˆ˜ ìˆë‹¤.\nì¿¼ë¦¬ë„ ë¬¸ì„œì™€ ê°™ì€ í¬ê¸°ë¡œ ë“¤ì–´ê°€ì•¼ í•œë‹¤.\nì¿¼ë¦¬ë„ ë¬¸ì„œë¡œ ì·¨ê¸‰í•˜ì—¬ ë²¡í„°ë¡œ í‘œí˜„í•œë‹¤.\ní•˜ì§€ë§Œ ë§Œì•½ ì´ ê°œë…ì„ ì›¹ ê²€ìƒ‰ì—”ì§„ì— ì ìš©í•œë‹¤ë©´ ì°¨ì›ì´ ìˆ˜ì–µê°œê°€ ëœë‹¤.\nì´ëŠ”Â ë§¤ìš° sparseí•œ ë²¡í„°ì´ê³  ëŒ€ë¶€ë¶„ì˜ ê°’ì€ 0ì¼ ê²ƒì´ê¸°ì—, ë‹¤ë¥¸ ë°©ë²•ì´ í•„ìš”í•˜ë‹¤.\nê³µê°„ì—ì„œ ì¿¼ë¦¬ ë²¡í„°(ë¬¸ì„œ)ì˜ ìœ ì‚¬ë„(proximity)ì— ëŒ€í•´ ë­í‚¹ì„ ë§¤ê¸´ë‹¤.\nscore(q,d1), score(q,d2)\u0026hellip;ëŠ” ë²¡í„°ê³µê°„ ìƒì—ì„œ ë¬¸ì„œë²¡í„°ê°€ ì¿¼ë¦¬ë²¡í„°ì™€ ì–¼ë§ˆë‚˜ í¡ì‚¬í•œì§€ë¥¼ íŒë³„í•œë‹¤.\nproximity â‰ˆ inverse of distance\nì¦‰, ê±°ë¦¬ê°€ ê°€ê¹Œìš¸ìˆ˜ë¡ ë¬¸ì„œì™€ ì¿¼ë¦¬ê°€ ë¹„ìŠ·í•˜ë‹¤\nboolean ëª¨ë¸ì—ì„œ ë²—ì–´ë‚˜ê¸° ìœ„í•´ ì´ ì‘ì—…ì„ ìˆ˜í–‰í•œë‹¤.\nëŒ€ì‹ ì— ë” ì—°ê´€ìˆëŠ” ë¬¸ì„œì— ë” ë†’ì€ ë­í¬ë¥¼ ë¶€ì—¬í•œë‹¤.\nTF-IDFì˜ ë‹¤ì–‘í•œ ì„ íƒì§€\r#\rtf-idfì˜ ê°€ì¤‘ì¹˜ ì•Œê³ ë¦¬ì¦˜ì€ ì„ íƒì˜ í­ì´ ë‹¤ì–‘í•˜ë‹¤.\nê°€ì¥ ë§ì´ ì“°ì´ëŠ” ê²ƒì€ ë¶‰ì€ ìƒ‰ í‘œì‹œê°€ ë˜ì–´ìˆë‹¤.\në§ì€ ê²€ìƒ‰ ì—”ì§„ë“¤ì´ ì¿¼ë¦¬ë‚˜ ë¬¸ì„œì— ëŒ€í•´ ë‹¤ì–‘í•œ ê°€ì¤‘ì¹˜ ë¶€ì—¬ë°©ì‹ì„ í—ˆìš©í•œë‹¤.\ní‘œê¸°ë²•\nddd.qqq\nì•ì˜ 3ê¸€ì: ë¬¸ì„œì— ëŒ€í•œ ì•Œê³ ë¦¬ì¦˜\në’¤ì˜ 3ê¸€ì: ì¿¼ë¦¬ì— ëŒ€í•œ ì•Œê³ ë¦¬ì¦˜\nex) lnc.ltc\ndocument â†’ lnc\nlogarithmic tf no idf cosine normalization query â†’ ltc\nlogarithmic tf (t) â‡’ idf cosine normalization ë§¤ìš° í‘œì¤€ì ì¸ ê°€ì¤‘ì¹˜ ë¶€ì—¬ ë°©ì‹ìœ¼ë¡œ lnc.ltcê°€ ìˆë‹¤.\nquiz: documentì— no idfë¥¼ ì ìš©í•˜ëŠ”ê²ƒì´ ë‚˜ìœ ì•„ì´ë””ì–´ì¸ê°€?\nltc.lncê°€ ë” ì¼ë°˜ì ìœ¼ë¡œ ë³´ì¸ë‹¤.\nex â€” lnc.ltc\r#\rë¬¸ì„œ: car insurance auto insurance ì¿¼ë¦¬: best car insurance tf-raw: termì˜ ë°œìƒíšŸìˆ˜\ntf-wt: $1+log(tf_{t,d})$. ì¦‰, termì˜ ë°œìƒë¹ˆë„ë¥¼ ê°€ì¤‘ì¹˜ë¡œ ë°”ê¾¼ ê²ƒ.\nidf: $log{N\\over df_t}$\nwt: tf-wt * idf : $(1+log(tf_{t,d}))\\times log{N\\over df_t}$\nn\u0026rsquo;lize: ë¬¸ì„œ ê¸¸ì´(wt ì œê³± ì´í•©ì˜ ë£¨íŠ¸) ë¡œ wtë¥¼ ë‚˜ëˆˆ ê²ƒ\në¬¸ì„œì˜ ê¸¸ì´: $\\sqrt {1^2+0^2+1^2+1.3^2} \\simeq 1.92$\nì‹¤ì œ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ëŠ” ë‚´ì ê°’ì˜ í•©ìœ¼ë¡œ autoì— ëŒ€í•œ ë‚´ì ê°’ 0, bestì— ëŒ€í•œ ë‚´ì ê°’ 0, carì— ëŒ€í•œ ë‚´ì ê°’ 0.27(0.52 * 0.52)ê³¼ insuranceì— ëŒ€í•œ ë‚´ì ê°’ 0.53(0.78 * 0.68)ì„ ë”í•œ 0.8ì´ë‹¤.\nquiz: ë¬¸ì„œì˜ ìˆ˜, Nì€ ëª‡ì¼ê¹Œ? "},{"id":46,"href":"/posts/2023-09-23-Loss-function/","title":"ì†ì‹¤ í•¨ìˆ˜(Loss Function)ì— ëŒ€í•´ ì•Œì•„ë³´ì.","section":"Blog","content":" âœ”ï¸ ê°„ë‹¨ ìš”ì•½\nì‹ ê²½ë§ì˜ í•™ìŠµ ì¤‘ ë°›ëŠ” ë²Œì ì˜ ê¸°ì¤€\níšŒê·€ì™€ ë¶„ë¥˜ ë¬¸ì œì—ì„œ ë‹¤ë¥¸ loss functionì„ ì‚¬ìš©í•œë‹¤.\n{: .prompt-info } Gradient, MAE, MSE, RMSE\nLoss : ì˜ˆì¸¡ ê°’ê³¼ ì‹¤ì œ ê°’ì˜ ì°¨ì´\nì‹ ê²½ë§ì˜ í•™ìŠµ ì¤‘ ì˜¤ë‹µì— ëŒ€í•´ ë°›ëŠ” ë²Œì \në‘ ê°’ì˜ ì°¨ì´ëŠ” ë‹¨ìˆœíˆ ëº„ì…ˆì˜ ì ˆëŒ“ê°’ì„ ì˜ë¯¸í•˜ëŠ” ê²ƒì€ ì•„ë‹ˆë©°, ìƒí™©ì— ë”°ë¼ ë‹¤ì–‘í•˜ê²Œ ë‚˜íƒ€ë‚œë‹¤.\nex) ì •ë‹µê³¼ ì™„ì „íˆ ë™ë–¨ì–´ì§„ ëŒ€ë‹µì„ í•˜ë©´ ë” ë§ì€ ë²Œì ì„ ë°›ëŠ”ë‹¤.\nLoss Function\r#\rì‹ ê²½ë§ì´ ë²Œì ì„ ë°›ëŠ” ê¸°ì¤€\nì‹ ê²½ë§ì˜ í•™ìŠµ ê³¼ì •ì—ì„œ ê°€ì¤‘ì¹˜ $\\mathbf w$ë¥¼ í‰ê°€í•˜ëŠ” í•¨ìˆ˜.\në‚˜ëŠ” ì†ì‹¤ í•¨ìˆ˜ê°€ í•¨ìˆ˜ë¼ëŠ” ê²ƒì„ ì œëŒ€ë¡œ ì¸ì§€í•˜ì§€ ëª»í–ˆì„ ë•Œ ëª¨ë¸ í‰ê°€ Metricê³¼ í—·ê°ˆë ¸ê¸° ë•Œë¬¸ì—, ì†ì‹¤ í•¨ìˆ˜ë¼ëŠ” ê²ƒì„ ë‹¤ì‹œ í•œë²ˆ ì¸ì§€í•˜ê³  ì§€ë‚˜ê°€ì.\n2ì°¨ì› ê·¸ë˜í”„ë¡œ ë¹„ìœ í–ˆì„ ë•Œ, ê°€ì¤‘ì¹˜ $\\mathbf w$ëŠ” xì¢Œí‘œì— í•´ë‹¹í•˜ê³  ì†ì‹¤ í•¨ìˆ˜ì˜ ê°’ì€ yì¢Œí‘œì— í•´ë‹¹í•œë‹¤.\nì†ì‹¤ í•¨ìˆ˜ì˜ ìµœì†Ÿê°’ì´ ë˜ëŠ” ì§€ì ì— $\\mathbf w$ë¥¼ ìœ„ì¹˜ì‹œí‚¤ëŠ” ê²ƒì´ ì‹ ê²½ë§ì˜ ëª©í‘œì´ë‹¤.\nê³ ë“± ìˆ˜í•™ì„ ë¹Œë ¤ ì„¤ëª…í•˜ìë©´, ë‹¨ìˆœíˆ ë¯¸ë¶„ê°’ì´ 0ì´ ë˜ëŠ” ì§€ì ì„ íŒŒì•…í•˜ì—¬ ì†ì‹¤ í•¨ìˆ˜ì˜ ìµœì†Ÿê°’ì„ êµ¬í•˜ë©´ ëœë‹¤.\ní•˜ì§€ë§Œ ê³ ì°¨ì›ì—ì„œì˜ ì†ì‹¤ í•¨ìˆ˜ ë¯¸ë¶„ì€ ì‰½ì§€ ì•Šì„ ë¿ë”ëŸ¬, í•¨ìˆ˜ ì „ì²´ì—ì„œ ë¯¸ë¶„ê°’ì´ 0ì´ ë˜ëŠ” ì§€ì ì„ ë°”ë¡œ ì°¾ì•„ë‚´ëŠ” ê²ƒì€ í˜„ì‹¤ì ìœ¼ë¡œ ë¶ˆê°€ëŠ¥í•˜ë‹¤.\në”°ë¼ì„œ, ì‹ ê²½ë§ì€ $\\mathbf w$ì—ì„œ ì†ì‹¤ í•¨ìˆ˜ì˜ ê¸°ìš¸ê¸°ë¥¼ ì¸¡ì •í•˜ì—¬ lossê°€ ë‚®ì•„ì§€ëŠ” ë°©í–¥ìœ¼ë¡œ ê°€ì¤‘ì¹˜ë¥¼ ì¡°ê¸ˆì”© ì´ë™í•˜ëŠ” ì „ëµì„ ì‚¬ìš©í•œë‹¤.\nì´ ë•Œì˜ ì¡°ê¸ˆì„ ê²°ì •í•˜ëŠ” ê²ƒì´ Optimizerì´ë‹¤.\ní•´ê²°í•˜ê³ ì í•˜ëŠ” ë¬¸ì œì— ë§ê²Œ loss functionì„ ì„¤ì •í•´ ì‚¬ìš©í•´ì£¼ë©´ ëœë‹¤.\nì‹ ê²½ë§ í•™ìŠµì„ í†µí•´ ì†ì‹¤ í•¨ìˆ˜ $J$ì˜ ìµœì €ì ì„ ì°¾ì•„ì•¼ í•œë‹¤.\nì‹ ê²½ë§ì˜ í•™ìŠµ ì•Œê³ ë¦¬ì¦˜\r#\rí›ˆë ¨ ë°ì´í„° ì…ë ¥ ë§¤ê°œë³€ìˆ˜ $\\mathbf w$ë¥¼ ë‚œìˆ˜ë¡œ ì´ˆê¸°í™” while (true): ì†ì‹¤ í•¨ìˆ˜$J(\\mathbf w)$ ê³„ì‚°(loss ê³„ì‚°) lossë¥¼ ë‚®ì¶”ëŠ” ë°©í–¥ $\\Delta \\mathbf w$ ê³„ì‚° $\\mathbf w = \\mathbf w + \\Delta \\mathbf w$ return ê°€ì¤‘ì¹˜(ë§¤ê°œë³€ìˆ˜) ì†ì‹¤ í•¨ìˆ˜ J(w)ì˜ ì¡°ê±´\r#\rwê°€ í›ˆë ¨ ì§‘í•©ì— ìˆëŠ” ìƒ˜í”Œì„ ëª¨ë‘ ë§íˆë©´, $J(w) = 0$ì´ë‹¤. wê°€ í‹€ë¦¬ëŠ” ìƒ˜í”Œì´ ë§ì„ìˆ˜ë¡ $J(w)$ì˜ ê°’ì´ í¬ë‹¤. ìœ„ì˜ ì¡°ê±´ì„ ë§Œì¡±í•˜ëŠ” ìˆ˜ì‹ì€ ì•„ì£¼ ë‹¤ì–‘í•˜ê¸° ë•Œë¬¸ì—, ì ì ˆí•œ ì†ì‹¤ í•¨ìˆ˜ë¥¼ ì„ íƒí•´ì•¼ í•œë‹¤.\nì†ì‹¤ í•¨ìˆ˜ì˜ ì¢…ë¥˜\r#\ríšŒê·€(Regression)\r#\rMAE(Mean Absolute Error) â€” $\\left|ì •ë‹µ - ì˜ˆì¸¡ê°’\\right|$ì˜ í‰ê· \nê°„ë‹¨ ìš”ì•½\ní‹€ë¦° ë§Œí¼ ë²Œì ì„ ì–»ëŠ”ë‹¤.\nëª¨ë“  ì§€ì ì—ì„œ ê·¸ë˜ë””ì–¸íŠ¸ëŠ” ë™ì¼í•˜ë‹¤.\nê°€ì¥ ê°„ë‹¨í•œ ì†ì‹¤ í•¨ìˆ˜.\nì œê³±ì„ ì·¨í•˜ì§€ ì•Šê¸° ë•Œë¬¸ì—, ëª¨ë“  ì˜¤ì°¨ëŠ” ê·¸ëŒ€ë¡œ ë°˜ì˜ëœë‹¤.\nì§ê´€ì ìœ¼ë¡œ ë§í•˜ìë©´, ì˜¤ì°¨ë§Œí¼ ë²Œì ì´ ìŒ“ì¸ë‹¤.\nê¸°ìš¸ê¸° ê´€ì ìœ¼ë¡œ, ëª¨ë“  ê°€ì¤‘ì¹˜ì—ì„œ ê·¸ë˜ë””ì–¸íŠ¸ì˜ í¬ê¸°ê°€ ë™ì¼í•˜ë‹¤.\në”°ë¼ì„œ MSEë‚˜ RMSEì— ë¹„í•´ ìƒëŒ€ì ìœ¼ë¡œ ì´ìƒì¹˜ì— ëŒ€í•´ Robustí•˜ë‹¤.\n(ì´ìƒì¹˜ë„ ì˜¤ì°¨ë§Œí¼ë§Œ ë²Œì ì´ ìŒ“ì´ê¸° ë•Œë¬¸)\n$$ \\frac{1}{n} \\sum_{i=1}^n\\left|{y_i}-\\hat y_i\\right| $$\nMSE(Mean Squared Error) â€” $(ì •ë‹µ - ì˜ˆì¸¡ê°’)^2$ì˜ í‰ê· \nê°„ë‹¨ ìš”ì•½\nì •ë‹µê³¼ì˜ ê±°ë¦¬ê°€ ë©€ìˆ˜ë¡ ë” ë§ì€ ë²Œì ì„ ë¶€ì—¬í•˜ì!\nì˜¤ì°¨ë¥¼ ì œê³± í•˜ë©´ ë˜ê² ë„¤?\nì •ë‹µì—ì„œ ë©€ì–´ì§ˆìˆ˜ë¡ ê·¸ë˜ë””ì–¸íŠ¸ì˜ í¬ê¸°ê°€ ì¦ê°€í•œë‹¤.\n$$ M S E=\\frac{1}{n} \\sum_{i=1}^n\\left({y_i}-\\hat y_i\\right)^2 $$\në¯¸ë‹ˆ ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì²˜ë¦¬(ìƒ˜í”Œì˜ ì˜¤ì°¨ë¥¼ í‰ê·  ë‚¸ë‹¤.)\n$$ \\begin{aligned}J\\left(\\mathbf{U}^1, \\mathbf{U}^2\\right) \u0026amp; =\\frac{1}{|M|} \\sum_{\\mathbf{x} \\in M}|\\mathbf{y}-\\mathbf{0}|^2 \\\u0026amp; =\\frac{1}{|M|} \\sum_{\\mathbf{x} \\in M}\\left|\\mathbf{y}-\\tau_2\\left(\\mathbf{U}^2 \\tau_1\\left(\\mathbf{U}^1 \\mathbf{x}^{\\mathrm{T}}\\right)\\right)\\right|^2\\end{aligned} $$\nì˜¤ì°¨ê°’ì— ì œê³±ì„ ì·¨í•˜ê¸° ë•Œë¬¸ì— 0~1 ì‚¬ì´ì˜ ê°’ì€ ìƒëŒ€ì ìœ¼ë¡œ ì‘ê²Œ ë°˜ì˜ë˜ê³ , 1ë³´ë‹¤ í° ê°’ì€ ìƒëŒ€ì ìœ¼ë¡œ ë” í¬ê²Œ ë°˜ì˜ëœë‹¤. í•™ìŠµì´ ëŠë ¤ì§€ê±°ë‚˜ í•™ìŠµì´ ì•ˆë˜ëŠ” ìƒí™©ì„ ì´ˆë˜í•  ê°€ëŠ¥ì„±ì´ ìˆë‹¤. ì •ë‹µê³¼ ì˜ˆì¸¡ê°’ì˜ ì°¨ì´ê°€ í´ìˆ˜ë¡ ë” í¬ê²Œ ë°˜ì˜ë˜ê¸° ë•Œë¬¸ì—, ì´ìƒì¹˜ì— ë§¤ìš° ë¯¼ê°í•˜ë‹¤. RMSE(Root MSE) â€” $\\sqrt{(ì •ë‹µ - ì˜ˆì¸¡ê°’)^2\\text {ì˜ í‰}ê· }$\nê°„ë‹¨ ìš”ì•½\nMSEì— ë£¨íŠ¸ ì”Œìš´ ê°’.\nì–¼í• MAEì™€ ë™ì¼í•œ ê²ƒ ì•„ë‹ˆì•¼? ìƒê°í•  ìˆ˜ ìˆì§€ë§Œ, ê³„ì‚° ìˆœì„œì—ì„œ ì°¨ì´ê°€ ë°œìƒí•˜ê³ , $1\\over n$ì´ ì•„ë‹ˆë¼ $1\\over \\sqrt n$ì„ í–ˆë‹¤ëŠ” ì ì´ MAEì™€ ë‹¤ë¥´ë‹¤.\n$$ R M S E=\\sqrt{\\frac{1}{n} \\sum_{i=1}^n\\left(\\hat{y_i}-y_i\\right)^2} $$\nMSEì™€ ë§ˆì°¬ê°€ì§€ë¡œ ê° ì˜¤ì°¨ê°’ì˜ í¬ê¸°ì— ë”°ë¼ ë‹¤ë¥¸ ê·¸ë˜ë””ì–¸íŠ¸ë¥¼ ê°€ì§€ê²Œ ëœë‹¤.\në¶„ë¥˜(Classification)\r#\rEntropy\ní™•ë¥  ë¶„í¬ì˜ ë¬´ì‘ìœ„ì„±(ë¶ˆí™•ì‹¤ì„±)ì„ ì¸¡ì •í•˜ëŠ” í•¨ìˆ˜\n$$ H(x)=-\\sum_{i=1, k} P\\left(e_i\\right) \\log P\\left(e_i\\right) $$\nCross-Entropy\nì •ë³´ëŸ‰ì„ ìƒì§•í•œë‹¤. â†’ ë¶ˆê³µì •ì„± ë¬¸ì œ í•´ê²°\në‘ í™•ë¥  ë¶„í¬ Pì™€ Qê°€ ë‹¤ë¥¸ ì •ë„ë¥¼ ì¸¡ì •í•˜ëŠ” í•¨ìˆ˜\n$$ H(P, Q)=-\\sum_{i=1, k} P\\left(e_i\\right) \\log Q\\left(e_i\\right) $$\nê³µì •í•œ ì£¼ì‚¬ìœ„ì—ëŠ” íŠ¹ë³„í•œ ì •ë³´ê°€ ì¡´ì¬í•˜ì§€ ì•ŠëŠ”ë‹¤.\n$$ -\\left(\\frac{1}{6} \\log \\frac{1}{6}+\\ldots+\\frac{1}{6} \\log \\frac{1}{6}\\right)=1.7918 $$\nì°Œê·¸ëŸ¬ì§„ ì£¼ì‚¬ìœ„ì—ì„œëŠ” íŠ¹ì • ê°’ì´ ë” ì˜ë‚˜ì˜¨ë‹¤ëŠ” ì •ë³´ê°€ ì¶”ê°€ëœë‹¤.\nê³µì •í•œ ì£¼ì‚¬ìœ„ì™€ ì°Œê·¸ëŸ¬ì§„ ì£¼ì‚¬ìœ„ì˜ êµì°¨ ì—”íŠ¸ë¡œí”¼\n$$ -\\left(\\frac{1}{6} \\log \\frac{1}{2}+\\frac{1}{6} \\log \\frac{1}{10}+\\cdots+\\frac{1}{6} \\log \\frac{1}{10}\\right)=2.0343 $$\nBinary Cross-Entropy\ntf.nn.sigmoid_cross_entropy_with_logits( )\n$$ B C E=-\\frac{1}{N} \\sum_{i=0}^N y_i \\cdot \\log \\left(\\hat{y_i}\\right)+\\left(1-y_i\\right) \\cdot \\log \\left(1-\\hat{y_i}\\right) $$\nCategorical Cross-Entropy\ntf.nn.softmax_cross_entropy_with_logits_v2( )\n$$ C C E=-\\frac{1}{N} \\sum_{i=0}^N \\sum_{j=0}^J y_j \\cdot \\log \\left(\\hat{y_j}\\right)+\\left(1-y_j\\right) \\cdot \\log \\left(1-\\hat{y_j}\\right) $$\n"},{"id":47,"href":"/posts/2023-09-24-Gradient_Descent/","title":"ì†ì‹¤ í•¨ìˆ˜ì—ì„œ ìµœì  í•´ë¥¼ ì°¾ëŠ” ë°©ë²•: Gradient Descent(ê²½ì‚¬ í•˜ê°•ë²•)","section":"Blog","content":" ì†ì‹¤ í•¨ìˆ˜, í™•ë¥ ì  ê²½ì‚¬ í•˜ê°•ë²•, ìˆ˜ì¹˜ ë¯¸ë¶„, ë°°ì¹˜ ëª¨ë“œ, ë¯¸ë‹ˆ ë°°ì¹˜ ëª¨ë“œ, íŒ¨í„´ ëª¨ë“œ, Local Minima, Global Minima, Optimizer\nGradient Descent(ê²½ì‚¬í•˜ê°•ë²•)\r#\rìì—° ê³¼í•™ê³¼ ê³µí•™ì—ì„œ ì˜¤ë«ë™ì•ˆ ì‚¬ìš©í•´ì˜¨ ìµœì í™” ë°©ë²•\nì†ì‹¤ í•¨ìˆ˜ì˜ ìµœì  í•´ë¥¼ ì°¾ê¸° ìœ„í•œ ë°©ë²•\n1ì°¨ ê·¼ì‚¿ê°’Â ë°œê²¬ì„ ìœ„í•œ ìµœì í™” ì•Œê³ ë¦¬ì¦˜\në¯¸ë¶„ ê°’ $\\partial J\\over\\partial w_1$ì˜ ë°˜ëŒ€ ë°©í–¥ì´ ìµœì  í•´ì— ì ‘ê·¼í•˜ëŠ” ë°©í–¥ì´ë‹¤.\në”°ë¼ì„œ, í˜„ì¬ ê°€ì¤‘ì¹˜ $w_1$ì— $-{\\partial J\\over\\partial w_1}$ì„ ë”í•˜ë©´ ìµœì  í•´ì— ê°€ê¹Œì›Œì§„ë‹¤.\nêµ³ì´ ê°€ê¹Œì›Œì§ˆ í•„ìš” ì—†ì´, ì†ì‹¤ í•¨ìˆ˜ë¥¼ ë¯¸ë¶„í•´ì„œ ë°”ë¡œ ê·¹ê°’ì„ ì°¾ìœ¼ë©´ ë˜ì§€ ì•Šì„ê¹Œ? ì¼ë°˜ì ìœ¼ë¡œ ì†ì‹¤ í•¨ìˆ˜ê°€ ë§¤ìš° ë³µì¡í•˜ê³  ë¹„ì„ í˜•ì ì¸ ê²½ìš°ê°€ ë§ê¸° ë•Œë¬¸ì—, ë¯¸ë¶„ì„ í†µí•´ ê·¹ê°’ì„ ê³„ì‚°í•˜ê¸° ì–´ë µë‹¤. ë¯¸ë¶„ì„ êµ¬í˜„í•˜ëŠ” ê²ƒë³´ë‹¤ ê²½ì‚¬ í•˜ê°•ë²•ìœ¼ë¡œ ìµœì†Ÿê°’ì„ ì°¾ëŠ” ê²ƒì´ ë” íš¨ìœ¨ì ì´ë‹¤. â–ªï¸ ë°©í–¥ì€ ì•Œì§€ë§Œ, ìµœì í•´ê¹Œì§€ì˜ ê±°ë¦¬ì— ëŒ€í•œ ì •ë³´ê°€ ì—†ê¸° ë•Œë¬¸ì— **í•™ìŠµë¥  $\\rho$**ë¥¼ ê³±í•´ì„œ ì¡°ê¸ˆì”© ì´ë™í•œë‹¤.\r$$ w_{t+1} = w_t + \\rho\\left(-{\\partial J\\over\\partial w_t}\\right) $$\n$$ w \\leftarrow w + \\eta \\left( -\\frac{\\partial L}{\\partial w}\\right) $$\n$J, L$ : ì†ì‹¤ í•¨ìˆ˜ $\\rho,\\eta \\text{(ë¡œ, ì—íƒ€)}$ : í•™ìŠµë¥  $\\leftarrow$ : ì—…ë°ì´íŠ¸ë¥¼ ì˜ë¯¸í•œë‹¤. í‘œê¸°ëŠ” ë‹¤ì–‘í•˜ë‹¤.\në§¤ê°œë³€ìˆ˜ê°€ ì—¬ëŸ¿ì¸ ê²½ìš°, í¸ë¯¸ë¶„ìœ¼ë¡œ êµ¬í•œ ê¸°ìš¸ê¸°ë¥¼ ì‚¬ìš©í•œë‹¤.\në§¤ê°œë³€ìˆ˜ë§ˆë‹¤ ë…ë¦½ì ìœ¼ë¡œ ë¯¸ë¶„í•œë‹¤.\n$$ {\\tt{w = w + \\rho\\left(\\tt-\\triangledown w\\right)}} \\ \\tt\\triangledown w = \\left({\\partial J\\over\\partial w_0},{\\partial J\\over\\partial w_1},{\\partial J\\over\\partial w_2},\u0026hellip;,{\\partial J\\over\\partial w_d}\\right) $$\nì ì ˆí•œ í•™ìŠµë¥ \r#\rí•™ìŠµë¥ ì€ í•œë²ˆì— ìµœì í•´ë¥¼ í–¥í•´ ë‚˜ì•„ê°€ëŠ” ê±°ë¦¬ë¥¼ ì˜ë¯¸í•œë‹¤.\ní•™ìŠµë¥ ì´ ë„ˆë¬´ ë‚®ë‹¤ë©´, ìˆ˜ë ´í•˜ëŠ” ë° ì‹œê°„ì´ ë„ˆë¬´ ì˜¤ë˜ ê±¸ë¦¬ê²Œ ë˜ê³ ,\ní•™ìŠµë¥ ì´ ë„ˆë¬´ ë†’ë‹¤ë©´, ìµœì í•´ì— ìˆ˜ë ´í•˜ì§€ ëª»í•˜ê³  ë‹¤ë¥¸ê³³ìœ¼ë¡œ ë°œì‚°í•˜ê²Œ ëœë‹¤.\ní•™ìŠµë¥ ì„ ì ì ˆíˆ ì¡°ì •í•˜ëŠ” ê²ƒì´ ë§¤ìš° ì¤‘ìš”í•˜ë‹¤.\r#\rê¸°ê³„ í•™ìŠµì˜ ê²½ì‚¬ í•˜ê°•ë²•\r#\rì—¬ëŸ¬ ì¸¡ë©´ì—ì„œ í‘œì¤€ ê²½ì‚¬ í•˜ê°•ë²•ê³¼ ë‹¤ë¥´ë‹¤. ì¡ìŒì´ ì„ì¸ ë°ì´í„°ì˜ ê°œì… ë°©ëŒ€í•œ ë§¤ê°œë³€ìˆ˜ ì¼ë°˜í™” ëŠ¥ë ¥ì´ í•„ìš” ê¸°ê³„ í•™ìŠµì—ì„œ ìµœì  í•´ë¥¼ ì°¾ëŠ” ê²ƒì€ ì‰½ì§€ ì•Šë‹¤. ì •í™•ë¥ ì´ ë“±ë½ì„ ê±°ë“­í•˜ë©° ìˆ˜ë ´í•˜ì§€ ì•ŠëŠ” ë¬¸ì œ í›ˆë ¨ ì§‘í•©ì—ì„œì˜ ë†’ì€ ì„±ëŠ¥ì´ í…ŒìŠ¤íŠ¸ ì§‘í•©ì—ì„œì˜ ì„±ëŠ¥ìœ¼ë¡œ ì´ì–´ì§€ì§€ ì•ŠëŠ” ë¬¸ì œ ê²½ì‚¬ í•˜ê°•ë²• ì ìš© ë°©ë²•\r#\r1. BGD: Batch Gradient Descent\r#\rë°°ì¹˜ ëª¨ë“œ\ní‹€ë¦° ìƒ˜í”Œì„ ëª¨ì€ ë‹¤ìŒ í•œêº¼ë²ˆì— ë§¤ê°œë³€ìˆ˜ ê°±ì‹ í•œë‹¤.\ní•œ epochì— ë§¤ê°œë³€ìˆ˜ ê°±ì‹ ì´ ë‹¨ í•œë²ˆë§Œ ì¼ì–´ë‚œë‹¤.\nì¦‰, ëª¨ë“  ìƒ˜í”Œì„ í™•ì¸í•œ í›„, ìµœì ì˜ ë°©í–¥ìœ¼ë¡œ í•œ ê±¸ìŒ ì›€ì§ì¸ë‹¤.\nê³„ì‚°ëŸ‰ì´ ë§ê³  ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦°ë‹¤.\n2. SGD: Stochastic gradient descent(í™•ë¥ ì  ê²½ì‚¬ í•˜ê°•ë²•)\r#\ríŒ¨í„´ ëª¨ë“œì™€ ë¯¸ë‹ˆë°°ì¹˜ ëª¨ë“œì˜ ê²½ì‚¬í•˜ê°•ë²•ì—ëŠ” ëœë¤ ìƒ˜í”Œë§ì´ ì ìš©ë˜ê¸° ë•Œë¬¸ì—, Stochastic(í™•ë¥ ì )ì´ë¼ëŠ” ìˆ˜ì‹ì–´ë¥¼ ë¶™ì¸ë‹¤.\në°ì´í„°ë¥¼ ë¬´ì‘ìœ„ë¡œ ì„ íƒí•˜ì—¬ í›¨ì”¬ ì ì€ ë°ì´í„°ì…‹ìœ¼ë¡œ í‰ê· ê°’ì„ ì¶”ì •í•  ìˆ˜ ìˆë‹¤.\níŒ¨í„´ ëª¨ë“œ\nìƒ˜í”Œ í•˜ë‚˜ì— ëŒ€í•´ ì „ë°© ê³„ì‚°ì„ ìˆ˜í–‰í•˜ê³  ì˜¤ë¥˜ì— ë”°ë¼ ë°”ë¡œ ë§¤ê°œë³€ìˆ˜ ê°±ì‹ \níŒ¨í„´ ë³„ë¡œ ë§¤ê°œë³€ìˆ˜ ê°±ì‹ \nepochê°€ ì‹œì‘í•  ë•Œ ìƒ˜í”Œì„ ë’¤ì„ì–´ ëœë¤ ìƒ˜í”Œë§ íš¨ê³¼ ë°œìƒ\ní•˜ë‚˜ì˜ ìƒ˜í”Œì„ í™•ì¸í•œ í›„, ì •ë³´ë¥¼ ë°˜ì˜í•˜ì—¬ ë°”ë¡œ í•œ ê±¸ìŒ ì›€ì§ì¸ë‹¤.\në°˜ë³µì´ ì¶©ë¶„í•˜ë©´ SGDê°€ íš¨ê³¼ë¥¼ ë³¼ ìˆ˜ ìˆì§€ë§Œ, ë…¸ì´ì¦ˆê°€ ë§¤ìš° ì‹¬í•´ ìµœì €ì ì„ ì°¾ì§€ ëª»í•  ìˆ˜ë„ ìˆë‹¤.\në¯¸ë‹ˆ ë°°ì¹˜ëª¨ë“œ(ë”¥ëŸ¬ë‹)\në°°ì¹˜ ëª¨ë“œì™€ íŒ¨í„´ ëª¨ë“œì˜ ì¤‘ê°„\ní›ˆë ¨ ì§‘í•©ì„ ì¼ì •í•œ í¬ê¸°ì˜ ë¶€ë¶„ ì§‘í•©ìœ¼ë¡œ ë‚˜ëˆˆ ë‹¤ìŒ ë¶€ë¶„ ì§‘í•©ë³„ë¡œ ì²˜ë¦¬í•œë‹¤.\në¶€ë¶„ ì§‘í•©ìœ¼ë¡œ ë‚˜ëˆŒ ë•Œ ëœë¤ ìƒ˜í”Œë§ì„ ì ìš©í•œë‹¤.\nê³„ì‚° ì†ë„ê°€ í›¨ì”¬ ë¹ ë¥´ë‹¤.\nLocal Minimaì— ë¹ ì§€ì§€ ì•Šê³ , Global Minimaì— ìˆ˜ë ´í•  ê°€ëŠ¥ì„±ì´ ë” ë†’ë‹¤.\nbatch size\në¯¸ë‹ˆë°°ì¹˜ ëª¨ë“œì—ì„œì˜ ë§¤ê°œë³€ìˆ˜.\në°°ì¹˜ í¬ê¸°ë¥¼ ì‘ê²Œ ë‘ëŠ” ê²ƒì´ Generalization ì„±ëŠ¥ì´ ì¢‹ë‹¤.\në°°ì¹˜ì‚¬ì´ì¦ˆê°€ ë„ˆë¬´ ì»¤ì§€ë©´Â Sharp Minimumì— ë¹ ì§€ê²Œ ëœë‹¤.\nFlat Minimumì€ Generalization ì„±ëŠ¥ì´ ì¢‹ë‹¤.\në°˜ëŒ€ë¡œ, ë°°ì¹˜ì‚¬ì´ì¦ˆê°€ ì‘ì„ìˆ˜ë¡ noiseì˜ ì˜í–¥ë ¥ì´ ì»¤ì§€ë¯€ë¡œ Sharp Minimumì—ì„œ íƒˆì¶œí•  í™•ë¥ ì´ ë†’ë‹¤.\nì°¸ê³  ë…¼ë¬¸ : On Large-batch Training for Deep Learning : Generalization Gap and Sharp Minima, 2017\nGradient Descent Algorithmì—ëŠ” ì—¬ëŸ¬ ë¬¸ì œì ë“¤ì´ ì¡´ì¬í•˜ëŠ”ë°, ì´ë¥¼ í•´ê²°í•œ Optimizerë“¤ì´ ë“±ì¥í•œë‹¤.\nQuiz. $f(x,y,z)$ì˜ ê·¸ë˜ë””ì–¸íŠ¸ ë²¡í„°ëŠ”?\n$f(x,y,z) = 9x^2 + 5y^3 - 3z$\n$\\tt ans = (18x, 15y^2, -3)$\n"},{"id":48,"href":"/posts/2023-11-13-%EC%97%B0%EA%B4%80-%EB%B6%84%EC%84%9D/","title":"ì—°ê´€ ë¶„ì„(Association Analysis) ì •ë¦¬","section":"Blog","content":"\rì—°ê´€ ê·œì¹™ ë¶„ì„(Association Rule Analysis)\r#\rì¶”ì²œ ì‹œìŠ¤í…œì˜ ê°€ì¥ ê³ ì „ì ì¸ ë°©ë²•ë¡ \nì¥ë°”êµ¬ë‹ˆ ë¶„ì„, ì„œì—´ ë¶„ì„ì´ë¼ê³ ë„ ë¶ˆë¦°ë‹¤.\nìƒí’ˆì˜ êµ¬ë§¤, ì¡°íšŒ ë“± í•˜ë‚˜ì˜ ì—°ì†ëœ ê±°ë˜ë“¤ ì‚¬ì´ì˜ ê·œì¹™ì„ ë°œê²¬í•˜ê¸° ìœ„í•´ ì ìš©í•˜ëŠ” ë°©ë²•\nì¦‰, ì‚¬ìš©ìì˜ ì¥ë°”êµ¬ë‹ˆ ë‚´ì— í¬í•¨ëœ ìƒí’ˆë“¤ì˜ ê·œì¹™ì„ ë°œê²¬í•˜ê¸° ìœ„í•´ ì ìš©í•˜ëŠ” ë°©ë²•\nìœ ì € ì •ë³´(ìœ ì € í–‰ë™ ì •ë³´)ë¥¼ í™œìš©í•˜ëŠ” ë¶„ì„ ë°©ë²•\nê·œì¹™\nIF {condition} THEN {result}\n{condition} â†’ {result}\nì—°ê´€ ê·œì¹™\nê·œì¹™ ê°€ìš´ë° ì¼ë¶€ ê¸°ì¤€(ë¹ˆë²ˆí•¨ì˜ ê¸°ì¤€)ì„ ë§Œì¡±í•˜ëŠ” ê²ƒ\nIF {antecedent} THEN {consequent}\në¹ˆë²ˆí•˜ê²Œ ë°œìƒí•˜ëŠ” ê·œì¹™ì„ ì˜ë¯¸í•œë‹¤.\nex) {ê¸°ì €ê·€} â†’ {ë§¥ì£¼}\n{ìš°ìœ } â†’ {ë¹µ}\nì´ ë•Œ, í™”ì‚´í‘œëŠ” ì—°ê´€ ê´€ê³„ë¥¼ ë‚˜íƒ€ë‚¼ ë¿, ì¸ê³¼ê´€ê³„ë¥¼ ì˜ë¯¸í•˜ì§€ ì•ŠëŠ”ë‹¤.\nItemset\nì—°ê´€ê·œì¹™ì„ êµ¬ì„±í•˜ëŠ” ìƒí’ˆì˜ ì§‘í•©(antecedent, consequent)\ní•˜ë‚˜ ì´ìƒì˜ ì§‘í•©ìœ¼ë¡œ êµ¬ì„±\nantecedentì™€ consequentëŠ” disjoint(ì„œë¡œì†Œ)ë¥¼ ë§Œì¡±í•´ì•¼ í•œë‹¤.\nk-itemset: kê°œì˜ itemìœ¼ë¡œ ì´ë£¨ì–´ì§„ itemset\nsupport count($\\sigma$)\nì „ì²´ transaction dataì—ì„œ itemsetì´ ë“±ì¥í•˜ëŠ” íšŸìˆ˜\n$\\tt \\sigma(\\text{ë¹µ,\\ ìš°ìœ }) = 3$\nsupport\nitemsetì—ì„œ ì „ì²´ transaction dataì—ì„œ ë“±ì¥í•˜ëŠ” ë¹„ìœ¨\nsupport countë¡œ ê³„ì‚°ëœ ê°’.\nsupport({ë¹µ, ìš°ìœ }) = 3 / 5 = 0.6\nì—°ê´€ ê·œì¹™ì—ì„œ ê°€ì¥ ì¤‘ìš”í•œ ê°’.\nfrequent itemset\nìœ ì €ê°€ ì§€ì •í•œ minimum support ê°’ ì´ìƒì˜ itemset\nminimum support ê°’ì„ ë„˜ì§€ ëª»í•œ itemsetì€ infrequent itemsetì´ë¼ê³  ë¶€ë¥¸ë‹¤.\nì—°ê´€ ê·œì¹™ì˜ ì²™ë„\r#\rfrequent itemsetë“¤ ì‚¬ì´ì˜ ì—°ê´€ ê·œì¹™ì„ ë§Œë“¤ê¸° ìœ„í•´ì„œ measurementê°€ í•„ìš”í•˜ë‹¤.\n$X\\rightarrow Y$ê°€ ì¡´ì¬í•  ë•Œ, $(X,Y : \\text{itemset, N: ì „ì²´ transaction ìˆ˜})$\nsupport\në‘ itemset $X,Y$ë¥¼ ëª¨ë‘ í¬í•¨í•˜ëŠ” transactionì˜ ë¹„ìœ¨\nì¦‰, ì „ì²´ transactionì— ëŒ€í•œ itemsetì˜ í™•ë¥ ê°’\nì¢‹ì€ ê·œì¹™ì„ ì°¾ê±°ë‚˜, ë¶ˆí•„ìš”í•œ ì—°ì‚°ì„ ì¤„ì¼ ë•Œ ì‚¬ìš©ëœë‹¤.\nconfidence\n$Y$ì˜ $X$ì— ëŒ€í•œ ì¡°ê±´ë¶€ í™•ë¥ \nconfidenceê°€ ë†’ì„ìˆ˜ë¡ ìœ ìš©í•œ ê·œì¹™ì´ë‹¤.\nlift\n[$X$ê°€ í¬í•¨ëœ transactionê°€ìš´ë° $Y$ê°€ ë“±ì¥í•  í™•ë¥ ] / [$Y$ê°€ ë“±ì¥í•  í™•ë¥ ]\n1ì„ ê¸°ì¤€ìœ¼ë¡œ ë‚˜íƒ€ë‚œë‹¤.\n0~1 ì‚¬ì´ì˜ ê°’ì´ ì•„ë‹ˆë‹¤.\n$\\tt lift = 1 â‡’ X,Y\\ ë…ë¦½$\n$\\tt lift \u0026gt; 1 â‡’ X,Y\\ ì–‘ì˜\\ ìƒê´€ê´€ê³„$\n$\\tt lift \u0026lt; 1 â‡’ X,Y\\ ìŒì˜\\ ìƒê´€ê´€ê³„$\nì˜ˆì‹œ\nX = ë¹µ, Y = ê³„ë€\nsupport $$ \\begin{aligned}s(X \\rightarrow Y)\u0026amp;=\\frac{n(X \\cup Y)}{N}=P(X \\cap Y)\\\u0026amp;=\\frac{n(2,5)}{5}=0.4 \\end{aligned} $$\nconfidence $$ \\begin{aligned} c(X \\rightarrow Y)\u0026amp;=\\frac{n(X \\cup Y)}{n(X)}=\\frac{n(2,5)}{n(2,4,5)}=0.66 \\\u0026amp; =P(Y \\mid X)=\\frac{P(X \\cap Y)}{P(X)}=\\frac{0.4}{0.6}=0.66\\end{aligned} $$\nlift $$ \\begin{aligned} l(X \\rightarrow Y)=\\frac{c(X \\rightarrow Y)}{s(Y)}\u0026amp;=\\frac{0.66}{0.4}=1.66 \\\u0026amp; =\\frac{P(X \\cap Y)}{P(X) P(Y)}=\\frac{s(X \\rightarrow Y)}{s(X) s(Y)}=\\frac{0.4}{0.6 \\cdot 0.4}=1.66\\end{aligned} $$\nì—°ê´€ ê·œì¹™ì˜ ì‚¬ìš©\r#\rItem ìˆ˜ê°€ ë§ì•„ì§ˆìˆ˜ë¡, ê°€ëŠ¥í•œ itemsetì— ëŒ€í•œ ruleì˜ ìˆ˜ê°€ ê¸°í•˜ê¸‰ìˆ˜ì ìœ¼ë¡œ ë§ì•„ì§„ë‹¤.\nì´ ì¤‘ ìœ ì˜ë¯¸í•œ ruleë§Œ ì‚¬ìš©í•´ì•¼ í•œë‹¤.\nminimum support, minimum confidenceë¡œ ì˜ë¯¸ì—†ëŠ” ruleì„ screen out\nì „ì²´ transaction ì¤‘ì—ì„œ ë„ˆë¬´ ì ê²Œ ë“±ì¥í•˜ê±°ë‚˜, ì¡°ê±´ë¶€ í™•ë¥ ì´ ì•„ì£¼ ë‚®ì€ ruleì„ í•„í„°ë§í•œë‹¤.\nliftê°’ìœ¼ë¡œ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬ í›„ ì˜ë¯¸ìˆëŠ” ruleì„ í‰ê°€í•œë‹¤.\nì‚¬ìš©ì ì…ì¥ì—ì„œ liftê°’ì„ ì‚¬ìš©í•˜ë©´ ë” ë§Œì¡±ìŠ¤ëŸ¬ìš´ ì¶”ì²œì„ ì–»ê²Œ ëœë‹¤.\nex) ì™€ì¸($X$), ì™€ì¸ ì˜¤í”„ë„ˆ($Y$), ìƒìˆ˜($Z$)ë¼ê³  í•  ë•Œ,\n$P(Y|X) = 0.1$, $P(Z|X) = 0.2$ì¸ ê²½ìš°\nìœ„ ìˆ˜ì‹ë§Œ ë´¤ì„ ë•, ì™€ì¸ì„ ìƒ€ì„ ë•Œ ì˜¤í”„ë„ˆê°€ ì•„ë‹Œ ìƒìˆ˜ë¥¼ ì‚´ í™•ë¥ ì´ ë” ë†’ë‹¤.\n$P(Y) = 0.01$\n$P(Z) = 0.2$\ní•˜ì§€ë§Œ, ê° ë¬¼ê±´ì„ ì‚´ í™•ë¥ ì„ ê¸°ë°˜ìœ¼ë¡œ liftê°’ì„ ê³„ì‚°í•˜ê²Œë˜ë©´ ì™€ì¸ â€” ì™€ì¸ì˜¤í”„ë„ˆì˜ liftê°’ì´ 10ì´ ë˜ê³ , ì™€ì¸ â€” ìƒìˆ˜ì˜ liftê°’ì€ 1ì´ ëœë‹¤.\nliftê°€ í¬ë‹¤ëŠ” ê²ƒì€ ruleì„ êµ¬ì„±í•˜ëŠ” antecedentì™€ consequentê°€ ì—°ê´€ì„±ì´ ë†’ê³  ìœ ì˜ë¯¸í•˜ë‹¤ëŠ” ëœ»\nì—°ê´€ ê·œì¹™ì˜ íƒìƒ‰(Mining Association Rules)\r#\rì£¼ì–´ì§„ íŠ¸ëœì­ì…˜ ê°€ìš´ë°, ì•„ë˜ ì¡°ê±´ì„ ë§Œì¡±í•˜ëŠ” ê°€ëŠ¥í•œ ëª¨ë“  ì—°ê´€ ê·œì¹™ì„ ì°¾ëŠ”ë‹¤.\nsupport â‰¥ minimum support confidence â‰¥ minimum confidence ê°€ì¥ ì‰½ê²Œ ë– ì˜¬ë¦¬ëŠ” ë°©ë²•ì€ Bruth force.\ní•˜ì§€ë§Œ ì—°ì‚°ëŸ‰ì´ ë„ˆë¬´ ì»¤ì„œ ë§¤ìš° ë¹„íš¨ìœ¨ì ì´ë‹¤.\níš¨ìœ¨ì ì¸ Association Rule Miningì„ ìœ„í•œ ë‹¨ê³„\nFrequent Itemset Generation\nminimum support ì´ìƒì˜ ëª¨ë“  itemsetì„ ìƒì„±í•œë‹¤.\nì—°ì‚°ëŸ‰ì´ ê°€ì¥ ë§ë‹¤.\nì—°ê´€ ë¶„ì„ì—ì„œ ê°€ì¥ ì¤‘ìš”í•˜ë‹¤.\nìƒì„± ì „ëµ\nê°€ëŠ¥í•œ í›„ë³´ itemsetì˜ ê°œìˆ˜ë¥¼ ì¤„ì¸ë‹¤.\nApriori Algorithm : ê°€ì§€ì¹˜ê¸°ë¥¼ í™œìš©í•˜ì—¬ íƒìƒ‰í•´ì•¼ í•˜ëŠ” Mì„ ì¤„ì¸ë‹¤. íƒìƒ‰í•˜ëŠ” transactionì˜ ìˆ«ìë¥¼ ì¤„ì¸ë‹¤.\nItemsetì˜ í¬ê¸°ê°€ ì»¤ì§ì— ë”°ë¼ ì „ì²´ Nê°œ transactionë³´ë‹¤ ì ì€ ê°œìˆ˜ë¥¼ íƒìƒ‰í•œë‹¤.\nDHP(Direct Hashing \u0026amp; Pruning) Algorithm íƒìƒ‰ íšŸìˆ˜ë¥¼ ì¤„ì¸ë‹¤.\níš¨ìœ¨ì ì¸ ìë£Œêµ¬ì¡°ë¥¼ ì‚¬ìš©í•˜ì—¬ í›„ë³´ itemsetê³¼ transactionì„ ì €ì¥í•œë‹¤.\nëª¨ë“  itemsetê³¼ transactionì˜ ì¡°í•©ì— ëŒ€í•´ íƒìƒ‰í•  í•„ìš”ê°€ ì—†ë‹¤.\nFP â€” Growth Algorithm Rule Generation\nminimum confidence ì´ìƒì˜ association ruleì„ ìƒì„±í•œë‹¤.\nì´ ë•Œ, ruleì„ ì´ë£¨ëŠ” antecedentì™€ consequentëŠ” ì„œë¡œì†Œë¥¼ ë§Œì¡±í•´ì•¼ í•œë‹¤.\n"},{"id":49,"href":"/posts/2023-06-14-%EC%9D%B8%EA%B8%B0%EB%8F%84-%EA%B8%B0%EB%B0%98-%EC%B6%94%EC%B2%9C/","title":"ì¸ê¸°ë„ ê¸°ë°˜ ì¶”ì²œì´ë€?","section":"Blog","content":" ê°„ë‹¨ ìš”ì•½\nê°€ì¥ ì¸ê¸°ìˆëŠ” ì•„ì´í…œì„ ê·œì¹™ì„ ê¸°ë°˜ìœ¼ë¡œ ì¶”ì²œí•œë‹¤.\nì¸ê¸°ë„ì˜ ì²™ë„\n- ì¡°íšŒìˆ˜, í‰ê·  í‰ì , ë¦¬ë·° ê°œìˆ˜, ì¢‹ì•„ìš”/ì‹«ì–´ìš” ìˆ˜\nì˜ˆì‹œ ë„¤ì´ë²„ ì‡¼í•‘ ë­í‚¹ ìˆœ ë‹¤ìŒ ë‰´ìŠ¤, ëŒ“ê¸€ ì¶”ì²œ ë ˆë”§ Hot ì¶”ì²œ Score ê³„ì‚° ë°©ë²•\r#\rMost Popular: ì¡°íšŒìˆ˜ê°€ ê°€ì¥ ë§ì€ ì•„ì´í…œ\r#\rìµœì‹ ì„±ì„ ê³ ë ¤í•˜ì§€ ì•Šìœ¼ë©´ í•œë²ˆ ì¡°íšŒìˆ˜ê°€ ë†’ì€ ì•„ì´í…œì´ ê³„ì† ì¶”ì²œë˜ê²Œ ëœë‹¤.\nScore Formula\nê°€ì¥ ë§ì´ ì¡°íšŒëœ ë‰´ìŠ¤ë¥¼ ì¶”ì²œí•˜ê¸°\nì¢‹ì•„ìš”ê°€ ê°€ì¥ ë§ì€ ê²Œì‹œê¸€ì„ ì¶”ì²œí•˜ê¸°\nHacker News Formula\në‰´ìŠ¤ ì¶”ì²œ ì„œë¹„ìŠ¤\n$$ score = \\frac{pageviews -1}{(age + 2)^{gravity}} $$\ngravity = 1.8 ì¡°íšŒìˆ˜ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì¶”ì²œí•´ì£¼ë©´ì„œ, ì‹œê°„ì´ ì§€ë‚¨ì— ë”°ë¼ ìŠ¤ì½”ì–´ë¥¼ ê°ì†Œì‹œí‚¨ë‹¤.\nRaddit Formula\nì‹œê°„ì— ë”°ë¥¸ ê°€ì  ë°©ì‹\n$$ score = \\log_{10}{(ups-downs)} + \\frac{sign(ups-downs)\\times seconds}{45000} $$\nì²«ë²ˆì§¸ term â€” Popularity\ní•´ë‹¹ ê°’ì´ ë†’ì•„ì§ˆìˆ˜ë¡ ì ìˆ˜ê°€ ë†’ì•„ì§„ë‹¤.\nlog scaleì´ê¸° ë•Œë¬¸ì—, ì²« voteê°€ ê°€ì¥ ê°€ì¹˜ê°€ ë†’ê³ , voteê°€ ìŒ“ì¼ìˆ˜ë¡ ì˜í–¥ë ¥ì´ ì ì  ì•½í•´ì§„ë‹¤.\në‘ë²ˆì§¸ term â€” í¬ìŠ¤íŒ…ì´ ê²Œì‹œëœ ì ˆëŒ€ ì‹œê°„\nìµœê·¼ í¬ìŠ¤íŒ…ì¼ìˆ˜ë¡ ì ˆëŒ€ ì‹œê°„ê°’ì´ ë†’ê¸° ë•Œë¬¸ì—, ë” ë†’ì€ scoreì„ ê°€ì§„ë‹¤.\nHighly Rated: í‰ê·  í‰ì ì´ ê°€ì¥ ë†’ì€ ì•„ì´í…œ\r#\rí‰ì ì˜ ì‹ ë¢°ë„ë¥¼ íŒŒì•…í•  í•„ìš”ê°€ ìˆë‹¤.\ní‰ê°€ì˜ ê°œìˆ˜ê°€ ì¶©ë¶„í•œì§€ë„ ê³ ë ¤í•´ì•¼ í•œë‹¤.\nSteam Rating Formula\n$$ avg_rating = \\frac{\\text{# of positive review}}{\\text{# of reviews}} $$\n$$ score = avg_rating - (avg_rating - 0.5) \\times 2^{-\\log\\text{(# of reviews)}} $$\nratingì€ í‰ê· ê°’ì„ ì‚¬ìš©í•˜ë˜, ì „ì²´ review ê°œìˆ˜ì— ë”°ë¼ ratingì„ ë³´ì •í•œë‹¤.\nreview ê°œìˆ˜ê°€ ì•„ì£¼ ë§ì•„ì§€ë©´ scoreì€ í‰ê·  ratingê³¼ ê±°ì˜ ë¹„ìŠ·í•´ì§„ë‹¤.\nSteam Rating Formula â†’ Movie Rating\n$$ avg_rating = \\frac{\\text{# of positive review}}{\\text{# of reviews}} $$\n$$ score = avg_rating - (avg_rating - 3.0) \\times 2^{-\\log\\text{(# of reviews)}} $$\n"},{"id":50,"href":"/posts/2023-09-27-Recsys-Overview/","title":"ì¶”ì²œ ì‹œìŠ¤í…œ ê°œìš”","section":"Blog","content":" Naver BoostCamp AI Techì—ì„œ í•™ìŠµí•œ ë‚´ìš©ì„ ì¬êµ¬ì„±í–ˆìŠµë‹ˆë‹¤.\ní•´ë‹¹ ê²Œì‹œê¸€ì€ ì§€ì†ì ìœ¼ë¡œ ì—…ë°ì´íŠ¸í•  ì˜ˆì •ì…ë‹ˆë‹¤.\në…¸ì…˜ì— ì •ë¦¬í–ˆë˜ ë‚´ìš©ì„ ë³µìŠµí•˜ë©° ë¸”ë¡œê·¸ì— ì¡°ê¸ˆì”© ì—…ë¡œë“œí•˜ê³  ìˆìŠµë‹ˆë‹¤.\n{: .prompt-info }\nì¶”ì²œ ì‹œìŠ¤í…œ\r#\rì¶”ì²œ ì‹œìŠ¤í…œ í‰ê°€ íŒ¨ëŸ¬ë‹¤ì„\r#\rRule Base\r#\rì¸ê¸°ë„ ê¸°ë°˜ ì¶”ì²œ\nì—°ê´€ ë¶„ì„(Association Analysis)\nCBF: Content Based Filtering\r#\r1. Vectorizer â€” ì•„ì´í…œ íŠ¹ì„±ì„ ë²¡í„° í˜•íƒœë¡œ ì–´ë–»ê²Œ í‘œí˜„í•˜ëŠ”ê°€\r#\rTF-IDF\nTF-IDF ê¸°ë°˜ ì¶”ì²œ\nBM25\nWord2Vec\n2. Similarity â€” íŠ¹ì„±í™”ëœ ì•„ì´í…œì´ ì„œë¡œ ì–¼ë§ˆë‚˜ ë¹„ìŠ·í•œê°€\r#\rSimilarity\nDistance\nCF: Collaborative Filtering(í˜‘ì—… í•„í„°ë§)\r#\rNBCF: Neighborhood-based CF(ì´ì›ƒ ê¸°ë°˜ í˜‘ì—… í•„í„°ë§)\r#\rMBCF: Model based Collaborative Filtering(ëª¨ë¸ ê¸°ë°˜ í˜‘ì—… í•„í„°ë§)\r#\rSupervised Learning Model\rML based CF\nNaive Bayes Classification GBM: Gradient Boosting Machine GBDT: Gradient Boosting Decision Trees XGBoost: Extreme gradient boosting LGBM: LightGBM CatBoost DL based CF\nBackground\rDL based CFì˜ ì¥ì \nNonlinear Transformation\ndataì˜ non-linearityë¥¼ íš¨ê³¼ì ìœ¼ë¡œ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆë‹¤.\në³µì¡í•œ user-item interaction patternì„ íš¨ê³¼ì ìœ¼ë¡œ ëª¨ë¸ë§\nuserì˜ ì„ í˜¸ë„ ì˜ˆì¸¡ ìš©ì´\nRepresentation Learning\nì‚¬ëŒì´ ì§ì ‘ feature designí•˜ì§€ ì•Šì•„ë„ ëœë‹¤. í…ìŠ¤íŠ¸, ì´ë¯¸ì§€, ì˜¤ë””ì˜¤ ë“± ë‹¤ì–‘í•œ ì¢…ë¥˜ì˜ ì •ë³´ë¥¼ ì¶”ì²œ ì‹œìŠ¤í…œì— í™œìš©í•  ìˆ˜ ìˆë‹¤. ê³¼ê±° ì•„ì´í…œì˜ ì´ë¯¸ì§€ë¥¼ í™œìš©í•˜ì—¬ ìƒˆë¡œìš´ ì•„ì´í…œì— ëŒ€í•œ íŠ¹ì§• ì¶”ì¶œ ê°€ëŠ¥ ì‚¬ìš©ìê°€ ë‚¨ê¸´ í…ìŠ¤íŠ¸ë¥¼ í™œìš©í•˜ì—¬ ì·¨í–¥ì— ëŒ€í•œ íŠ¹ì§• ì¶”ì¶œ ê°€ëŠ¥ ìƒˆë¡œìš´ ì•„ì´í…œì´ë‚˜ ì¸ê¸° ì—†ëŠ” ì•„ì´í…œë„ ì¶”ì²œì´ ê°€ëŠ¥ ì‚¬ìš©ìì—ê²Œ ì•„ì´í…œì„ ì™œ ì¶”ì²œí•˜ëŠ” ì´ìœ ì— ëŒ€í•œ ì„¤ëª…ë ¥ì´ ì¦ê°€ ë‹¤ì–‘í•œ ë§¥ë½ ì •ë³´ë¥¼ í•¨ê»˜ í™œìš©í•˜ê¸° ë•Œë¬¸ì— ë³´ë‹¤ ì •êµí•œ ì¶”ì²œì´ ê°€ëŠ¥ Sequence Modeling\nDNNì€ ìì—°ì–´ì²˜ë¦¬, ìŒì„± ì‹ í˜¸ ì²˜ë¦¬ ë“± sequential modeling taskì—ì„œ ì„±ê³µì ìœ¼ë¡œ ì ìš©ëœë‹¤. ì¶”ì²œ ì‹œìŠ¤í…œì—ì„œ next-item prediction, session-based recommendationë“±ì— ì‚¬ìš©ëœë‹¤. Various Architectures\nCNN, RNN ë“± ë¹„ì •í˜• ë°ì´í„° íŠ¹ì§• ì¶”ì¶œì— íŠ¹í™”ëœ êµ¬ì¡° í™œìš©ì´ ê°€ëŠ¥í•˜ë‹¤. Flexibility\nTensorflow, PyTorch ë“± ë‹¤ì–‘í•œ DL í”„ë ˆì„ì›Œí¬ ì˜¤í”ˆ ì¶”ì²œì‹œìŠ¤í…œ ëª¨ë¸ë§ flexibilityê°€ ë†’ìœ¼ë©° ë” íš¨ìœ¨ì ìœ¼ë¡œ ì„œë¹™í•  ìˆ˜ ìˆë‹¤. end-to-end êµ¬ì¡°ë¡œì¨ Domain adaptation,Generative modelingë“±ì˜ ì‘ìš© ëª¨ë¸ í™œìš©ì´ ê°€ëŠ¥í•˜ë‹¤. ë‹¨ì \nInterpretability â†’ Black Box Data Requirement â†’ ë§ì€ ì–‘ì˜ ë°ì´í„° í•„ìš” Extensive Hyperparameter Tuning â†’ ë§ì€ ì‹œê°„ ì†Œìš” ì¶”ì²œì—ì„œëŠ” DLì´ MLì„ ì••ë„í•˜ì§€ëŠ” ì•ŠëŠ”ë‹¤.\nì¶”ì²œì„ ìˆ˜í–‰í•  ë•Œ Latencyê°€ ì¤‘ìš”í•˜ê¸° ë•Œë¬¸ì—, ë„ˆë¬´ ë³µì¡í•œ ëª¨ë¸ì€ ì‚¬ìš©í•˜ì§€ ëª»í•œë‹¤.\nMLP: Multilayer Perceptron(ë‹¤ì¸µ í¼ì…‰íŠ¸ë¡ ) ê³„ì—´ ëª¨ë¸\nNCF: Neural Collaborative Filtering YouTube Recommendation AE: Autoencoder(ì˜¤í† ì¸ì½”ë”) ê³„ì—´ ëª¨ë¸\nì…ë ¥ê°’ (rating)ì„ reconstruction (decoding) í•  ìˆ˜ ìˆê²Œë” í•™ìŠµí•¨ìœ¼ë¡œì¨ ratingì´ ê°€ì§€ê³  ìˆëŠ” ì ì¬ì ì¸ íŒ¨í„´ì´ latent factor(information bottleneck)ì— ì•”í˜¸í™” (encoding)ëœë‹¤.\nDAE: Denoising Autoencoder\nU/I-RBM AutoRec NeuMF: Neural MF CDAE: Collaborative Denoising Auto-Encoder GNN: Graph Neural Network ê³„ì—´ ëª¨ë¸\nGCN: Graph Convolution Network\nNGCF: Neural Graph Collaborative Filtering LightGCN CNN: Convolutional Neural Network(ì»¨ë³¼ë£¨ì…˜ ì‹ ê²½ë§) ê³„ì—´ ëª¨ë¸\nImage-Based Recommendations VBPR: Visual BPR DeepCoNN(ì‹¬ì¸µ í˜‘ë ¥ ì‹ ê²½ë§) RNN:Recurrent Neural Network(ìˆœí™˜ì‹ ê²½ë§) ê³„ì—´ ëª¨ë¸\nLSTM(Long Short Term Memory), GRU(Gated Recurrent Unit)\nGRU4Rec RRN: Recurrent Recommender Network WDN: Wide \u0026amp; Deep Network DeepFM DIN: Deep Interest Network DCN: Deep \u0026amp; Cross Network BST: Behavior Sequence Transformer TabNet Unsupervised Learning Model\rBackground : User-free Model\rë¹„ì§€ë„í•™ìŠµ ëª¨ë¸ë“¤ ì¤‘, User-free ëª¨ë¸ë¡œ í™œìš©ë˜ëŠ” ê²½ìš°ê°€ ë§ë‹¤.\rUser-free ëª¨ë¸ì˜ ì¥ì  ($=\\gamma_u$ë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šì„ ë•Œì˜ ì¥ì )\nìƒˆë¡œìš´ ì‚¬ìš©ìì— ëŒ€í•´ inferenceê°€ ê°€ëŠ¥í•˜ë‹¤.\n$\\gamma_u$ëŠ” ìƒˆë¡œìš´ ì‚¬ìš©ìê°€ ë°œìƒí•  ë•Œë§ˆë‹¤ ì¬í•™ìŠµì„ í•„ìš”ë¡œ í•œë‹¤.\nì´ë ¥ì´ ê±°ì˜ ì—†ëŠ” ì‚¬ìš©ìì— ëŒ€í•œ ëŒ€ì‘ì´ ê°€ëŠ¥í•˜ë‹¤.\nMF ê³„ì—´ì˜ ëª¨ë¸ì€ ì´ëŸ° ìƒí™©ì—ì„œ $\\gamma_u$ê°€ ì œëŒ€ë¡œ í•™ìŠµë˜ì§€ ì•Šìœ¼ë¯€ë¡œ ì„±ëŠ¥ì´ ì¢‹ì§€ ì•Šë‹¤.\nCF ëª¨ë¸ì—ì„œ ì¢…ì¢… ë¬´ì‹œë˜ê³¤ í•˜ëŠ” sequential ì‹œë‚˜ë¦¬ì˜¤ì— ëŒ€í•´ ëŒ€ì‘ì´ ê°€ëŠ¥í•˜ë‹¤.\nMFì˜ $\\gamma_u$ëŠ” sequenceë¥¼ ê³ ë ¤í•˜ì§€ ì•ŠëŠ”ë‹¤.\nì‹¤ì œ ì¶”ì²œ ì‹œìŠ¤í…œì˜ deploymentë¥¼ ê³ ë ¤í•˜ë©´, ìƒˆë¡œìš´ ì‚¬ìš©ìê°€ ë°œìƒí•  ë•Œë§ˆë‹¤ ì¬í•™ìŠµì´ í•„ìš”í•œ ì ì€ í° ë‹¨ì ì´ë‹¤. ë”°ë¼ì„œ, user-free ëª¨ë¸ì€ ì „í†µì ì¸ MF ê³„ì—´ì˜ ëª¨ë¸ë³´ë‹¤ ì‹¤ìš©ì ì´ë¼ê³  ë³¼ ìˆ˜ ìˆë‹¤. Latent Factor Model(Embedding)\nSVD: Singular Value Decomposition(íŠ¹ì´ê°’ ë¶„í•´)\nMF: Matrix Factorization\nWRMF: Weighted Regularized MF (MF for Implicit Feedback) ALS: Alternating Least Square BPR: Bayesian Personalized Ranking Feedback\nWord2Vec CBOW: Continous Bag of Word SG: Skip-Gram SGNS: Skip-Gram with Negative Sampling Item2Vec Clustering(êµ°ì§‘í™”)\nKNN: K-Nearest Neighbor(K-ìµœê·¼ì ‘ ì´ì›ƒ)\nANN: Approximate Nearest Neighbor\nANNOY: Approximate Nearest Neighbor Oh Yeah HNSW: Hierarchical Navigable Small World Graphs IVF: Inverted File Index PQ: Product Quantization â€” Compression Clusteringì˜ ê²½ìš° ë‹¤ë¥¸ ì¶”ì²œ ë°©ë²•ë¡ ê³¼ í•¨ê»˜ ì‚¬ìš©í•˜ì—¬ íš¨ê³¼ì ì¸ ì¶”ì²œ ìˆ˜í–‰ì´ ê°€ëŠ¥í•˜ë‹¤.\nêµ°ì§‘ë‚´ì˜ ë‹¤ë¥¸ ì‚¬ìš©ìê°€ ì„ í˜¸í•˜ëŠ” ì•„ì´í…œ ì¶”ì²œ êµ°ì§‘í™” ì´í›„ í˜‘ë ¥ í•„í„°ë§(Collaborative Filtering) ì‚¬ìš©ì„ í†µí•´ ì˜ˆì¸¡ ì •í™•ë„ í–¥ìƒ ë¹„ìŠ·í•œ ì‚¬ìš©ì êµ°ì§‘ì˜ ë°ì´í„°ë¥¼ ì¶”ì¶œí•˜ì—¬ ì•„ì´í…œ ì„ í˜¸ë„ë¥¼ ê³„ì‚°í•˜ê³ , ì´ë¥¼ ì‚¬ì „ í™•ë¥ (prior probability)ë¡œ í™œìš©í•˜ì—¬ ë² ì´ì§€ì•ˆ ë°©ë²•ë¡  ì ìš© RL(ê°•í™” í•™ìŠµ)\r#\rMAB: Multi-Armed Bandit\r#\rHybrid CF\r#\rCARS: Context-aware Recommender System(ë§¥ë½ ê¸°ë°˜ ì¶”ì²œ ì‹œìŠ¤í…œ)\r#\rFM: Factorization Machine FFM: Field-aware Factorization Machine ì¶”ì²œ ë¼ì´ë¸ŒëŸ¬ë¦¬\r#\rSurprise Implicit Lightfm MSrecommenders Spotlight Buffalo Torchrec TFrecommenders "},{"id":51,"href":"/posts/2023-06-06-%EC%B6%94%EC%B2%9C-%EC%8B%9C%EC%8A%A4%ED%85%9C-%ED%8F%89%EA%B0%80-%EC%A7%80%ED%91%9C/","title":"ì¶”ì²œ ì‹œìŠ¤í…œ í‰ê°€ íŒ¨ëŸ¬ë‹¤ì„","section":"Blog","content":" MAP, NDCG\në¹„ì¦ˆë‹ˆìŠ¤ / ì„œë¹„ìŠ¤ ê´€ì \nì¶”ì²œ ì‹œìŠ¤í…œ ì ìš©ìœ¼ë¡œ ì¸í•´ ë§¤ì¶œ, PV(Page View) ì¦ê°€\nì¶”ì²œ ì•„ì´í…œìœ¼ë¡œ ì¸í•´ ìœ ì € CTR(ë…¸ì¶œ ëŒ€ë¹„ í´ë¦­)ì˜ ìƒìŠ¹\ní’ˆì§ˆ ê´€ì \nì •í™•ì„±(Accuracy)\nì—°ê´€ì„±(Relevance): ì¶”ì²œëœ ì•„ì´í…œì´ ìœ ì €ì—ê²Œ ê´€ë ¨ì´ ìˆëŠ”ê°€?\në‹¤ì–‘ì„±(Diversity): ì¶”ì²œëœ Top-K ì•„ì´í…œì— ì–¼ë§ˆë‚˜ ë‹¤ì–‘í•œ ì•„ì´í…œì´ ì¶”ì²œë˜ëŠ”ê°€?\nì‹ ë¢°ì„±(Confidence) : ì¶”ì²œ ê²°ê³¼ë¥¼ ì œê³µí•˜ëŠ” ì‹œìŠ¤í…œì´ ì‹ ë¢°í•  ë§Œí•œê°€?\ní‘œì¤€í¸ì°¨ê°€ ì ì€ ì¶”ì²œ ì‹œìŠ¤í…œì¼ìˆ˜ë¡ ë” ë†’ì€ Confidenceë¥¼ ê°€ì§„ë‹¤.\nì‹ ë¢°ì„±(Trust) : ì‚¬ìš©ìê°€ ì¶”ì²œ ê²°ê³¼ì— ì–¼ë§ˆë‚˜ ë¯¿ìŒì„ ê°€ì§€ëŠ”ê°€?\nì¶”ì²œ ê²°ê³¼ì— ì„¤ëª…ì´ ì¶”ê°€ëœë‹¤ë©´ ì‚¬ìš©ìê°€ ì¶”ì²œ ê²°ê³¼ë¥¼ ë” ë¯¿ê²Œ ëœë‹¤.\nNoveltyì™€ TrustëŠ” Trade-off ê´€ê³„ì— ìˆë‹¤.\nì‚¬ìš©ìê°€ ì´ë¯¸ ì•Œê³  ìˆê±°ë‚˜ ì¢‹ì•„í•˜ëŠ” ì•„ì´í…œì„ ì¶”ì²œí•˜ë©´ ì¶”ì²œ ê²°ê³¼ì— ëŒ€í•œ ì‹ ë¢°ë„ê°€ ì˜¬ë¼ê°„ë‹¤.\nì ìš© ë²”ìœ„(Coverage): ì¶”ì²œë˜ëŠ” ì•„ì´í…œì´ ì „ì²´ ì¤‘ ì–¼ë§ˆë‚˜ ì°¨ì§€í•˜ëŠ”ê°€?\nê°œì¸í™”(Personalization): ê°œì¸í™”ëœ ì•„ì´í…œì´ ì¶”ì²œë˜ê³  ìˆëŠ”ê°€?\nìƒˆë¡œì›€(Novelty): ì–¼ë§ˆë‚˜ ìƒˆë¡œìš´ ì•„ì´í…œì´ ì¶”ì²œë˜ê³  ìˆëŠ”ê°€?\nì¸ê¸° í•­ëª©ë§Œ ì¶”ì²œí•˜ëŠ” ê²½ìš°ë¥¼ ì¤„ì´ê¸° ìœ„í•´ ì‚¬ìš©\nì°¸ì‹ í•¨(Serendipity): ìœ ì €ê°€ ê¸°ëŒ€í•˜ì§€ ëª»í•œ ëœ»ë°–ì˜ ì•„ì´í…œì´ ì¶”ì²œë˜ëŠ”ê°€?\nSerendipity â‰  novelty (un-awareness) Serendipity = novelty + relevance + unexpectedness ê°•ì¸í•¨(Robustness \u0026amp; Stability)\nScalability : ì¶”ì²œ ì‹œìŠ¤í…œì´ ëŒ€ìš©ëŸ‰ ë°ì´í„° ë° íŠ¸ë˜í”½ì„ íš¨ê³¼ì ì´ê³  íš¨ìœ¨ì ìœ¼ë¡œ ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ”ê°€?\ní‰ì  ì˜ˆì¸¡ í‰ê°€\r#\r(user, item) ìŒì— ëŒ€í•œ compatibility scoreë¥¼ ì§ì ‘ ì˜ˆì¸¡í•œë‹¤.\nRMSE: Root Mean Squared Error\nMAE: Mean Absolute Error\nAUC\në­í‚¹ í‰ê°€\r#\rTop-K Rankingì€ userë³„ë¡œ ì˜ˆì¸¡ëœ compatibility score ìˆœì„œì— ë”°ë¼ ranked list of itemsë¥¼ ìƒì„±í•œë‹¤.\nUser Study\r#\rì‚¬ìš©ìë“¤ì„ ëª¨ì§‘í•´ì„œ ì‹œìŠ¤í…œê³¼ ìƒí˜¸ì‘ìš©í•˜ê²Œ í•œ í›„ í”¼ë“œë°±ì„ ìˆ˜ì§‘í•œë‹¤.\ní™œë°œí•œ ì‚¬ìš©ì ì°¸ì—¬ì— ë°”íƒ•ì„ ë‘ê³  ìˆê¸° ë•Œë¬¸ì—, ì‹¤ì œ ì‚¬ìš©í™˜ê²½ê³¼ ë™ë–¨ì–´ì§€ëŠ” ê²½ìš°ë„ ìˆë‹¤.\nê· ì¼í•œ ì§‘ë‹¨ì„ ë§Œë“¤ê¸° ìœ„í•´ì„  ë§ì€ ì‹œê°„ê³¼ ë¹„ìš©ì´ ì†Œëª¨ë˜ê¸° ë•Œë¬¸ì—, í˜„ì‹¤ì ìœ¼ë¡œ ì ìš©í•˜ê¸°ê°€ ì‰½ì§€ ì•Šë‹¤.\nOffline Test\r#\rì´ë¯¸ ìˆ˜ì§‘ëœ ë°ì´í„° (historical datasets)ë¥¼ í™œìš©í•˜ì—¬ ì•Œê³ ë¦¬ì¦˜ì˜ ì„±ëŠ¥ì„ í‰ê°€í•˜ëŠ” ë°©ë²•\nê³¼ê±° ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ íŒŒì•…í•œë‹¤.\nìƒˆë¡œìš´ ì¶”ì²œ ëª¨ë¸ì„ ê²€ì¦í•˜ê¸° ìœ„í•´ ê°€ì¥ ìš°ì„ ì ìœ¼ë¡œ ìˆ˜í–‰ë˜ëŠ” ë‹¨ê³„\nìœ ì €ë¡œë¶€í„° ìˆ˜ì§‘í•œ ë°ì´í„°ë¥¼ train/valid/testë¡œ ë‚˜ëˆ„ì–´ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ê°ê´€ì ì¸ ì§€í‘œë¡œ í‰ê°€\në°ì´í„° ë¶„í•  ì „ëµì— ë”°ë¼ì„œ ì‹œìŠ¤í…œ ì„±ëŠ¥ í‰ê°€ì— í° ì˜í–¥ì„ ì¤„ ìˆ˜ ìˆë‹¤. ìƒí™©ì— ë§ëŠ” ì ì ˆí•œ ë¶„í•  ì „ëµì´ í•„ìš” ë°ì´í„° ë¶„í•  ì „ëµ\nLeave One Last\nì‚¬ìš©ìë‹¹ ë§ˆì§€ë§‰ êµ¬ë§¤ë¥¼ Test setìœ¼ë¡œ, ë§ˆì§€ë§‰ì—ì„œ 2ë²ˆì§¸ë¥¼ Valid setìœ¼ë¡œ ë¶„í• \nì¥ì \ní•™ìŠµ ì‹œ ë§ì€ ë°ì´í„° ì‚¬ìš© ê°€ëŠ¥\në‹¨ì \nì‚¬ìš©ìë‹¹ ë§ˆì§€ë§‰ êµ¬ë§¤ë¡œë§Œ í‰ê°€í•˜ë¯€ë¡œ í…ŒìŠ¤íŠ¸ ì„±ëŠ¥ì´ ì „ì²´ì ì¸ ì„±ëŠ¥ì„ ë°˜ì˜í•œë‹¤ê³  ë³´ê¸° ì–´ë µë‹¤. í›ˆë ¨ ì¤‘ì— ëª¨ë¸ì´ í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒí˜¸ì‘ìš©ì„ íŠ¹ì§•ìœ¼ë¡œ í•™ìŠµí•  ê°€ëŠ¥ì„± ì¡´ì¬ Temporal User / Global Split\nì‹œê°„ì„ ì´ìš©í•œ ë¶„í•  ì „ëµ\nTemporal User ì‚¬ìš©ì ë³„ë¡œ ì‹œê°„ ìˆœì„œì— ë”°ë¼ ì¼ì • ë¹„ìœ¨ë¡œ ë°ì´í„° ë¶„í•  Leave One lastì™€ ìœ ì‚¬ Data leakage ë¬¸ì œ Temporal Global (ê¶Œì¥ë¨) ê° ìœ ì € ê°„ì— ê³µìœ ë˜ëŠ” ì‹œì ì„ ê³ ì •í•˜ì—¬, íŠ¹ì • ì‹œì  ì´í›„ì— ì´ë¤„ì§„ ëª¨ë“  ìƒí˜¸ì‘ìš©ì„ test setìœ¼ë¡œ ë¶„í•  í•™ìŠµ ë° ê²€ì¦ì— ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ìƒí˜¸ì‘ìš©ì´ ì ì€ ë¬¸ì œ í˜„ì‹¤ê³¼ ê°€ì¥ ìœ ì‚¬í•œ í‰ê°€ í™˜ê²½ì„ ì œê³µ Random Split\nê° ì‚¬ìš©ì ë³„ interactionì„ randomí•˜ê²Œ ì•„ì´í…œì„ ì„ íƒí•˜ì—¬ ë¶„í• \nì‚¬ìš©í•˜ê¸° ì‰¬ì›€ ë§ì€ train set Data leakage ë¬¸ì œ User Split\nì‚¬ìš©ìê°€ ê²¹ì¹˜ì§€ ì•Šê²Œ ì‚¬ìš©ìë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë¶„í• \nCold-start ë¬¸ì œì— ëŒ€ì‘í•˜ëŠ” ëª¨ë¸ ìƒì„± ê°€ëŠ¥ User-free ëª¨ë¸ì—ë§Œ ì‚¬ìš© ê°€ëŠ¥ Future Data leakage ë¬¸ì œ CV: Cross Validation(êµì°¨ ê²€ì¦)\në³´í†µ offline testì—ì„œ ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì—¬ì•¼ online testì— íˆ¬ì…ëœë‹¤.\nì‹¤ì œ ì„œë¹„ìŠ¤ ìƒí™©ì—ì„œëŠ” ë‹¤ì–‘í•œ ì–‘ìƒì„ ë³´ì¸ë‹¤.(serving bias)\ní•œê³„\nê²°ì¸¡ê°’ì„ ì•„ë¬´ë¦¬ ì¶”ë¡ í•œë‹¤ê³  í•˜ë”ë¼ë„, ì‹¤ì œ ìœ ì €ê°€ ì¢‹ì•„í• ì§€ ì‹«ì–´í• ì§€ëŠ” ì•Œ ìˆ˜ ì—†ë‹¤. ë°ì´í„° ìˆ˜ì§‘ ì´í›„, ì‹œê°„ì˜ íë¦„ì— ë”°ë¼ ì‚¬ìš©ì ì„ í˜¸ë„ ë° ì•„ì´í…œì˜ íŠ¹ì„±ì´ ë³€í™”í•˜ëŠ” ê²ƒì„ ë°˜ì˜í•  ìˆ˜ ì—†ë‹¤. accuracy ê´€ë ¨ ì§€í‘œë§Œìœ¼ë¡œëŠ” serendipity ë° novelty ì™€ ê°™ì€ ì¶”ì²œ ì‹œìŠ¤í…œì˜ ì¤‘ìš”í•œ íŠ¹ì„±ì„ í¬ì°©í•  ìˆ˜ ì—†ë‹¤. ì¶”ì²œ ì‹œìŠ¤í…œì— ì¡´ì¬í•˜ëŠ” feedback loopë¡œ ì¸í•´ ë‹¤ì–‘í•œ biasë“¤ì´ ì¦í­ë˜ê¸° ë•Œë¬¸ì— ë¶€ì •í™•í•œ ìƒëŒ€í‰ê°€ë¡œ ì´ì–´ì§ˆ ìˆ˜ ìˆë‹¤. Data Bias Selection Bias Precision/Recall/MAP@K\r#\rPrecision@K\nìš°ë¦¬ê°€ ì¶”ì²œí•œ Kê°œ ì•„ì´í…œ ê°€ìš´ë° ì‹¤ì œ ìœ ì €ê°€ ê´€ì‹¬ìˆëŠ” ì•„ì´í…œì˜ ë¹„ìœ¨\ndef precision_at_k(actual, predicted, k): act_set = set(actual) pred_set = set(predicted[:k]) result = len(act_set \u0026amp; pred_set) # normalized_prec = result / float(min(len(act_set),k)) return prec Recall@K\nìœ ì €ê°€ ê´€ì‹¬ìˆëŠ” ì „ì²´ ì•„ì´í…œ ê°€ìš´ë° ìš°ë¦¬ê°€ ì¶”ì²œí•œ ì•„ì´í…œì˜ ë¹„ìœ¨\ndef recall_at_k(actual, predicted, k): act_set = set(actual) pred_set =set(predicted[:k]) result = len(act_set \u0026amp; pred_set) / float(len(act_set)) return result ex) ìš°ë¦¬ê°€ ì¶”ì²œí•œ ì•„ì´í…œ ê°œìˆ˜: 5(=K)\nì¶”ì²œí•œ ì•„ì´í…œ ì¤‘ ìœ ì €ê°€ ê´€ì‹¬ìˆëŠ” ì•„ì´í…œ ê°œìˆ˜: 2\nìœ ì €ê°€ ê´€ì‹¬ìˆëŠ” ì•„ì´í…œì˜ ì „ì²´ ê°œìˆ˜: 3\n$\\tt Precision@5 = 2/5$\n$\\tt Recall@5 = 2/3$\nAP@K\nPrecision@1ë¶€í„° Precision@Kê¹Œì§€ì˜ í‰ê· ê°’\nPrecision@Kì™€ ë‹¬ë¦¬, ê´€ë ¨ ì•„ì´í…œì„ ë” ë†’ì€ ìˆœìœ„ì— ì¶”ì²œí• ìˆ˜ë¡ ì ìˆ˜ê°€ ìƒìŠ¹í•œë‹¤.\n$$ A P @ K=\\frac{1}{m} \\sum_{i=1}^K \\text { Precision@i } $$\nMAP(Mean AP)@K\nëª¨ë“  ìœ ì €ì— ëŒ€í•œ Average Precision ê°’ì˜ í‰ê· \n$$ M A P @ K=\\frac{1}{|U|} \\sum_{u=1}^{|U|}(A P @ K)_u $$\nnDCG: Normalized Discounted Cumulative Gain\r#\rì¶”ì²œ ì‹œìŠ¤í…œì— ê°€ì¥ ë§ì´ ì‚¬ìš©ë˜ëŠ” ì§€í‘œ ì¤‘ í•˜ë‚˜\nê²€ìƒ‰(IR)ì—ì„œ ë“±ì¥í•œ ì§€í‘œ.\nPrecision@K, MAP@Kì™€ ë§ˆì°¬ê°€ì§€ë¡œ Top K ë¦¬ìŠ¤íŠ¸ë¥¼ ë§Œë“¤ê³  ìœ ì €ê°€ ì„ í˜¸í•˜ëŠ” ì•„ì´í…œì„ ë¹„êµí•˜ì—¬ ê°’ì„ êµ¬í•œë‹¤.\nMAP@Kì™€ ë§ˆì°¬ê°€ì§€ë¡œ ì¶”ì²œì˜ ìˆœì„œì— ê°€ì¤‘ì¹˜ë¥¼ ë” ë§ì´ ë‘ì–´ ì„±ëŠ¥ì„ í‰ê°€í•˜ë©°, 1ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ì¢‹ë‹¤.\nMAPì™€ ë‹¬ë¦¬, ì—°ê´€ì„±ì„ ì´ì§„ê°’ì´ ì•„ë‹Œ ìˆ˜ì¹˜ë¡œë„ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤.\nìœ ì €ì—ê²Œ ì–¼ë§ˆë‚˜ ë” ê´€ë ¨ ìˆëŠ” ì•„ì´í…œì„ ìƒìœ„ë¡œ ë…¸ì¶œì‹œí‚¤ëŠ”ì§€ ì•Œ ìˆ˜ ìˆë‹¤.\nCG\nìƒìœ„ Kê°œ ì•„ì´í…œì˜ ê´€ë ¨ë„ë¥¼ í•©í•œ ê²ƒ\nìˆœì„œì— ë”°ë¼ Discountí•˜ì§€ ì•Šê³ , ë™ì¼í•˜ê²Œ ë”í•œ ê°’\n$$ C G_K=\\sum_{i=1}^K r e l_i\n$$\nDCG\nìˆœì„œì— ë”°ë¼ Cumulative Gainì„ Discountí•œë‹¤.\n$$ D C G_K=\\sum_{i=1}^K \\frac{r e l_i}{\\log _2(i+1)} $$\nIdeal DCG\nì´ìƒì ì¸ ì¶”ì²œì´ ì¼ì–´ë‚¬ì„ ë•Œì˜ DGCê°’\nì¦‰, DCGì˜ ìµœëŒ€ê°’\n$$\nI D C G=\\sum_{i=1}^K \\frac{r e l_i^{o p t}}{\\log _2(i+1)}\n$$\nNormalized DCG\n$$ N D C G=\\frac{D C G}{I D C G} $$\nOnline A/B Test\r#\rë™ì‹œì— ëŒ€ì¡°êµ° Aì™€ Bì˜ ì„±ëŠ¥ì„ í‰ê°€í•œë‹¤.\n(ëŒ€ì¡°êµ°ê³¼ ì‹¤í—˜êµ°ì˜ í™˜ê²½ì€ ìµœëŒ€í•œ ë™ì¼í•´ì•¼ í•œë‹¤.)\nì‹¤ì œ ì„œë¹„ìŠ¤ë¥¼ í†µí•´ ì–»ì–´ì§€ëŠ” ê²°ê³¼ë¥¼ í†µí•´ ìµœì¢… ì˜ì‚¬ê²°ì •ì´ ì´ë£¨ì–´ì§„ë‹¤.\nëŒ€ë¶€ë¶„ í˜„ì—…ì—ì„œ ì˜ì‚¬ê²°ì •ì— ì‚¬ìš©í•˜ëŠ” ìµœì¢… ì§€í‘œëŠ” ëª¨ë¸ ì„±ëŠ¥ì´ ì•„ë‹Œ ë§¤ì¶œ, CTR ë“±ì˜ ë¹„ì¦ˆë‹ˆìŠ¤/ì„œë¹„ìŠ¤ ì§€í‘œ\nMRR: Mean Reciprocal Rank\r#\r$$ M R R=\\frac{1}{|U|} \\sum_{u \\in U} \\frac{1}{\\operatorname{rank}_u\\left(i_u\\right)} $$\n"},{"id":52,"href":"/posts/2023-10-05-%EC%B6%94%EC%B2%9C-%EC%8B%9C%EC%8A%A4%ED%85%9C/","title":"ì¶”ì²œ ì‹œìŠ¤í…œì´ë€?","section":"Blog","content":" ì •ë³´ í•„í„°ë§(IF) ê¸°ìˆ ì˜ ì¼ì¢…. íŠ¹ì • ì‚¬ìš©ìê°€ ê´€ì‹¬ ê°€ì§ˆ ë§Œí•œ ì •ë³´ë¥¼ ì¶”ì²œí•˜ëŠ” ê²ƒ. Background\r#\rê¸°ì¡´\nìœ ì €ê°€ ì›í•˜ëŠ” ê²ƒì„ ê²€ìƒ‰í•˜ì—¬ ì´ì— ë§ëŠ” ì•„ì´í…œ ê²°ê³¼ë¥¼ ë³´ì—¬ì£¼ëŠ” Pull ë°©ì‹\nì¶”ì²œ ì‹œìŠ¤í…œ\nìœ ì €ê°€ ì›í•˜ëŠ” ê²ƒì„ ìœ ì¶”í•˜ì—¬ ì œì‹œí•˜ëŠ” Push ë°©ì‹\nìœ ì €ê°€ ìì‹ ì˜ ë‹ˆì¦ˆë¥¼ ì¿¼ë¦¬ë¡œ í‘œí˜„í•˜ì§€ ì•Šì•„ë„ ëœë‹¤.\në‹¤ì–‘í•œ ì¢…ë¥˜ì˜ ì•„ì´í…œë“¤ì„ ìœ ì €ì—ê²Œ ë…¸ì¶œì‹œí‚¬ ìˆ˜ ìˆë‹¤.\nì¶”ì²œ ì‹œìŠ¤í…œì˜ í•„ìš”ì„±\nê³¼ê±°ì—ëŠ” ìœ ì €ê°€ ì ‘í•  ìˆ˜ ìˆëŠ” ìƒí’ˆ, ì»¨í…ì¸ ê°€ ì œí•œì \nTV ì±„ë„, ì˜í™”ê´€, ë°±í™”ì , ì‹ ë¬¸ ë“±\nì›¹/ëª¨ë°”ì¼ í™˜ê²½ì— ì˜í•´ ë‹¤ì–‘í•œ ìƒí’ˆ, ì»¨í…ì¸  ë“±ì¥ â†’ ì •ë³´ ê³¼ë‹¤.\nì¼ë¶€ ìœ ëª…í•œ ì•„ì´í…œì´ ë§ì´ ì†Œë¹„ë˜ëŠ” ê²ƒ(\rThe Long Tail Phenomenon)ì´ ì•„ë‹ˆë¼, ì•„ì£¼ ë§ì€ Long Tail ì•„ì´í…œì´ ì¶”ì²œì„ í†µí•´ ì†Œë¹„ëœë‹¤.(Long Tail ì¶”ì²œ)\nì •ë³´ë¥¼ ì°¾ëŠ”ë° ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦°ë‹¤.\nìœ ì €ê°€ ì›í•˜ëŠ” ê±¸ ì–´ë–¤ í‚¤ì›Œë“œë¡œ ì°¾ì•„ì•¼ í•˜ëŠ”ì§€ ëª¨ë¥¼ ìˆ˜ ìˆë‹¤.\nLong Tail Recommendation ì‚¬ë¡€\nìœ íŠœë¸Œ ë™ì˜ìƒ ì¶”ì²œ SNS ì¹œêµ¬ ì¶”ì²œ ì¶”ì²œ ì‹œìŠ¤í…œì˜ ëª©ì \nì •ë³´ ìˆ˜ì§‘, íƒìƒ‰ ì‹œê°„ ë‹¨ì¶•í•˜ê¸° ì„ íƒì˜ í­ì„ ë„“íˆê¸° ìœ ì € â†’ ì•„ì´í…œ\níŠ¹ì • ìœ ì €ì—ê²Œ ì í•©í•œ ì•„ì´í…œì„ ì¶”ì²œ\nì•„ì´í…œ â†’ ìœ ì €\níŠ¹ì • ì•„ì´í…œì—ê²Œ ì í•©í•œ ìœ ì € ì¶”ì²œ\nì¶”ì²œì‹œìŠ¤í…œì—ì„œ ì‚¬ìš©í•˜ëŠ” ì •ë³´\r#\rìœ ì € ê´€ë ¨ ì •ë³´\nìœ ì € í”„ë¡œíŒŒì¼ë§\nì¶”ì²œ ëŒ€ìƒ ìœ ì €ì— ê´€ë ¨ëœ ì •ë³´ë¥¼ êµ¬ì¶•í•˜ì—¬, ê°œë³„ ìœ ì € í˜¹ì€ ìœ ì € ê·¸ë£¹ë³„ë¡œ ì¶”ì²œ\nì‹ë³„ì(Idendifier)\nìœ ì € ID, ë””ë°”ì´ìŠ¤ ID, ë¸Œë¼ìš°ì € ì¿ í‚¤\në°ëª¨ê·¸ë˜í”½ ì •ë³´\nì„±ë³„, ì—°ë ¹, ì§€ì—­, ê´€ì‹¬ì‚¬\nìœ ì € í–‰ë™ ì •ë³´\ní˜ì´ì§€ ë°©ë¬¸ ê¸°ë¡, ì•„ì´í…œ í‰ê°€, êµ¬ë§¤ ë“±ì˜ í”¼ë“œë°± ê¸°ë¡\nì•„ì´í…œ ê´€ë ¨ ì •ë³´\nì¶”ì²œ ì•„ì´í…œ ì¢…ë¥˜\ní¬íƒˆ: ë‰´ìŠ¤, ë¸”ë¡œê·¸, ì›¹íˆ° ë“± ì»¨í…ì¸  ì¶”ì²œ ê´‘ê³ /ì»¤ë¨¸ìŠ¤: ê´‘ê³  ì†Œì¬, ìƒí’ˆ ì¶”ì²œ ë¯¸ë””ì–´: ì˜í™”, ìŒì•…, ë™ì˜ìƒ ì¶”ì²œ ì•„ì´í…œ í”„ë¡œíŒŒì¼ë§\nì•„ì´í…œ ID ì•„ì´í…œì˜ ê³ ìœ ì •ë³´ Content base Recommendationì—ì„œëŠ” ì•„ì´í…œì˜ ê³ ìœ ì •ë³´ë§Œ í™œìš©í•˜ê¸°ë„ í•œë‹¤. ìƒí˜¸ì‘ìš© ì •ë³´(ìœ ì € â€” ì•„ì´í…œ)\nìœ ì €ì™€ ì•„ì´í…œì˜ ìƒí˜¸ì‘ìš© ë°ì´í„°\nìœ ì €ê°€ ì˜¨/ì˜¤í”„ë¼ì¸ì—ì„œ ì•„ì´í…œê³¼ ìƒí˜¸ì‘ìš©í•  ë•Œ ë°œìƒí•˜ëŠ” ë¡œê·¸\nì¶”ì²œ ì‹œìŠ¤í…œì„ í•™ìŠµí•˜ëŠ” ë°ì´í„°ì˜ Feedbackì´ ëœë‹¤.\nExplicit Feedback\nìœ ì €ì—ê²Œ ì•„ì´í…œì— ëŒ€í•œ ë§Œì¡±ë„ë¥¼ ì§ì ‘ ë¬¼ì–´ë³¸ ê²½ìš°\nex) ì˜í™”ì— ëŒ€í•œ í‰ì \nImplicit Feedback\nìœ ì €ê°€ ì•„ì´í…œì„ í´ë¦­í•˜ê±°ë‚˜ êµ¬ë§¤í•œ ê²½ìš°\nex) ì¿ íŒ¡ì—ì„œ ì•„ì´í…œì„ êµ¬ë§¤í•˜ë©´ â†’ Implicit feedback = Y\nì¶”ì²œ Task\r#\rë­í‚¹(Ranking)\r#\rìœ ì €ì—ê²Œ ì í•©í•œ ì•„ì´í…œ Top Kê°œë¥¼ ì¶”ì²œí•˜ëŠ” ë¬¸ì œ\ní‰ê°€ ì§€í‘œ: Precision@K, Recall@K, MAP@K, nDCG@K\nTop Kê°œë¥¼ ì„ ì •í•˜ê¸° ìœ„í•œ ê¸°ì¤€ í˜¹ì€ Score í•„ìš”í•˜ë‹¤. ìœ ì €(X)ê°€ ì•„ì´í…œ(Y)ì— ê°€ì§€ëŠ” ì •í™•í•œ ì„ í˜¸ë„ë¥¼ êµ¬í•  í•„ìš”ëŠ” ì—†ë‹¤. Top-k Ranking ë¬¸ì œëŠ” 0ê³¼ 1ì˜ binary-valueë¡œ ì´ë¤„ì§„ implicit feedbackì„ ì˜ˆì¸¡í•˜ëŠ” íƒœìŠ¤í¬ì— ì£¼ë¡œ ì‚¬ìš©ëœë‹¤.\nì˜ˆì¸¡(Prediction)\r#\rìœ ì €ê°€ ì•„ì´í…œì— ê°€ì§ˆ ì„ í˜¸ë„ë¥¼ ì •í™•í•˜ê²Œ ì˜ˆì¸¡(í‰ì  or í´ë¦­/êµ¬ë§¤ í™•ë¥ )\ní‰ê°€ ì§€í‘œ: MAE, RMSE, AUC\nExplicit Feedback: ì² ìˆ˜ê°€ ì•„ì´ì–¸ë§¨ì— ëŒ€í•´ ë‚´ë¦´ í‰ì ê°’ì„ ì˜ˆì¸¡ Implicit Feedback: ì˜í¬ê°€ ì•„ì´í°12ë¥¼ ì¡°íšŒí•˜ê±°ë‚˜ êµ¬ë§¤í•  í™•ë¥ ì„ ì˜ˆì¸¡ ìœ ì € â€” ì•„ì´í…œ í–‰ë ¬ì„ ì±„ìš°ëŠ” ë¬¸ì œ real-valueë¡œ ì´ë¤„ì§„ explicit feedbackì„ ì˜ˆì¸¡í•˜ëŠ” íƒœìŠ¤í¬ì— ì£¼ë¡œ ì‚¬ìš©ëœë‹¤.\nì¶”ì²œ ì‹œìŠ¤í…œ íŠ¹ì§•\në„ë©”ì¸ì— ëŒ€í•œ ë†’ì€ ì˜ì¡´ì„± ë„ë©”ì¸ ì§€ì‹ì—ì„œ ë¹„ë¡¯ëœ ì¸ì‚¬ì´íŠ¸ê°€ ë§ë‹¤. ë‹¤ì–‘í•œ ë°ì´í„°ë¥¼ ë³¼ ìˆ˜ ìˆëŠ” ê¸°íšŒì— ëŒ€í•œ ì¤‘ìš”ì„±ì´ í¬ë‹¤. íì‡„ì ì¸ ë°ì´í„° ëŒ€ìƒ ë°ì´í„°ì¸ User Dataì™€ Item DataëŠ” ëŒ€ë¶€ë¶„ ê¸°ì—… ë‚´ë¶€ì˜ ê¸°ë°€ ì‚¬í•­ì— í•´ë‹¹í•œë‹¤. ì¶”ì²œ ì‹œìŠ¤í…œ ì¢…ë¥˜\r#\rSimple Aggregate (popularity, average score, recent uploads) Association Analysis Content-based Recommendation Collaborative Filtering Item2Vec Recommendation and ANN Deep Learning-based Recommendation Context-aware Recommendation Multi-Armed Bandit(MAB)-based Recommendation #\rì¶”ì²œ ì‹œìŠ¤í…œì—ì„œëŠ” MLì„ ì£¼ìš”í•˜ê²Œ ì‚¬ìš©í•œë‹¤.\nMLì˜ ì„±ëŠ¥ì„ ì••ë„í•˜ëŠ” DLì´ ì•„ì§ ë‚˜ì˜¤ì§€ ì•Šì•˜ë‹¤.\në§ì€ ìœ ì €ê°€ ì‚¬ìš©í•˜ëŠ” Serviceì—ì„œ í° íŠ¸ë˜í”½ì„ ê°ë‹¹í•´ì•¼ í•œë‹¤.\nìµœëŒ€í•œ ê°€ë²¼ìš´ ëª¨ë¸ì„ ì“°ê²Œ ëœë‹¤.\nì¶”ì²œ ì‹œìŠ¤í…œì˜ ë¶„ì„ í”„ë¡œì„¸ìŠ¤\r#\rPreprocessing\r#\rData Transform Data Split Data Split Strategy Random split by ratio Random split by user Leave one out split Split by timepoint Model\r#\rContent-Based Filtering Collaborative Filtering Context Aware Recommendation Hybrid(Content-Based Filtering + Collaborative Filtering) Evaluation\r#\rPrediction â€” Explicit Feedbackì„ ì˜ˆì¸¡í•˜ëŠ” íƒœìŠ¤í¬ì— ì£¼ë¡œ ì‚¬ìš©\nMAE MSE RMSE Rank â€” Implicit Feedbackì„ ì˜ˆì¸¡í•˜ëŠ” íƒœìŠ¤í¬ì— ì£¼ë¡œ ì‚¬ìš©\nPrecision@K Recall@K AP@K MAP@K NDCG@K "}]