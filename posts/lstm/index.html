<!DOCTYPE html>
<html lang="en"><head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <style>
        :root {
            --accent-color: #FF4D4D;
        }
    </style>

    
    
    
    
    
    

    
    <title>LSTM: Long Short Term Memory</title>
    <meta name="description" content="ML Engineer, Data Science">
    <meta name="keywords" content='blog, gokarna, hugo, LSTM, Long-Short-Term-Memory, RNN, Sequential Model'>

    <meta property="og:url" content="https://osmin625.github.io/posts/lstm/">
    <meta property="og:type" content="website">
    <meta property="og:title" content="LSTM: Long Short Term Memory">
    <meta property="og:description" content="ML Engineer, Data Science">
    <meta property="og:image" content="https://osmin625.github.io/images/from_scratch.webp">
    <meta property="og:image:secure_url" content="https://osmin625.github.io/images/from_scratch.webp">

    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:title" content="LSTM: Long Short Term Memory">
    <meta name="twitter:description" content="ML Engineer, Data Science">
    <meta property="twitter:domain" content="https://osmin625.github.io/posts/lstm/">
    <meta property="twitter:url" content="https://osmin625.github.io/posts/lstm/">
    <meta name="twitter:image" content="https://osmin625.github.io/images/from_scratch.webp">

    
    <link rel="canonical" href="https://osmin625.github.io/posts/lstm/" />

    
    <link rel="stylesheet" type="text/css" href="/css/normalize.min.css" media="print">

    
    <link rel="stylesheet" type="text/css" href="/css/main.min.css">

    
    <link id="dark-theme" rel="stylesheet" href="/css/dark.min.css">

    
    <script src="/js/bundle.min.d751e63c00e1161a75e60eaf823256097427a013523377fb636f7605271f62a5.js" integrity="sha256-11HmPADhFhp15g6vgjJWCXQnoBNSM3f7Y292BScfYqU="></script>

    
    
</head>
<body>
        <script type="text/javascript">
            
            setThemeByUserPref();
        </script><header class="header">
    <nav class="header-nav">

        
        <div class="avatar">
            <a href="https://osmin625.github.io">
                <img src='/images/from_scratch.webp' alt="OMIN" />
            </a>
        </div>
        

        <div class="nav-title">
            <a class="nav-brand" href="https://osmin625.github.io">OMIN</a>
        </div>

        <div class="nav-links">
            
            <div class="nav-link">
                <a href="https://osmin625.github.io/"><span data-feather='home'></span> Home </a>
            </div>
            
            <div class="nav-link">
                <a href="https://osmin625.github.io/categories/"><span data-feather='list'></span> Categories </a>
            </div>
            
            <div class="nav-link">
                <a href="https://osmin625.github.io/tags/"><span data-feather='tag'></span> Tags </a>
            </div>
            
            <div class="nav-link">
                <a href="https://osmin625.github.io/posts/"><span data-feather='book'></span> archive </a>
            </div>
            

            <span class="nav-icons-divider"></span>
            <div class="nav-link dark-theme-toggle">
                <span id="dark-theme-toggle-screen-reader-target" class="sr-only"></span>
                <a>
                    <span id="theme-toggle-icon" data-feather="moon"></span>
                </a>
            </div>

            <div class="nav-link" id="hamburger-menu-toggle">
                <span id="hamburger-menu-toggle-screen-reader-target" class="sr-only">menu</span>
                <a>
                    <span data-feather="menu"></span>
                </a>
            </div>

            
            <ul class="nav-hamburger-list visibility-hidden">
                
                <li class="nav-item">
                    <a href="https://osmin625.github.io/"><span data-feather='home'></span> Home </a>
                </li>
                
                <li class="nav-item">
                    <a href="https://osmin625.github.io/categories/"><span data-feather='list'></span> Categories </a>
                </li>
                
                <li class="nav-item">
                    <a href="https://osmin625.github.io/tags/"><span data-feather='tag'></span> Tags </a>
                </li>
                
                <li class="nav-item">
                    <a href="https://osmin625.github.io/posts/"><span data-feather='book'></span> archive </a>
                </li>
                
                <li class="nav-item dark-theme-toggle">
                    <span id="dark-theme-toggle-screen-reader-target" class="sr-only">theme</span>
                    <a>
                        <span id="theme-toggle-icon" data-feather="moon"></span>
                    </a>
                </li>
            </ul>

        </div>
    </nav>
</header>
<main id="content">
    <div class="post container">
    <div class="post-header-section">
        <h1>LSTM: Long Short Term Memory</h1>
        <small role="doc-subtitle"></small>
        <p class="post-date">12월 4, 2022
        
        </p>

        <ul class="post-tags">
        
            <li class="post-tag"><a href="https://osmin625.github.io/tags/lstm">LSTM</a></li>
        
            <li class="post-tag"><a href="https://osmin625.github.io/tags/long-short-term-memory">Long-Short-Term-Memory</a></li>
        
            <li class="post-tag"><a href="https://osmin625.github.io/tags/rnn">RNN</a></li>
        
            <li class="post-tag"><a href="https://osmin625.github.io/tags/sequential-model">Sequential Model</a></li>
        
        </ul>
    </div>

    <div class="post-content">
        <p>
            <p>RNN의 장기문맥 의존성을 해결하기 위해 탄생한 모델</p>
<ul>
<li>
<p><strong>선별적 게이트</strong>라는 개념으로 선별 기억 능력을 확보한다.</p>
<p><img src="/imgs/lstm0.png" alt="lstm"></p>
<p>그림은 이해를 돕기위해 O,X로 표현했지만, 실제로는 게이트는 0~1 사이의 실수값으로 열린 정도를 조절한다.</p>
</li>
</ul>
<p>게이트의 여닫는 정도는 가중치로 표현되며 가중치는 학습으로 알아낸다.</p>
<p><strong>가중치</strong></p>
<p>순환 신경망의 <code>$\{U, V, W\}$</code>에 4개를 추가하여 <code>$\{U, U_i , U_o , W, W_i , W_o , V\}$</code></p>
<ul>
<li>
<p><code>$i$</code> : 입력 게이트</p>
</li>
<li>
<p><code>$o$</code> : 출력 게이트</p>
</li>
<li>
<p>다양한 구조 설계가 가능하다.</p>
<p><img src="/imgs/lstm1.png" alt="lstm">
<img src="/imgs/lstm2.png" alt="lstm"></p>
</li>
</ul>
<h2 id="model-concept">Model Concept</h2>
<h3 id="cell-state"><strong>Cell State</strong></h3>
<ul>
<li>
<p>LSTM의 핵심</p>
</li>
<li>
<p>모듈 그림에서 수평으로 그어진 윗 선에 해당</p>
</li>
<li>
<p>일종의 컨베이어 벨트</p>
<p>작은 linear interaction만을 적용시키면서 데이터의 흐름은 그대로 유지한다.</p>
<p>아무런 동작을 추가하지 않는다면, 정보는 전혀 바뀌지 않고 그대로 흐른다.</p>
<p><img src="/imgs/lstm3.png" alt="lstm"></p>
</li>
</ul>
<p><strong>Cell State에서 gate에 의해 정보가 추가되거나 삭제된다.</strong></p>
<h3 id="gate"><strong>Gate</strong></h3>
<ol>
<li>
<p><strong>Forget Gate</strong></p>
<p><img src="/imgs/lstm4.png" alt="lstm"></p>
</li>
<li>
<p><strong>Input Gate</strong></p>
<p><img src="/imgs/lstm5.png" alt="lstm"></p>
</li>
</ol>
<ul>
<li>
<p><strong>Cell State 업데이트</strong></p>
<p><img src="/imgs/lstm6.png" alt="lstm"></p>
</li>
</ul>
<ol>
<li>
<p><strong>Output Gate</strong></p>
<p><img src="/imgs/lstm7.png" alt="lstm"></p>
</li>
</ol>
<h3 id="수식-요약">수식 요약</h3>
<p><code>$$ \begin{aligned}f_t &amp; =\sigma_g\left(W_f x_t+U_f h_{t-1}+b_f\right) \\i_t &amp; =\sigma_g\left(W_i x_t+U_i h_{t-1}+b_i\right) \\o_t &amp; =\sigma_g\left(W_o x_t+U_o h_{t-1}+b_o\right) \\\tilde{c}_t &amp; =\sigma_c\left(W_c x_t+U_c h_{t-1}+b_c\right) \\c_t &amp; =f_t \odot c_{t-1}+i_t \odot \tilde{c}_t \\h_t &amp; =o_t \odot \sigma_h\left(c_t\right)\end{aligned} $$</code></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>ft <span style="color:#f92672">=</span> sigmoid(np<span style="color:#f92672">.</span>dot(xt, Wf) <span style="color:#f92672">+</span> np<span style="color:#f92672">.</span>dot(ht_1, Uf) <span style="color:#f92672">+</span> bf)  <span style="color:#75715e"># forget gate</span>
</span></span><span style="display:flex;"><span>it <span style="color:#f92672">=</span> sigmoid(np<span style="color:#f92672">.</span>dot(xt, Wi) <span style="color:#f92672">+</span> np<span style="color:#f92672">.</span>dot(ht_1, Ui) <span style="color:#f92672">+</span> bi)  <span style="color:#75715e"># input gate</span>
</span></span><span style="display:flex;"><span>ot <span style="color:#f92672">=</span> sigmoid(np<span style="color:#f92672">.</span>dot(xt, Wo) <span style="color:#f92672">+</span> np<span style="color:#f92672">.</span>dot(ht_1, Uo) <span style="color:#f92672">+</span> bo)  <span style="color:#75715e"># output gate</span>
</span></span><span style="display:flex;"><span>Ct <span style="color:#f92672">=</span> ft <span style="color:#f92672">*</span> Ct_1 <span style="color:#f92672">+</span> it <span style="color:#f92672">*</span> np<span style="color:#f92672">.</span>tanh(np<span style="color:#f92672">.</span>dot(xt, Wc) <span style="color:#f92672">+</span> np<span style="color:#f92672">.</span>dot(ht_1, Uc) <span style="color:#f92672">+</span> bc)
</span></span><span style="display:flex;"><span>ht <span style="color:#f92672">=</span> ot <span style="color:#f92672">*</span> np<span style="color:#f92672">.</span>tanh(Ct)
</span></span></code></pre></div><h3 id="모델-요약">모델 요약</h3>
<p><img src="/imgs/lstm8.png" alt="lstm"></p>
<h3 id="input--output-shape">Input / Output Shape</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> torch
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> torch.nn <span style="color:#66d9ef">as</span> nn
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Size: [batch_size, seq_len, input_size/num_of_features]</span>
</span></span><span style="display:flex;"><span>input <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>randn(<span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">5</span>, <span style="color:#ae81ff">4</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>lstm <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>LSTM(input_size<span style="color:#f92672">=</span><span style="color:#ae81ff">4</span>, hidden_size<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>, batch_first<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>output, h <span style="color:#f92672">=</span> lstm(input)
</span></span><span style="display:flex;"><span>output<span style="color:#f92672">.</span>size()  <span style="color:#75715e"># =&gt; torch.Size([3, 5, 2]), batch_size, seq_len, hidden_size</span>
</span></span></code></pre></div><p><img src="/imgs/lstm9.png" alt="lstm"></p>
<ul>
<li>LSTM을 활용하여 주식 가격을 예측 — 과거 5일의 종가를 예측하는 경우
<ul>
<li>Seq_len = 5</li>
<li>Input_size = 1(종가)</li>
<li>Batch_size = N</li>
</ul>
</li>
<li>LSTM을 활용하여 주식 가격을 예측 — 과거 5일의 시가, 종가, 거래량을 예측하는 경우
<ul>
<li>Seq_len = 5</li>
<li>Input_size = 3(시가, 종가, 거래량)</li>
<li>Batch_size = N</li>
</ul>
</li>
<li>LSTM을 활용하여 주식 가격을 예측 — 여러 연속형 변수와 범주형 변수가 포함된 경우
<ul>
<li>Seq_len = 5</li>
<li>Input_size = embedding size(시가, 종가, 거래량)</li>
<li>Batch_size = N</li>
</ul>
</li>
</ul>
<h3 id="model">Model</h3>
<p>기본으로 제공되는 Feature들을 병합하여 LSTM에 주입한다.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, input):
</span></span><span style="display:flex;"><span>		test, question, tag, _, mask, interaction, _ <span style="color:#f92672">=</span> input
</span></span><span style="display:flex;"><span>		batch_size <span style="color:#f92672">=</span> interaction<span style="color:#f92672">.</span>size(<span style="color:#ae81ff">0</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>		<span style="color:#75715e">#Embedding</span>
</span></span><span style="display:flex;"><span>		embed_interaction <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>embedding_interaction(interaction)
</span></span><span style="display:flex;"><span>		embed_test <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>embedding_test(test)
</span></span><span style="display:flex;"><span>		embed_question <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>embedding_question(question)
</span></span><span style="display:flex;"><span>		embed_tag <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>embedding_tag(tag)
</span></span><span style="display:flex;"><span>		
</span></span><span style="display:flex;"><span>		embed <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>cat([embed_interaction,
</span></span><span style="display:flex;"><span>											 embed_test,
</span></span><span style="display:flex;"><span>											 embed_question,
</span></span><span style="display:flex;"><span>											 embed_tag,], <span style="color:#ae81ff">2</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>		X <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>conb_proj(embed)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>		hidden <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>init_hidden(batch_size)
</span></span><span style="display:flex;"><span>		out, hidden <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>lstm(X, hidden)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>		out <span style="color:#f92672">=</span> out<span style="color:#f92672">.</span>contiguous()<span style="color:#f92672">.</span>view(batch_size, <span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, self<span style="color:#f92672">.</span>hidden_dim)
</span></span><span style="display:flex;"><span>		
</span></span><span style="display:flex;"><span>		out <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>fc(out)
</span></span><span style="display:flex;"><span>		preds <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>activation(out)<span style="color:#f92672">.</span>view(batch_size, <span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>		<span style="color:#66d9ef">return</span> preds
</span></span></code></pre></div><h3 id="lstm--attention">LSTM + Attention</h3>
<p>기존의 LSTM 모델에 Attention Layer을 추가한다.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, input):
</span></span><span style="display:flex;"><span>		test, question, tag, _, mask, interaction, _ <span style="color:#f92672">=</span> input
</span></span><span style="display:flex;"><span>		batch_size <span style="color:#f92672">=</span> interaction<span style="color:#f92672">.</span>size(<span style="color:#ae81ff">0</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>		<span style="color:#75715e">#Embedding</span>
</span></span><span style="display:flex;"><span>		embed_interaction <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>embedding_interaction(interaction)
</span></span><span style="display:flex;"><span>		embed_test <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>embedding_test(test)
</span></span><span style="display:flex;"><span>		embed_question <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>embedding_question(question)
</span></span><span style="display:flex;"><span>		embed_tag <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>embedding_tag(tag)
</span></span><span style="display:flex;"><span>		
</span></span><span style="display:flex;"><span>		embed <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>cat([embed_interaction,
</span></span><span style="display:flex;"><span>											 embed_test,
</span></span><span style="display:flex;"><span>											 embed_question,
</span></span><span style="display:flex;"><span>											 embed_tag,], <span style="color:#ae81ff">2</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>		X <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>conb_proj(embed)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>		hidden <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>init_hidden(batch_size)
</span></span><span style="display:flex;"><span>		out, hidden <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>lstm(X, hidden)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>		out <span style="color:#f92672">=</span> out<span style="color:#f92672">.</span>contiguous()<span style="color:#f92672">.</span>view(batch_size, <span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, self<span style="color:#f92672">.</span>hidden_dim)
</span></span><span style="display:flex;"><span>		
</span></span><span style="display:flex;"><span>		<span style="color:#f92672">**</span>extended_attention_mask <span style="color:#f92672">=</span> mask<span style="color:#f92672">.</span>unsqueeze(<span style="color:#ae81ff">1</span>)<span style="color:#f92672">.</span>unsqueeze(<span style="color:#ae81ff">2</span>)
</span></span><span style="display:flex;"><span>		extended_attention_mask <span style="color:#f92672">=</span> extended_attention_mask<span style="color:#f92672">.</span>to(dtype<span style="color:#f92672">=</span>torch<span style="color:#f92672">.</span>float32)
</span></span><span style="display:flex;"><span>		extended_attention_mask <span style="color:#f92672">=</span> (<span style="color:#ae81ff">1.0</span> <span style="color:#f92672">-</span> extended_attention_mask) <span style="color:#f92672">*</span> <span style="color:#f92672">-</span><span style="color:#ae81ff">10000.0</span>
</span></span><span style="display:flex;"><span>		head_mask <span style="color:#f92672">=</span> [<span style="color:#66d9ef">None</span>] <span style="color:#f92672">*</span> self<span style="color:#f92672">.</span>n_layers
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>		encoded_layers <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>attn(out, extended_attention_mask, head_mask<span style="color:#f92672">=</span>head_mask)
</span></span><span style="display:flex;"><span>		sequence_output <span style="color:#f92672">=</span> encoded_layer[<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>]<span style="color:#f92672">**</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>		out <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>fc(sequence_output)
</span></span><span style="display:flex;"><span>		preds <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>activation(out)<span style="color:#f92672">.</span>view(batch_size, <span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>		<span style="color:#66d9ef">return</span> preds
</span></span></code></pre></div><h3 id="bert">BERT</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, input):
</span></span><span style="display:flex;"><span>		test, question, tag, _, mask, interaction, _ <span style="color:#f92672">=</span> input
</span></span><span style="display:flex;"><span>		batch_size <span style="color:#f92672">=</span> interaction<span style="color:#f92672">.</span>size(<span style="color:#ae81ff">0</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>		<span style="color:#75715e">#Embedding</span>
</span></span><span style="display:flex;"><span>		embed_interaction <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>embedding_interaction(interaction)
</span></span><span style="display:flex;"><span>		embed_test <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>embedding_test(test)
</span></span><span style="display:flex;"><span>		embed_question <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>embedding_question(question)
</span></span><span style="display:flex;"><span>		embed_tag <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>embedding_tag(tag)
</span></span><span style="display:flex;"><span>		
</span></span><span style="display:flex;"><span>		embed <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>cat([embed_interaction,
</span></span><span style="display:flex;"><span>											 embed_test,
</span></span><span style="display:flex;"><span>											 embed_question,
</span></span><span style="display:flex;"><span>											 embed_tag,], <span style="color:#ae81ff">2</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>		X <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>conb_proj(embed)
</span></span><span style="display:flex;"><span>		
</span></span><span style="display:flex;"><span>		<span style="color:#f92672">**</span><span style="color:#75715e"># Bert</span>
</span></span><span style="display:flex;"><span>		encoded_layers <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>encoder(inputs_embeds<span style="color:#f92672">=</span>X, attention_mask<span style="color:#f92672">=</span>mask)
</span></span><span style="display:flex;"><span>		out<span style="color:#f92672">=</span>encoded_layers[<span style="color:#ae81ff">0</span>]<span style="color:#f92672">**</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>		out <span style="color:#f92672">=</span> out<span style="color:#f92672">.</span>contiguous()<span style="color:#f92672">.</span>view(batch_size, <span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, self<span style="color:#f92672">.</span>hidden_dim)
</span></span><span style="display:flex;"><span><span style="color:#f92672">****</span>
</span></span><span style="display:flex;"><span>		out <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>fc(sequence_output)
</span></span><span style="display:flex;"><span>		preds <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>activation(out)<span style="color:#f92672">.</span>view(batch_size, <span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>		<span style="color:#66d9ef">return</span> preds
</span></span></code></pre></div><p><strong>참고 자료</strong></p>
<p><a href="https://dgkim5360.tistory.com/entry/understanding-long-short-term-memory-lstm-kr">Long Short-Term Memory (LSTM) 이해하기</a></p>

        </p>
        
    </div>

    <div class="prev-next">
        
    </div>

    
    
    <svg id="btt-button" class="arrow-logo" xmlns="http://www.w3.org/2000/svg" height="1em" viewBox="0 0 384 512" onclick="topFunction()" title="Go to top">
        
        <path d="M177 159.7l136 136c9.4 9.4 9.4 24.6 0 33.9l-22.6 22.6c-9.4 9.4-24.6 9.4-33.9 0L160 255.9l-96.4 96.4c-9.4 9.4-24.6 9.4-33.9 0L7 329.7c-9.4-9.4-9.4-24.6 0-33.9l136-136c9.4-9.5 24.6-9.5 34-.1z"/>
    </svg>
    
    <script>
        let backToTopButton = document.getElementById("btt-button");

        window.onscroll = function() {
            scrollFunction()
        };

        function scrollFunction() {
            if (document.body.scrollTop > 20 || document.documentElement.scrollTop > 20) {
                backToTopButton.style.display = "block";
            } else {
                backToTopButton.style.display = "none";
            }
        }

        function topFunction() {
            smoothScrollToTop();
        }

        function smoothScrollToTop() {
            const scrollToTop = () => {
                const c = document.documentElement.scrollTop || document.body.scrollTop;
                if (c > 0) {
                    window.requestAnimationFrame(scrollToTop);
                    window.scrollTo(0, c - c / 8);
                }
            };
            scrollToTop();
        }
    </script>
    
    
    <div id="tex">
        <script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [['$$','$$']],
    processEscapes: true,
    processEnvironments: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
    TeX: { equationNumbers: { autoNumber: "AMS" },
         extensions: ["AMSmath.js", "AMSsymbols.js"] }
  }
  });
  MathJax.Hub.Queue(function() {
    
    
    
    var all = MathJax.Hub.getAllJax(), i;
    for(i = 0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script> 
    </div>
    
</div>

<aside class="post-toc">
    <nav id="toc">
        <nav id="TableOfContents">
  <ul>
    <li>
      <ul>
        <li><a href="#model-concept">Model Concept</a>
          <ul>
            <li><a href="#cell-state"><strong>Cell State</strong></a></li>
            <li><a href="#gate"><strong>Gate</strong></a></li>
            <li><a href="#수식-요약">수식 요약</a></li>
            <li><a href="#모델-요약">모델 요약</a></li>
            <li><a href="#input--output-shape">Input / Output Shape</a></li>
            <li><a href="#model">Model</a></li>
            <li><a href="#lstm--attention">LSTM + Attention</a></li>
            <li><a href="#bert">BERT</a></li>
          </ul>
        </li>
      </ul>
    </li>
  </ul>
</nav>
    </nav>
</aside>


    

        </main><footer class="footer">
    
    

    
    <span>&copy; 2024 OMIN</span>
    
</footer>
</body>
</html>
